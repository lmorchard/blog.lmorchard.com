<?xml version="1.0" encoding="UTF-8"?>
  <rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
      <title>blog.lmorchard.com</title>
      <description>It&#39;s all spinning wheels &amp; self-doubt until the first pot of coffee.</description>
      <link>https://lmorchard.github.io/blog.lmorchard.com</link>
      <atom:link href="https://lmorchard.github.io/blog.lmorchard.com/index.rss" rel="self" type="application/rss+xml" />      
      <item>
          <title>Trying to imagine hackers of cognition and the infinite</title>
          <description>I&#39;ve just read Mark Pilgrim&#39;s post, &quot;The infinite hotel&quot;, which I&#39;m sure I&#39;ll need to re-read a few times and chase down references to read.  Also I&#39;m reading Gödel, Escher, Bach again for the third time, since I first read it in high school and needed corks in my ears to prevent brain slurry from spilling out.  I really need to read more of this sort of thing, refresh myself on all the math I took in college, and explore some of this really abstract stuff.


Something I&#39;ve been musing about lately, without any real novel ideas or insights, is about the history of computation and these thinking machines.  Not history in terms of events and when, but in terms of the concepts and discoveries leading up to keyboards, screens, and code today.  Thinking about things like recursion, and sets, and logic, and all the patterns and revolutions in thought that are the basis for everyday business and life today.


I&#39;ve been trying to imagine the world in each moment where each of these things were new, when these things were worked out in minds and on paper.  When there were no computational engines available to carry out calculations or work out conclusions to logical constructions.  
Today, these discoveries are crystallized in computing architectures, and so geeks hack and play and learn by example.  The construction of the CPU is objective fact, independent of subjective thought or understanding, and the behavior of code demonstrates the laws and rules.  Before, the rules were carefully reasoned out and intuited from observations on the objective universe, but now they&#39;re assimilated by example from mechanically working artifacts.


I&#39;m not sure I&#39;m expressing this very well, or if my thoughts are very well formed altogether, but I&#39;m trying to imagine mental life without readily available, objectively existing computational artifacts with which we can play, without prohibitive investments of effort or time.  No scripting languages with which to just try out logical constructions.  No calculators with which to solve formulae.  All manual, all by hand, all worked out by careful thought and precision.  I&#39;m trying to imagine what geeks like me, as I am today, would be like at a time when everyone dealing in these things was an abstraction astronaut, and there was not really a such thing as that-which-just-works or worse-is-better.  Does this make any sense?


Again, this is not really an expression of anything coherent or novel.  This is mostly me just in awe of how we got here, and trying to get myself above the mode of being just a hacker chasing down the phylogeny of all that&#39;s come before, and into some meta-mode of understanding of the things behind what makes these thinking machines and the thinking itself work.  Maybe after a few more decades of this I&#39;ll have some thoughts worth sharing synthesized from all that I&#39;ve learned.</description>
          
          <pubDate>Fri, 05 Dec 2003 17:40:00 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/12/05/hacking-infinite-and-cognition/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/12/05/hacking-infinite-and-cognition/</guid>
        </item><item>
          <title>Varied feed polling times versus item urgency in aggregators</title>
          <description>The problem with varying the polling interval is that the need varies. It&#39;s ok not to poll my little opensource website within 24 hours, but what about the announcements to the civil defence website or local municipal environment alerts, or the nuclear power plant news feed?

Source:Comments on The End of RSS 


Definitely a good point there.  For most of the feeds in my daily habit, what I use is an AIMD variation on my polling frequency per feed based on occurrence of new items.  For feeds with low-frequency but high-urgency items, a different algorithm should come into play.



On the other hand...  should incoming alerts with that much urgency really be conveyed via an architecture driven by polling?  Here&#39;s an excellent case for tying instant messaging systems and pub/sub into the works.</description>
          
          <pubDate>Wed, 26 Nov 2003 16:02:01 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/11/26/polling-and-urgency/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/11/26/polling-and-urgency/</guid>
        </item><item>
          <title>Didja get that memo?</title>
          
          
          <pubDate>Wed, 26 Nov 2003 14:50:43 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/11/26/the-memo/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/11/26/the-memo/</guid>
        </item><item>
          <title>Publishing Quick Links in blosxom with del.icio.us via xmlstarlet</title>
          <description>In case anyone is interested in using del.icio.us with blosxom in place of my own BookmarkBlogger, get yourself a copy of xmlstarlet and check out this shell script:

#!/bin/bash

&lt;p&gt;DATE=${1-&lt;code&gt;date +%Y-%m-%d&lt;/code&gt;}
BLOG=&quot;/Users/deusx/desktop/decafbad-entries/links&quot;
FN=&quot;${BLOG}/&quot;&lt;code&gt;echo ${DATE} | sed -e &#39;y/0123456789-/oabcdefghij/&#39;&lt;/code&gt;&quot;.txt&quot;&lt;/p&gt;
&lt;p&gt;curl -s -u deusx:HAHAHA &#39;&lt;a href=&quot;http://del.icio.us/api/posts/get?dt=&#39;$%7BDATE%7D&quot;&gt;http://del.icio.us/api/posts/get?dt=&#39;${DATE}&lt;/a&gt; |&lt;br&gt;    tidy -xml -asxml -q -f /dev/null |&lt;br&gt;    xml sel -t -o &quot;Quick Links&quot; -n &lt;br&gt;            -e &#39;ul&#39;  -m &#39;//post&#39; &lt;br&gt;            -e &#39;li&#39;  -e &#39;a&#39; -a &#39;href&#39; -v &#39;@href&#39; &lt;br&gt;            -b -v &#39;text()&#39; -n  &gt; ${FN}&lt;/p&gt;
&lt;p&gt;touch -d &quot;${DATE} 23:59&quot; ${FN}


You could do this with XSLT, but hacking with a REST-ish &amp; XML producing web service entirely in a shell script seemed oddly appealing to me that week.  Extending this sort of thing to blogging systems other than blosxom is left as an exercise to the reader.


Update: Hmm, looks like one of the blosxom plugins I&#39;m using hates the variables in my code above.  So I stuck curly braces in, which seem to get through okay.</description>
          
          <pubDate>Fri, 21 Nov 2003 14:47:10 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/11/21/delicious-quicklinks/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/11/21/delicious-quicklinks/</guid>
        </item><item>
          <title>Building the Recipe Web III</title>
          <description>I haven’t had much time to tinker in the past week or two, but I have still been thinking about recipes and blogs.  I’ve also been trading emails with Troy Hakala, who’s one of the food &amp; techie geeks responsible for RecipeZaar and who also has had a hand in the aforementioned RecipeML spec -- clearly, this is someone who knows what’s what with recipes online.

&lt;p&gt;The &lt;a href=&quot;http://www.recipezaar.com&quot;&gt;RecipeZaar&lt;/a&gt; site is very rich in functionality, allowing you to refine searches by various types of information in recipes such as ingredient, course, dietary needs, preparation, and occasion.  The recipes themselves are shown with calculated nutritional information, and discussion and ratings are offered as well.  But, what looks like one of the best features of the site is the recipe submission process.&lt;/p&gt;

&lt;p&gt;With all the various ways recipes are organized and indexed on this site, I would expect that entering a new recipe would demand some tedious form field entry to get the information into the database.  And, while there is a tiny bit of that toward the end, with regard to categorization and cooking time, the entry of ingredients and cooking steps is done by pasting them in whole into two text areas.  It seems similar to some job listing sites which manage to parse &lt;span class=&quot;caps&quot;&gt;ASCII&lt;/span&gt; dumps of applicants&amp;#8217; resumes.&lt;/p&gt;

&lt;p&gt;As you&amp;#8217;d guess, a parser -- which I&amp;#8217;ve been told has been in constant refinement since the site started -- churns through the text of the ingredient list and separates things into their parts.  And these parts include things like amounts and units, and ingredients which are themselves linked to nutritional and descriptive data.  It&amp;#8217;s all cross-referenced and classified in a way that makes the &lt;span class=&quot;caps&quot;&gt;RDF&lt;/span&gt; fan in me cheer -- although this ain&amp;#8217;t &lt;span class=&quot;caps&quot;&gt;RDF&lt;/span&gt; or the Semantic Web.&lt;/p&gt;

&lt;p&gt;But what if it could be?  Or, at the least, what if recipes could join the list of content types available to be published by bloggers.  If not in &lt;span class=&quot;caps&quot;&gt;RDF&lt;/span&gt;, then say maybe in some further developed form of RecipeML?  I see recipes in my aggregator from &lt;a href=&quot;http://www.10500bc.org/archive/006335.php#006335&quot;&gt;a weblog like this&lt;/a&gt; and I wonder what things could be done with data like this if it were in some structured and machine-readable form.&lt;/p&gt;

&lt;p&gt;Well, for one thing, bloggers could ping &lt;a href=&quot;http://www.recipezaar.com&quot;&gt;RecipeZaar&lt;/a&gt; when they post new recipes, and that site could become the &lt;a href=&quot;http://www.feedster.com&quot;&gt;Feedster&lt;/a&gt; of recipes.  Bloggers could also provide &lt;span class=&quot;caps&quot;&gt;RSS&lt;/span&gt; indexes of their recipe postings, possibly for use by future aggregators -- although, I think some major work needs to be done on the &lt;a href=&quot;http://www.decafbad.com/blog/tech/dynamic_polling_freq_too.html&quot;&gt;behavior&lt;/a&gt; of &lt;a href=&quot;http://diveintomark.org/archives/2003/07/21/atom_aggregator_behavior_http_level&quot;&gt;aggregators&lt;/a&gt; before we go too far down that road.&lt;/p&gt;

&lt;p&gt;So, what would probably be a great start is to come up with some MovableType plugin which helps support recipe post authoring.  Ideally, this would produce both &lt;span class=&quot;caps&quot;&gt;HTML&lt;/span&gt; and RecipeML (or better) content, for both human and machine consumption.&lt;/p&gt;

&lt;p&gt;But,  there&amp;#8217;s that filling-out-of-forms issue again.  I just read &lt;a href=&quot;http://blogs.it/0100198/2003/11/18.html#a2022&quot;&gt;a post by Marc Canter&lt;/a&gt; talking about the growing formats for reviews, where he asks why there aren&amp;#8217;t more fields to fill out for a properly constructed post.  He acknowledges that end-users might not want to fill those out, but we need that data to be potentially available -- but my take on it is that end-users won&amp;#8217;t &lt;strong&gt;ever&lt;/strong&gt; want to fill out any fields that they don&amp;#8217;t have to.  And, if there are too many fields that they &lt;strong&gt;do&lt;/strong&gt; have to fill out, they&amp;#8217;ll never do it.&lt;/p&gt;

&lt;p&gt;You&amp;#8217;ll never get people posting reviews or recipes or anything beyond blog entries without dead simple tools to do it.  Because, of course, your average person sharing recipes has absolutely zero interest in authoring &lt;span class=&quot;caps&quot;&gt;XML&lt;/span&gt; or &lt;span class=&quot;caps&quot;&gt;RDF&lt;/span&gt;.  It&amp;#8217;s the techies and machines that want to see things authored in these strange formats, so the motivation to see rich, structured data is on the wrong side of the fence to encourage its production.&lt;/p&gt;

&lt;p&gt;Thus, &lt;a href=&quot;http://www.recipezaar.com&quot;&gt;RecipeZaar&lt;/a&gt; uses a parser in its recipe submission forms to ease the process as much as possible.  And, it&amp;#8217;s been successful, since they can point to over 70,000 recipes submitted to their site by users.&lt;/p&gt;

&lt;p&gt;One solution for this, if the people behind &lt;a href=&quot;http://www.recipezaar.com&quot;&gt;RecipeZaar&lt;/a&gt; like the idea, is to borrow their parser via web service for use in my hypothetical MovableType plugin.  This could also be used for any number of other blogging tools.  On the upside, we get the benefit of all the work done by Troy and company, and they get to pull in more recipes.  On the downside, we&amp;#8217;re dependant on a web service not under our control for the basic functionality of this plugin.  Maybe not such a bad option, practically speaking, since its a great site run by great people.&lt;/p&gt;

&lt;p&gt;So, just sharing some thoughts here.  I&amp;#8217;m excited to see more varieties of micro-content shared between the people of the web, but the thing I see least talked about is how this stuff will be authored.  I read about data formats and all that, but in terms of user interface, we haven&amp;#8217;t progressed much past the &lt;span class=&quot;caps&quot;&gt;HTML&lt;/span&gt; textarea.  Also, I often see handwaving and assumptions that the content is really pretty simple -- but as Troy Hakala would tell you, not even something as &amp;#8220;simple&amp;#8221; as a recipe is a slam dunk in terms of digestion by a machine.  There needs to be some happy medium between a natural human expression of information, and the rigorous structuring required by a machine, mediated by good user interface.&lt;/p&gt;</description>
          
          <pubDate>Thu, 20 Nov 2003 16:34:09 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/11/20/recipe-web-3/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/11/20/recipe-web-3/</guid>
        </item><item>
          <title>VoodooPad gets an XML-RPC wiki API</title>
          <description>You wanted to share the same documents with your coworkers and friends. Now you can.

With VoodooPad 1.1, you can view, edit, and save to any wiki that supports the &#39;vpwiki api&#39;.
Source:Flying Meat Software

&lt;p&gt;Funny, I&amp;#8217;ve been tinkering with &lt;a href=&quot;http://www.decafbad.com/twiki/bin/view/Main/XmlRpcToWiki&quot;&gt;a wiki &lt;span class=&quot;caps&quot;&gt;API&lt;/span&gt;&lt;/a&gt; along with a &lt;a href=&quot;http://www.jspwiki.org/Wiki.jsp?page=WikiRPCInterface&quot;&gt;few others tinkerers&lt;/a&gt; for a year or so now.  I wonder if we could get these APIs merged or synched and give VoodooPad access to a slew of wikiware?&lt;/p&gt;</description>
          
          <pubDate>Wed, 19 Nov 2003 01:36:31 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/11/19/voodoo-pad-wiki-api/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/11/19/voodoo-pad-wiki-api/</guid>
        </item><item>
          <title>Building the Recipe Web II</title>
          <description>Every once in a while, someone gets ideas about crossing recipes and computers. Of course, I love the idea. Two common ideas we hear a lot are 1) to put recipes in XML format and do all sorts of wonderful things and 2) that kitchen appliances should be smart and you should be able to feed them recipes and have your food made for you. They&#39;re both great ideas, but invariably, people underestimate the work involved (&quot;But it&#39;s just a recipe!&quot;) and overestimate the usefulness (&quot;It would be so cool!&quot;).
Source:Troy &amp; Gay

&lt;p&gt;Here&amp;#8217;s a &lt;a href=&quot;http://www.decafbad.com/comments/tech/the_recipe_web/#comment-aofioohohiooacd&quot;&gt;good response&lt;/a&gt; from someone who knows what he&amp;#8217;s talking about when it comes to recipes on the web&amp;#8212;he&amp;#8217;s one of the contributors to the aforementioned RecipeML format and is part of the team responsible for &lt;a href=&quot;http://www.recipezaar.com&quot;&gt;Recipezaar&lt;/a&gt; .  While I think that recipes as syndicated microcontent could be a good thing, Troy makes some important points here.&lt;/p&gt;</description>
          
          <pubDate>Mon, 17 Nov 2003 04:39:12 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/11/17/the-recipe-web-2/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/11/17/the-recipe-web-2/</guid>
        </item><item>
          <title>Building the Recipe Web?</title>
          <description>RecipeML is a format for representing recipes on computer. It is written in the increasingly popularExtensible Markup Language - XML.

If you run a recipe web site, or are creating a software program -- on any platform -- that works with recipes, then you should consider using RecipeML for coding your recipes! See the FAQs and the new examples for more info.

Source:RecipeML - Format for Online Recipes


So I&#39;m all about this microcontent thing, thinking recently about recipes since reading Marc Canter&#39;s post about them.  Actually, I&#39;ve been thinking about them for a couple of years now, since I&#39;d really like to start cooking some decent meals with the web&#39;s help.  Oh yeah, and I&#39;m a geek, so tinkering with some data would be fun too.


One thing I rarely notice mentioned when ideas like this come up is pre-existing work.  Like RecipeML or even the non-XML MealMaster format.  Both of these have been around for quite a long time, especially so in the case of MealMaster.  In fact, if someone wanted to bootstrap a collection of recipes, you can find a ton (150,000) of MealMaster recipes as well as a smaller archive (10,000) of RecipeML files.  Of course, I&#39;m not sure about the copyright situation with any of these, but it&#39;s a start anyway.


But, the real strength in a recipe web would come from cooking bloggers.  Supply them with tools to generate RecipeML, post them on a blog server, and index them in an RSS feed.  Then, geeks get to work building the recipe aggregators.  Hell, I&#39;m thinking I might even give this a shot.  Since I&#39;d really like to play with some RDF concepts, maybe I&#39;ll write some adaptors to munge RecipeML and MealMaster into RDF recipe data.  Cross that with FOAF and other RDF whackyness, and build an empire of recipe data.


The thing I wonder, though, is why hasn&#39;t anyone done this already?  And why hasn&#39;t anyone really mentioned much about what&#39;s out there already like RecipeML and MealMaster?  It seems like the perfect time to add this into the blogosphere.</description>
          
          <pubDate>Fri, 14 Nov 2003 23:51:23 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/11/14/the-recipe-web/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/11/14/the-recipe-web/</guid>
        </item><item>
          <title>The Whuffie Web II</title>
          <description>What I believe we are seeing is domain experts seeking each other out.  Crossing organizational and philosophical boundaries.
Source:Sam Ruby: Whuffie Web

...someone that&#39;s G-list globally might be A-list amongst pet owners.
Source:Danny Ayers: Whuffie Web


A very, very good point that I&#39;d missed at first thought about the Whuffie Web.  There&#39;s a matter of scale involved here, where the relative A&#39;s through Z&#39;s are completely different given your choice of grouping.  And, where choice of grouping is around topic area, the world&#39;s a bit of a smaller place and getting your questions answered is likely much easier.  Especially if you&#39;ve built up some Whuffie in that domain area by generating some useful answers and knowledge yourself.  For newcomers to a domain of knowledge, who have lesser stockpiles of Whuffie, they&#39;ll hopefully be fortunate enough to find much of what they&#39;re looking for chronicled in the archives of blogs of those who&#39;ve come before.  When they don&#39;t, though, it can still be a frustrating experience.



But, semantic web tech in and of itself doesn&#39;t solve the problem where data or knowledge is missing altogether.  How could it?  So, although I was a bit dismissive at first thought about what Dave Winer wrote, he nonetheless has a good point.  Even if the semantic web were richly populated with data and running in full swing, it would still be missing large swaths of Things People Know.  And, well, the thing to use in that case is-- wait for it-- People Who Know Things.  And the way you hopefully can get to them is by being nice and interesting, then blog the answers or ask the people answering your query to blog it themselves.  Then, hopefully, we have blogging tools which can do the bits of pre-digestion to allow that knowledge to be accessed via semantic web machinery to fill in the gaps.



This all takes me back to when I first encountered Usenet in my Freshman year of college, and became instantly enamoured with FAQs.  It seemed like there was a FAQ for everything: coffee, anime, meditation, Baha&#39;i faith, Objectivism, and hedgehogs.  It seems mighty naive to me now, but at the time, I so thought that this was the modern knowledge factory.  Through the contentious and anal bickerings of discussion threads on Usenet, and the subsequent meticulous maintenance of FAQ files, every trivial bit about everything within the sphere of human concerns would be documented and verified and available for perusal by interested parties.  Netiquette demanded that one pour over the FAQs before entering the conversational fray, so the same ground wouldn&#39;t be endlessly rehashed.  Approval from one&#39;s peers in the group came from generating new and novel things to add to the FAQ, and all were happy.



This, of course, summarizes thoughts coming from a Freshman compsci student getting his first relatively unfettered access to the internet, gushing about everything.  On the other hand, I have many of the above enthusiasms for the Semantic Web&#39;s promises.  In a few years, I expect that my enthusiasm will be more even, yet at the same time, I expect there still to be some real uses and benefits to this stuff stabilizing out of it all.  Hopefully, it doesn&#39;t get obliterated by spam before then, like Usenet, like email, and now (but hopefully not) in-blog discussions.</description>
          
          <pubDate>Wed, 12 Nov 2003 18:35:46 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/11/12/The-Whuffie-Web-II/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/11/12/The-Whuffie-Web-II/</guid>
        </item><item>
          <title>As a child, I would have teased Mark Pilgrim</title>
          <description>I see that Mark Pilgrim has posted a picture of himself as a kid, working at an Apple //e.  Based on what I wrote this past Summer about being Newly Digital in 1983, I would guess that around the same time I was working on a Commodore 64, and I would have teased him in a relentlessly geeky way about his clearly inferior machine.</description>
          
          <pubDate>Wed, 12 Nov 2003 17:48:36 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/11/12/as-a-child-64/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/11/12/as-a-child-64/</guid>
        </item><item>
          <title>How about a demo of the Whuffie Web?</title>
          <description>Let&#39;s do a demo of the Semantic Web, the real one, the one that exists today. Doc Searls has a question about  the iQue 3600 hand-held GPS. It is sexy. They say it only works with Windows, but Doc thinks it probably works with Linux too. A couple of thousand really smart people will read this. I&#39;m sure one of them knows the answer. Probably more than one. There&#39;s the query. Human intelligence is so under-rated by computer researchers, but when we do our job well, that&#39;s what we facilitate. Human minds communicating with other human minds. What could be easier to understand?
Source:Scripting News


Well, I certainly wouldn&#39;t call this the Semantic Web-- more like the Whuffie Web.  See, if we were all A-List bloggers, with our own constellations of readers willing to pitch in to answer a question, we could all make queries like the above.  A-List bloggers have the big Whuffie.  Most everyone else has much less Whuffie, thus their query powers are much less.  I somehow doubt that the Whuffie Web, if it were to take off in a big way, would work to equal benefit for everyone.  A cousin, the Lazyweb, sometime serves its petitioners well, but it&#39;s a fickle and unpredictable thing indeed.  Sometimes you get magic, sometimes you get shrugs.  This also links into the Whuffie Web, in that Lazyweb contributors will be more likely to service a request if it comes from a Big Time Blogger.  It&#39;s all about the Whuffie exchange.



On the other hand, if this Semantic Web thing were to take off, it&#39;d benefit anyone who could lay hands on the connectivity to acquire the data, and the CPU power to churn through it.  The data itself could come from anyone with the connectivity to provide the data, and the brain power to create and assemble it from information and knowledge.  No underestimation of human intelligence here.  If anything, it&#39;s an attempt to better respect the exercise human intelligence, to conserve it, and make it more available.  Were the Semantic Web to take off in a big and easy to use way, people could spend more time creating answers and less time answering questions, since the machines do the job of fielding the questions themselves.



Of course... without the Whuffie, where&#39;s the motivation to provide the data?</description>
          
          <pubDate>Tue, 11 Nov 2003 03:14:00 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/11/11/the-whuffie-web/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/11/11/the-whuffie-web/</guid>
        </item><item>
          <title>Reviews in RSS feeds</title>
          <description>The RVW specification is a module extension to the RSS 2.0 syndication format. RVW is intended to allow machine-readable reviews to be integrated into an RSS feed, thus allowing reviews to be automatically compiled from distributed sources.  In other words, you can write book, restaurant, movie, product, etc. reviews inside your own website, while allowing them to be used by Amazon or other review aggregators.
Source:Blogware Implements Distributed Reviews


Aww, yeah.  Bring on the microcontent.  Yay, hooray!  This is an XML namespace-based extension to RSS 2.0, and for even more flavor, it uses the work of other pre-existing specs, such as ENT, FOAF, and Dublin Core.  This wouldn&#39;t be hard at all to slip into an RSS 1.0 feed and an RDF database as well.</description>
          
          <pubDate>Mon, 03 Nov 2003 15:07:43 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/11/03/reviews-in-rdf/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/11/03/reviews-in-rdf/</guid>
        </item><item>
          <title>Panther, forgotten connections, and no more lockups</title>
          <description>Oh yeah, and, just noticed this upon arriving at work.  In the past 6 months, forgotten mounted shares and the subsequent filesystem-related lockups and beach-ball-spinnings in Jaguar have been my sole reason for reboot.


As it would happen, I forgot to disconnect from shares on my home LAN again, and awoke my PowerBook on the work LAN.  Before Panther, this would have lead to a reboot within 10 minutes.  This time, it did the Right Thing.  Yay hooray!


Oh, and the Grab application works for individual windows now-- something which seemed to always be greyed out before.</description>
          
          <pubDate>Wed, 29 Oct 2003 14:14:03 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/10/29/panther-and-forgotten-connections/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/10/29/panther-and-forgotten-connections/</guid>
        </item><item>
          <title>Late to the Panther party</title>
          <description>I know I&#39;m late to the blogosphere release party for Panther, but I just got it last night and, biting the bullet, installed it with only minimal effort toward backing things up.  I intend to eventually wipe this PowerBook completely and install fresh, but I couldn&#39;t wait.  :)


Mark Pilgrim published the most definitive coverage of the beastie I&#39;ve seen yet, with help of the denizens of #joiito to manage the onslaught of readers.  So, I won&#39;t make any attempt to catalog the new things.


A few impressions though:


Everything feels faster.  Windows slide around and resize like they&#39;ve been waxed underneath.  Things seem to launch faster.
A few small things have improved, like System Preferences quitting when I close the window, rather than hanging around waiting for me to open the window again or quit.
Some third-party extension I had installed threw Finder into a launch-and-crash loop for awhile.  So, if you&#39;ve yet to install, try to purge your system of extensions first.  This should be obvious, but is sometimes a surprise when it&#39;s actually a problem.
Expose looked like a neat feature when I first heard of it.  I fully expected it to be slow, stuttery, and &#39;cute&#39; when I finally played with it.  Now, having used it and slowly incorporating it into my usage habits, it&#39;s amazing.  Smooth and not stuttery at all, it looks like a computer interface feature from the movies.
Fast user switching, where desktops rotate in and out of view, also looks like you wish it would, and seems like it&#39;s from the movies.
I hate metal.
I hate metal.
I hate metal.


That is all.  For now.  Maybe.</description>
          
          <pubDate>Wed, 29 Oct 2003 13:38:46 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/10/29/late-to-the-panther-party/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/10/29/late-to-the-panther-party/</guid>
        </item><item>
          <title>A thought about the Nokia N-Gage</title>
          <description>After playing with an N-Gage, I think sidetalkin.com is freakin&#39; hillarious.  One thought on this sidetalking thing, though:


At least it keeps the screen from getting all schmutzed.  My Treo 300 screen gets absolutely filthy, due to me pressing the slab up against my head to talk.  Also, there seems to be a defect in the LCD developing, which seems to have something to do with, again, being pressed up against my face.


In most other ways, this thing looks to be a flop...  but the sidetalking thing might just not be such a bad idea.</description>
          
          <pubDate>Wed, 29 Oct 2003 13:06:02 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/10/29/nokia-ngage-schmutz/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/10/29/nokia-ngage-schmutz/</guid>
        </item>
    </channel>
  </rss>