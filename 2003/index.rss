<?xml version="1.0" encoding="UTF-8"?>
  <rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
      <title>blog.lmorchard.com</title>
      <description>It&#39;s all spinning wheels &amp; self-doubt until the first pot of coffee.</description>
      <link>https://lmorchard.github.io/blog.lmorchard.com</link>
      <atom:link href="https://lmorchard.github.io/blog.lmorchard.com/index.rss" rel="self" type="application/rss+xml" />
      <item>
          <title>Trying to imagine hackers of cognition and the infinite</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;I&#39;ve just read Mark Pilgrim&#39;s post, &quot;The infinite hotel&quot;, which I&#39;m sure I&#39;ll need to re-read a few times and chase down references to read.  Also I&#39;m reading Gödel, Escher, Bach again for the third time, since I first read it in high school and needed corks in my ears to prevent brain slurry from spilling out.  I really need to read more of this sort of thing, refresh myself on all the math I took in college, and explore some of this really abstract stuff.


Something I&#39;ve been musing about lately, without any real novel ideas or insights, is about the history of computation and these thinking machines.  Not history in terms of events and when, but in terms of the concepts and discoveries leading up to keyboards, screens, and code today.  Thinking about things like recursion, and sets, and logic, and all the patterns and revolutions in thought that are the basis for everyday business and life today.


I&#39;ve been trying to imagine the world in each moment where each of these things were new, when these things were worked out in minds and on paper.  When there were no computational engines available to carry out calculations or work out conclusions to logical constructions.  
Today, these discoveries are crystallized in computing architectures, and so geeks hack and play and learn by example.  The construction of the CPU is objective fact, independent of subjective thought or understanding, and the behavior of code demonstrates the laws and rules.  Before, the rules were carefully reasoned out and intuited from observations on the objective universe, but now they&#39;re assimilated by example from mechanically working artifacts.


I&#39;m not sure I&#39;m expressing this very well, or if my thoughts are very well formed altogether, but I&#39;m trying to imagine mental life without readily available, objectively existing computational artifacts with which we can play, without prohibitive investments of effort or time.  No scripting languages with which to just try out logical constructions.  No calculators with which to solve formulae.  All manual, all by hand, all worked out by careful thought and precision.  I&#39;m trying to imagine what geeks like me, as I am today, would be like at a time when everyone dealing in these things was an abstraction astronaut, and there was not really a such thing as that-which-just-works or worse-is-better.  Does this make any sense?


Again, this is not really an expression of anything coherent or novel.  This is mostly me just in awe of how we got here, and trying to get myself above the mode of being just a hacker chasing down the phylogeny of all that&#39;s come before, and into some meta-mode of understanding of the things behind what makes these thinking machines and the thinking itself work.  Maybe after a few more decades of this I&#39;ll have some thoughts worth sharing synthesized from all that I&#39;ve learned.&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Fri, 05 Dec 2003 17:40:00 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/12/05/hacking-infinite-and-cognition/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/12/05/hacking-infinite-and-cognition/</guid>
        </item><item>
          <title>Varied feed polling times versus item urgency in aggregators</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;The problem with varying the polling interval is that the need varies. It&#39;s ok not to poll my little opensource website within 24 hours, but what about the announcements to the civil defence website or local municipal environment alerts, or the nuclear power plant news feed?



Source:Comments on The End of RSS 






Definitely a good point there.  For most of the feeds in my daily habit, what I use is an AIMD variation on my polling frequency per feed based on occurrence of new items.  For feeds with low-frequency but high-urgency items, a different algorithm should come into play.



On the other hand...  should incoming alerts with that much urgency really be conveyed via an architecture driven by polling?  Here&#39;s an excellent case for tying instant messaging systems and pub/sub into the works.&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Wed, 26 Nov 2003 16:02:01 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/11/26/polling-and-urgency/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/11/26/polling-and-urgency/</guid>
        </item><item>
          <title>Didja get that memo?</title>
          
          
          <pubDate>Wed, 26 Nov 2003 14:50:43 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/11/26/the-memo/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/11/26/the-memo/</guid>
        </item><item>
          <title>Publishing Quick Links in blosxom with del.icio.us via xmlstarlet</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;In case anyone is interested in using del.icio.us with blosxom in place of my own BookmarkBlogger, get yourself a copy of xmlstarlet and check out this shell script:

#!/bin/bash

&lt;p&gt;DATE=${1-&lt;code&gt;date +%Y-%m-%d&lt;/code&gt;}
BLOG=&quot;/Users/deusx/desktop/decafbad-entries/links&quot;
FN=&quot;${BLOG}/&quot;&lt;code&gt;echo ${DATE} | sed -e &#39;y/0123456789-/oabcdefghij/&#39;&lt;/code&gt;&quot;.txt&quot;&lt;/p&gt;
&lt;p&gt;curl -s -u deusx:HAHAHA &#39;&lt;a href=&quot;http://del.icio.us/api/posts/get?dt=&#39;$%7BDATE%7D&quot;&gt;http://del.icio.us/api/posts/get?dt=&#39;${DATE}&lt;/a&gt; |&lt;br&gt;    tidy -xml -asxml -q -f /dev/null |&lt;br&gt;    xml sel -t -o &quot;Quick Links&quot; -n &lt;br&gt;            -e &#39;ul&#39;  -m &#39;//post&#39; &lt;br&gt;            -e &#39;li&#39;  -e &#39;a&#39; -a &#39;href&#39; -v &#39;@href&#39; &lt;br&gt;            -b -v &#39;text()&#39; -n  &amp;gt; ${FN}&lt;/p&gt;
&lt;p&gt;touch -d &quot;${DATE} 23:59&quot; ${FN}


You could do this with XSLT, but hacking with a REST-ish &amp;amp; XML producing web service entirely in a shell script seemed oddly appealing to me that week.  Extending this sort of thing to blogging systems other than blosxom is left as an exercise to the reader.


Update: Hmm, looks like one of the blosxom plugins I&#39;m using hates the variables in my code above.  So I stuck curly braces in, which seem to get through okay.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Fri, 21 Nov 2003 14:47:10 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/11/21/delicious-quicklinks/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/11/21/delicious-quicklinks/</guid>
        </item><item>
          <title>Building the Recipe Web III</title>
          
          
          <pubDate>Thu, 20 Nov 2003 16:34:09 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/11/20/recipe-web-3/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/11/20/recipe-web-3/</guid>
        </item><item>
          <title>VoodooPad gets an XML-RPC wiki API</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;You wanted to share the same documents with your coworkers and friends. Now you can.

With VoodooPad 1.1, you can view, edit, and save to any wiki that supports the &#39;vpwiki api&#39;.
Source:Flying Meat Software





&lt;p&gt;Funny, I’ve been tinkering with &lt;a href=&quot;http://www.decafbad.com/twiki/bin/view/Main/XmlRpcToWiki&quot;&gt;a wiki &lt;span class=&quot;caps&quot;&gt;API&lt;/span&gt;&lt;/a&gt; along with a &lt;a href=&quot;http://www.jspwiki.org/Wiki.jsp?page=WikiRPCInterface&quot;&gt;few others tinkerers&lt;/a&gt; for a year or so now.  I wonder if we could get these APIs merged or synched and give VoodooPad access to a slew of wikiware?&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Wed, 19 Nov 2003 01:36:31 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/11/18/voodoo-pad-wiki-api/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/11/18/voodoo-pad-wiki-api/</guid>
        </item><item>
          <title>Building the Recipe Web II</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;Every once in a while, someone gets ideas about crossing recipes and computers. Of course, I love the idea. Two common ideas we hear a lot are 1) to put recipes in XML format and do all sorts of wonderful things and 2) that kitchen appliances should be smart and you should be able to feed them recipes and have your food made for you. They&#39;re both great ideas, but invariably, people underestimate the work involved (&quot;But it&#39;s just a recipe!&quot;) and overestimate the usefulness (&quot;It would be so cool!&quot;).


Source:Troy &amp;amp; Gay





Here’s a good response from someone who knows what he’s talking about when it comes to recipes on the web—he’s one of the contributors to the aforementioned RecipeML format and is part of the team responsible for Recipezaar .  While I think that recipes as syndicated microcontent could be a good thing, Troy makes some important points here.&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Mon, 17 Nov 2003 04:39:12 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/11/16/the-recipe-web-2/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/11/16/the-recipe-web-2/</guid>
        </item><item>
          <title>Building the Recipe Web?</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;RecipeML is a format for representing recipes on computer. It is written in the increasingly popularExtensible Markup Language - XML.

If you run a recipe web site, or are creating a software program -- on any platform -- that works with recipes, then you should consider using RecipeML for coding your recipes! See the FAQs and the new examples for more info.



Source:RecipeML - Format for Online Recipes






So I&#39;m all about this microcontent thing, thinking recently about recipes since reading Marc Canter&#39;s post about them.  Actually, I&#39;ve been thinking about them for a couple of years now, since I&#39;d really like to start cooking some decent meals with the web&#39;s help.  Oh yeah, and I&#39;m a geek, so tinkering with some data would be fun too.


One thing I rarely notice mentioned when ideas like this come up is pre-existing work.  Like RecipeML or even the non-XML MealMaster format.  Both of these have been around for quite a long time, especially so in the case of MealMaster.  In fact, if someone wanted to bootstrap a collection of recipes, you can find a ton (150,000) of MealMaster recipes as well as a smaller archive (10,000) of RecipeML files.  Of course, I&#39;m not sure about the copyright situation with any of these, but it&#39;s a start anyway.


But, the real strength in a recipe web would come from cooking bloggers.  Supply them with tools to generate RecipeML, post them on a blog server, and index them in an RSS feed.  Then, geeks get to work building the recipe aggregators.  Hell, I&#39;m thinking I might even give this a shot.  Since I&#39;d really like to play with some RDF concepts, maybe I&#39;ll write some adaptors to munge RecipeML and MealMaster into RDF recipe data.  Cross that with FOAF and other RDF whackyness, and build an empire of recipe data.


The thing I wonder, though, is why hasn&#39;t anyone done this already?  And why hasn&#39;t anyone really mentioned much about what&#39;s out there already like RecipeML and MealMaster?  It seems like the perfect time to add this into the blogosphere.&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Fri, 14 Nov 2003 23:51:23 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/11/14/the-recipe-web/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/11/14/the-recipe-web/</guid>
        </item><item>
          <title>The Whuffie Web II</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;What I believe we are seeing is domain experts seeking each other out.  Crossing organizational and philosophical boundaries.


Source:Sam Ruby: Whuffie Web





...someone that&#39;s G-list globally might be A-list amongst pet owners.


Source:Danny Ayers: Whuffie Web






A very, very good point that I&#39;d missed at first thought about the Whuffie Web.  There&#39;s a matter of scale involved here, where the relative A&#39;s through Z&#39;s are completely different given your choice of grouping.  And, where choice of grouping is around topic area, the world&#39;s a bit of a smaller place and getting your questions answered is likely much easier.  Especially if you&#39;ve built up some Whuffie in that domain area by generating some useful answers and knowledge yourself.  For newcomers to a domain of knowledge, who have lesser stockpiles of Whuffie, they&#39;ll hopefully be fortunate enough to find much of what they&#39;re looking for chronicled in the archives of blogs of those who&#39;ve come before.  When they don&#39;t, though, it can still be a frustrating experience.



But, semantic web tech in and of itself doesn&#39;t solve the problem where data or knowledge is missing altogether.  How could it?  So, although I was a bit dismissive at first thought about what Dave Winer wrote, he nonetheless has a good point.  Even if the semantic web were richly populated with data and running in full swing, it would still be missing large swaths of Things People Know.  And, well, the thing to use in that case is-- wait for it-- People Who Know Things.  And the way you hopefully can get to them is by being nice and interesting, then blog the answers or ask the people answering your query to blog it themselves.  Then, hopefully, we have blogging tools which can do the bits of pre-digestion to allow that knowledge to be accessed via semantic web machinery to fill in the gaps.



This all takes me back to when I first encountered Usenet in my Freshman year of college, and became instantly enamoured with FAQs.  It seemed like there was a FAQ for everything: coffee, anime, meditation, Baha&#39;i faith, Objectivism, and hedgehogs.  It seems mighty naive to me now, but at the time, I so thought that this was the modern knowledge factory.  Through the contentious and anal bickerings of discussion threads on Usenet, and the subsequent meticulous maintenance of FAQ files, every trivial bit about everything within the sphere of human concerns would be documented and verified and available for perusal by interested parties.  Netiquette demanded that one pour over the FAQs before entering the conversational fray, so the same ground wouldn&#39;t be endlessly rehashed.  Approval from one&#39;s peers in the group came from generating new and novel things to add to the FAQ, and all were happy.



This, of course, summarizes thoughts coming from a Freshman compsci student getting his first relatively unfettered access to the internet, gushing about everything.  On the other hand, I have many of the above enthusiasms for the Semantic Web&#39;s promises.  In a few years, I expect that my enthusiasm will be more even, yet at the same time, I expect there still to be some real uses and benefits to this stuff stabilizing out of it all.  Hopefully, it doesn&#39;t get obliterated by spam before then, like Usenet, like email, and now (but hopefully not) in-blog discussions.&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Wed, 12 Nov 2003 18:35:46 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/11/12/The-Whuffie-Web-II/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/11/12/The-Whuffie-Web-II/</guid>
        </item><item>
          <title>As a child, I would have teased Mark Pilgrim</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;I see that Mark Pilgrim has posted a picture of himself as a kid, working at an Apple //e.  Based on what I wrote this past Summer about being Newly Digital in 1983, I would guess that around the same time I was working on a Commodore 64, and I would have teased him in a relentlessly geeky way about his clearly inferior machine.&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Wed, 12 Nov 2003 17:48:36 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/11/12/as-a-child-64/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/11/12/as-a-child-64/</guid>
        </item><item>
          <title>How about a demo of the Whuffie Web?</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;Let&#39;s do a demo of the Semantic Web, the real one, the one that exists today. Doc Searls has a question about  the iQue 3600 hand-held GPS. It is sexy. They say it only works with Windows, but Doc thinks it probably works with Linux too. A couple of thousand really smart people will read this. I&#39;m sure one of them knows the answer. Probably more than one. There&#39;s the query. Human intelligence is so under-rated by computer researchers, but when we do our job well, that&#39;s what we facilitate. Human minds communicating with other human minds. What could be easier to understand?


Source:Scripting News








Well, I certainly wouldn&#39;t call this the Semantic Web-- more like the Whuffie Web.  See, if we were all A-List bloggers, with our own constellations of readers willing to pitch in to answer a question, we could all make queries like the above.  A-List bloggers have the big Whuffie.  Most everyone else has much less Whuffie, thus their query powers are much less.  I somehow doubt that the Whuffie Web, if it were to take off in a big way, would work to equal benefit for everyone.  A cousin, the Lazyweb, sometime serves its petitioners well, but it&#39;s a fickle and unpredictable thing indeed.  Sometimes you get magic, sometimes you get shrugs.  This also links into the Whuffie Web, in that Lazyweb contributors will be more likely to service a request if it comes from a Big Time Blogger.  It&#39;s all about the Whuffie exchange.



On the other hand, if this Semantic Web thing were to take off, it&#39;d benefit anyone who could lay hands on the connectivity to acquire the data, and the CPU power to churn through it.  The data itself could come from anyone with the connectivity to provide the data, and the brain power to create and assemble it from information and knowledge.  No underestimation of human intelligence here.  If anything, it&#39;s an attempt to better respect the exercise human intelligence, to conserve it, and make it more available.  Were the Semantic Web to take off in a big and easy to use way, people could spend more time creating answers and less time answering questions, since the machines do the job of fielding the questions themselves.



Of course... without the Whuffie, where&#39;s the motivation to provide the data?&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Tue, 11 Nov 2003 03:14:00 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/11/10/the-whuffie-web/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/11/10/the-whuffie-web/</guid>
        </item><item>
          <title>Reviews in RSS feeds</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;The RVW specification is a module extension to the RSS 2.0 syndication format. RVW is intended to allow machine-readable reviews to be integrated into an RSS feed, thus allowing reviews to be automatically compiled from distributed sources.&amp;nbsp; In other words,&amp;nbsp;you can write book, restaurant, movie, product, etc.&amp;nbsp;reviews inside your own&amp;nbsp;website, while allowing them to be used by Amazon or other review aggregators.


Source:Blogware Implements Distributed Reviews






Aww, yeah.  Bring on the microcontent.  Yay, hooray!  This is an XML namespace-based extension to RSS 2.0, and for even more flavor, it uses the work of other pre-existing specs, such as ENT, FOAF, and Dublin Core.  This wouldn&#39;t be hard at all to slip into an RSS 1.0 feed and an RDF database as well.&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Mon, 03 Nov 2003 15:07:43 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/11/03/reviews-in-rdf/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/11/03/reviews-in-rdf/</guid>
        </item><item>
          <title>Panther, forgotten connections, and no more lockups</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;Oh yeah, and, just noticed this upon arriving at work.  In the past 6 months, forgotten mounted shares and the subsequent filesystem-related lockups and beach-ball-spinnings in Jaguar have been my sole reason for reboot.


As it would happen, I forgot to disconnect from shares on my home LAN again, and awoke my PowerBook on the work LAN.  Before Panther, this would have lead to a reboot within 10 minutes.  This time, it did the Right Thing.  Yay hooray!


Oh, and the Grab application works for individual windows now-- something which seemed to always be greyed out before.&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Wed, 29 Oct 2003 14:14:03 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/10/29/panther-and-forgotten-connections/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/10/29/panther-and-forgotten-connections/</guid>
        </item><item>
          <title>Late to the Panther party</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;I know I&#39;m late to the blogosphere release party for Panther, but I just got it last night and, biting the bullet, installed it with only minimal effort toward backing things up.  I intend to eventually wipe this PowerBook completely and install fresh, but I couldn&#39;t wait.  :)


Mark Pilgrim published the most definitive coverage of the beastie I&#39;ve seen yet, with help of the denizens of #joiito to manage the onslaught of readers.  So, I won&#39;t make any attempt to catalog the new things.


A few impressions though:


Everything feels faster.  Windows slide around and resize like they&#39;ve been waxed underneath.  Things seem to launch faster.
A few small things have improved, like System Preferences quitting when I close the window, rather than hanging around waiting for me to open the window again or quit.
Some third-party extension I had installed threw Finder into a launch-and-crash loop for awhile.  So, if you&#39;ve yet to install, try to purge your system of extensions first.  This should be obvious, but is sometimes a surprise when it&#39;s actually a problem.
Expose looked like a neat feature when I first heard of it.  I fully expected it to be slow, stuttery, and &#39;cute&#39; when I finally played with it.  Now, having used it and slowly incorporating it into my usage habits, it&#39;s amazing.  Smooth and not stuttery at all, it looks like a computer interface feature from the movies.
Fast user switching, where desktops rotate in and out of view, also looks like you wish it would, and seems like it&#39;s from the movies.
I hate metal.
I hate metal.
I hate metal.




That is all.  For now.  Maybe.&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Wed, 29 Oct 2003 13:38:46 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/10/29/late-to-the-panther-party/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/10/29/late-to-the-panther-party/</guid>
        </item><item>
          <title>A thought about the Nokia N-Gage</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;After playing with an N-Gage, I think sidetalkin.com is freakin&#39; hillarious.  One thought on this sidetalking thing, though:


At least it keeps the screen from getting all schmutzed.  My Treo 300 screen gets absolutely filthy, due to me pressing the slab up against my head to talk.  Also, there seems to be a defect in the LCD developing, which seems to have something to do with, again, being pressed up against my face.


In most other ways, this thing looks to be a flop...  but the sidetalking thing might just not be such a bad idea.&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Wed, 29 Oct 2003 13:06:02 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/10/29/nokia-ngage-schmutz/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/10/29/nokia-ngage-schmutz/</guid>
        </item>
    </channel>
  </rss>