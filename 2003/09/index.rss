<?xml version="1.0" encoding="UTF-8"?>
  <rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
      <title>blog.lmorchard.com</title>
      <description>It&#39;s all spinning wheels &amp; self-doubt until the first pot of coffee.</description>
      <link>https://lmorchard.github.io/blog.lmorchard.com</link>
      <atom:link href="https://lmorchard.github.io/blog.lmorchard.com/index.rss" rel="self" type="application/rss+xml" />
      <item>
          <title>Dynamic polling times for news aggregators, II</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;Okay, so that thing with the SQL I did Friday?
I&#39;m not exactly sure what I was thinking with it.  I was doing something
that seems really odd now, trying to collect counts of new items together
by hour, then averaging those hourly counts across a week.  Instead, I&#39;m
trying this now:


SELECT
  source,
  &#39;update_period&#39; AS name,
  round(min(24,max(1,(max(1,(iso8601_to_epoch(max(created)) -
    max(now() - (7*24*60*60), iso8601_to_epoch(min(created)))) /
   (60*60))) / count(id))),2) AS value
FROM
  items
WHERE
  created &amp;gt;= epoch_to_iso8601(now() - (7*24*60*60)) 
GROUP BY
  source


This bit of SQL, though still ugly, is much simpler.  This leaves out
the subselect, which I think I might have been playing with in order
to build a little graph display of new items over time by source.  What
the above does now is to get an average time between new items for the
past week, with a minimum of an hour, and a maximum of a day.  This
seems to be working much better.



An alternate algorithm I&#39;ve been playing with was suggested in
a comment
by Gnomon,
inspired by TCP/IP&#39;s Additive Increase / Multiplicative Decrease.
With this, I subtract an hour from the time between polls when a
poll finds new items, and then multiply by 2 every time a poll
comes up with nothing new.



Using the average of new items over time lessens my pummeling
of servers per hour, but the second approach is even lighter
on polling since it&#39;s biased toward large leaps backing off
from polling when new items are not found.  I&#39;ll likely be trading
off between the two to see which one seems to work best.



Hoping that, after playing a bit, I&#39;ll settle on one and my
aggregator will play much nicer with feeds, especially once
I get the HTTP client usage to correctly use things like
last-modified headers and ETags.  There&#39;s absolutely no reason
for a news aggregator to poll a feed every single hour of a day,
unless you&#39;re monitoring a feed that&#39;s mostly quiet, except
for emergencies.  In that case, well, a different polling
algorithm is needed, or maybe an instant messaging or pub/sub
architecture is required.



Update: As Gnomon
has corrected me in comments, I&#39;ve got the AIMD algorithm mixed up.
What I really should be doing is making quick jumps up in polling
frequency in response to new items (multiplicative decrease of
polling period) and creeping away in response to no new items
(additive increase of polling period).  As he notes, this approach
should make an aggregator jump to attention when clumps of new
posts come in, and gradually get bored over periods of silence.
I&#39;ve adjusted my code and will be tinkering with it.



Also, although Gnomon makes
a good point that bloggers and their posting habits are not easily
subject to statistical analysis,
I&#39;ve further refined my little SQL query to catch sources
which haven&#39;t seen any updates during the week (or ever):


SELECT 
  id as source,
  &#39;update_period&#39; AS name,
  round(min(24,max(1,coalesce(update_period,24)))) AS value
FROM sources
LEFT JOIN (
     SELECT
      source AS source_id,
            (iso8601_to_epoch(max(created)) -
              max(
                now()-(7*24*60*60),
                iso8601_to_epoch(min(created))
              )
            ) / (60*60) / count(id)
        AS update_period
    FROM items
    WHERE created &amp;gt;= epoch_to_iso8601(now() - (7*24*60*60)) 
    GROUP BY source
) ON sources.id=source_id


Also, in case anyone&#39;s interested, I&#39;ve checked all the above
into CVS.  This beastie&#39;s far from ready for prime time, but it
might be interesting to someone.&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Mon, 29 Sep 2003 17:48:28 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/09/29/dynamic-polling-freq-too/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/09/29/dynamic-polling-freq-too/</guid>
        </item><item>
          <title>Dynamic feed polling times for news aggregators</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;Today, my aggregator got
the following SQL worked into its feed poll scheduling machinery:

SELECT id as source,
       &#39;update_period&#39; as name,
       max(1, 1/max((1.0/24.0),
                    sum(update_count)/(7*24))) AS value 
FROM sources 
LEFT JOIN (
    SELECT source AS count_id,
                round(iso8601_to_epoch(created)/(60*60)) AS hour, 
                count(id) AS update_count 
    FROM items 
    WHERE created&amp;gt;epoch_to_iso8601(now()-(7*(24*60*60))) 
    GROUP BY hour
) ON id=count_id
GROUP BY source
ORDER BY value


It&#39;s likely that this is really nasty, but I have only a street-level
working knowledge of SQL.  Also, a few of the date functions are
specific to how I&#39;ve extended sqlite in Python.  It works though, and
what it does is this:



For each feed to which I&#39;m subscribed, work out
an average time between updates for the past week, with a maximum
period of 24 hours and a minimum of 1 hour.



My aggregator does this daily, and uses the results to determine how
frequently to schedule scans.  In this way, it automatically backs off
on checking feeds which update infrequently, and ramps up its polling
of more active feeds.  This shortens my feed downloading and scanning
time, and is kinder in general to everyone on my subscription list.



Next, among other things, I have to look into making sure that the
HTTP client parts of this beast pass all the
aggregator client
HTTP tests that Mark
Pilgrim put together.



Update: Well, it seemed like a good idea, anyway.  But, on
further examination, it has flaws.  The most notable is that it
assumes a polling frequency of once per hour.  This works right up
until I start changing the polling frequency with the results of the
calculation.  I haven&#39;t poked at it yet, but maybe if I take this
into account, it&#39;ll be more accurate.



On the other hand, I&#39;ve also been thinking about a much simpler
approach to ramping polling frequency up and down:  Start out at
a poll every hour.  If, after a poll, no new items are found,
double the time until the next poll.  If new items were found,
halve the time until the next poll.


Provide lower and upper limits to this, say between 1 hour and 1
week.  Also, consider the ramp up and ramp down factor as a variable
setting too.  Instead of a factor of 2, maybe try 1.5 or even 1.25 for
a more gradual change.  To go even further, I wonder if it would be
valuable to dynamically alter this factor itself, to try to get the
polling time zeroed in on a realistic polling time.



Okay.  There the simpler approach leaves simplicity.  I&#39;m sure there&#39;s
some decently elegant math that could be pulled in here.  :)&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Fri, 26 Sep 2003 02:45:29 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/09/25/dynamic-feed-scan-times/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/09/25/dynamic-feed-scan-times/</guid>
        </item><item>
          <title>Atom is its Name-O?</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;I would like to propose, nay, admonish, that the name of the format and spec
should be Atom, that the current naming vote should be killed, and we should
move on to grander things without the auspices of &quot;what&#39;s it called?!&quot; over
our heads. This has been going on far too long.


Source:Morbus Iff: &#39;Atom&#39; Should Be It&#39;s Name, and It&#39;s Name Was Atom





I haven&#39;t been anywhere near the epicenter of Atom/Pie/Echo much,
so this is mostly a &#39;me too&#39; kind of posting.  But, you know, as an
interested hacker waiting for dust to settle before I start paying
much attention, the decision on a name, as superficial as it is,
seems telling to me.

On one hand, I could take it to be representative of what&#39;s going
on inside the project as a whole.  (If they can&#39;t settle on a name,
how can they settle on what&#39;s included in the spec?)  On the other hand,
it could just be that naming the thing is the least interesting aspect
of the project.  But I consider that because I&#39;m a nerd, I&#39;ve been
there, and I want to see the project thrive.  Others might not be so
charitable or patient. :)

So just name the dang thing Atom already.&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Thu, 25 Sep 2003 13:31:01 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/09/25/atom-is-its-name-o/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/09/25/atom-is-its-name-o/</guid>
        </item><item>
          <title>Feedback loops and syndication</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;Enter attention.xml. Of course it monitors my attention list, noting what feeds are in what order. Then it pays attention to what items I read, in what order, or if not, then what feeds I scan, and for how long. The results are packaged up in an attention.xml file and shipped via some transport (RSS, FTP, whatever) to Technorati. Dave has some ideas about what he will provide in return: &quot;If you liked these feeds and items, then here are some ones you don&#39;t know about that you may want to add to your list.&quot;

But the real power comes in a weighted return feed that works like this: OK, I see who you think is important and what posts are most relevant to your interests. Then we factor in their attention.xml lists weighted by their location on your list, average the newly weighted list based on this trusted group of &quot;advisors&quot;, and return it to your aggregator, which rewrites the list accordingly.
Source: Steve Gillmor&#39;s Emerging Opps



Dave Winer says this guy’s full of shit.  I’m not sure why, or it if’s sarcasm.  In a lot of ways, what Steve Gilmore wrote about sounds like syndicating whuffie and what Gary Lawrence Murphy of TeledyN wrote about republishing RSS items read and rated from one’s news aggregator.

&lt;p&gt;Sounds like the next one of the next steps this tech needs to take to hit a new level of intelligence, forming a minimum-effort feedback loop from writers to readers and between readers themselves.  What did I read today, and was it interesting? What did you read today, and was it interesting?  What did we both read and both find interesting?  What did you read, and find interesting, that I didn’t read and &lt;strong&gt;might&lt;/strong&gt; find interesting?  And then, back around to the author again, what of your writings was found very interesting, and (maybe) by whom?&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Sun, 21 Sep 2003 19:54:42 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/09/21/rss-feedback/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/09/21/rss-feedback/</guid>
        </item><item>
          <title>Flash MX Hates Progressive JPEGs</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;Okay, I may be the last person fiddling with Flash to
discover this, but here&#39;s what I&#39;ve learned today:


Flash MX hates progressive JPEGs.


From the above: &quot;The Macromedia Flash Player does not have a
decompressor for progressive JPEG images, therefore files of this type
cannot be loaded dynamically and will not display when using the
loadMovie action.&quot;


This would have been nice to know, hours ago.  Or maybe fixed in
the past year or so since the above linked tech note.  See, although
I&#39;m a Jack of a lot of Trades, I don&#39;t really pay attention much
to things like JPEGs and their progressive natures.  It wasn&#39;t
until I finally started randomly clicking buttons on and off in
Macromedia Fireworks while exporting a test JPEG that I finally
narrowed down the problem.


This was after a day worth of examining ActionScript, XML data,
HTTP headers, and a mess of other random dead ends.  And a lot of
last-ditch random and exhaustive twiddling of checkboxes and
options.


Then, once I had the words I
wouldn&#39;t have had unless I already knew what my problem was, a Google search for
&quot;flash progressive jpeg&quot;
got me all kinds of info.


Problem is, the JPEGs supplied to the particular Flash app on which
I&#39;m hacking come from a random assortment of people working through
a content management system on the backend.  They upload them
with a form in their browser, and this Flash app gets a URL to the
image via an XML doc it loads.  Me, I&#39;m probably in bed when this
happens.  I&#39;d love to have tested every one... er, rather, no I
wouldn&#39;t.


So... Now I just have to figure out how to get all these people
to start making sure that their JPEGs aren&#39;t progressive.  Hmph.


I can only hope that this message gets indexed and maybe provides
more triangulation for some other poor sucker in the future.&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Fri, 19 Sep 2003 18:28:23 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/09/19/flash-hates-progressive-jpeg/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/09/19/flash-hates-progressive-jpeg/</guid>
        </item><item>
          <title>Don&#39;t copy that floppy, or cracked software strikes back</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;* Orangerobot uses cracked software.  I will respond to the following
commands: !ame &lt;msg&gt;, !amsg &lt;msg&gt;, !quit &lt;msg&gt;,
!open_cd, !switch_my_mouse_buttons

&lt;deusx&gt; Hmm.  If what Orangerobot just emoted is true, that&#39;s
funny as hell.

&lt;deusx&gt; !amsg Wang!

&lt;orangerobot&gt; Wang!

&lt;anitar&gt; and what&#39;s the purpose?

&lt;deusx&gt; AnitaR: Of the message from Orangerobot?

&lt;anitar&gt; yes

&lt;anitar&gt; must be part of the joke I&#39;m not getting

&lt;anitar&gt; yet

* Orangerobot uses cracked software.  I will respond to the following
commands: !ame &lt;msg&gt;, !amsg &lt;msg&gt;, !quit &lt;msg&gt;,
!open_cd, !switch_my_mouse_buttons

&lt;deusx&gt; AnitaR: Could be a joke, but it appears that this person
is using pirated software that&#39;s detected its illegitimacy and is
allowing us to manipulate that user&#39;s computer.

&lt;adamhill&gt; or its a social experiment by the person behind OR :)

&lt;deusx&gt; adamhill: Or that. :)  Either way, it&#39;s fun

&lt;anitar&gt; I&#39;m glad it isn&#39;t one of those experiments that tests
how strong a shock we&#39;ll give the owner

&lt;argyle&gt; ?def orangerobot

&lt;deusx&gt; Some googling points to this software:
http://www.klient.com

&lt;deusx&gt; !switch_my_mouse_buttons

&lt;deusx&gt; !ame likes cheddar cheese.

* Orangerobot likes cheddar cheese.

&lt;adamhill&gt; ?learn Orangerobot is either a person using cracked
software or a social experiment by a demented psych student

&lt;jibot&gt; I understand now, Dr. Chandra; orangerobot is either a
person using cracked software or a social experiment by a demented
psych student

&lt;deusx&gt; !open_cd

&lt;deusx&gt; okay, I&#39;m done.

* Orangerobot uses cracked software.  I will respond to the following
commands: !ame &lt;msg&gt;, !amsg &lt;msg&gt;, !quit &lt;msg&gt;,
!open_cd, !switch_my_mouse_buttons

&lt;deusx&gt; !quit hush.

&amp;lt;-- Orangerobot has quit (&quot;hush.&quot;)&lt;/deusx&gt;&lt;/msg&gt;&lt;/msg&gt;&lt;/msg&gt;&lt;/deusx&gt;&lt;/deusx&gt;&lt;/jibot&gt;&lt;/adamhill&gt;&lt;/deusx&gt;&lt;/deusx&gt;&lt;/deusx&gt;&lt;/argyle&gt;&lt;/anitar&gt;&lt;/deusx&gt;&lt;/adamhill&gt;&lt;/deusx&gt;&lt;/msg&gt;&lt;/msg&gt;&lt;/msg&gt;&lt;/anitar&gt;&lt;/anitar&gt;&lt;/anitar&gt;&lt;/deusx&gt;&lt;/anitar&gt;&lt;/orangerobot&gt;&lt;/deusx&gt;&lt;/deusx&gt;&lt;/msg&gt;&lt;/msg&gt;&lt;/msg&gt;&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Fri, 12 Sep 2003 19:35:25 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/09/12/dont-copy-software/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/09/12/dont-copy-software/</guid>
        </item><item>
          <title>An API for Wikis?  Here&#39;s one.</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;Some folks are experimenting with using Wiki to build websites.  I particularly like what Matt Haughey did with PHPWiki and a bit of CSS magic dust.  Looks nice, eh?  [Via Seb&#39;s Wikis are Ugly? post at Corante]



Janne Jalkanen&#39;s Wiki-based Weblog is interesting too.  Hmm.  Maybe blog API(s) can be used for Wikis too.  That reminds me, shouldn&#39;t Wiki formatted text have their own MIME type?  Is there one?  &quot;text/wiki&quot;?  For now, different dialects of Wiki formatting rules will have to be accounted for like &quot;text/wiki+moinmoin&quot;.
Source: Don Park&#39;s Daily Habit







It&#39;s been a while since I last worked on it, but I did implement an
XML-RPC API on a few wikis, called XmlRpcToWiki.  Janne Jalkanen
did a lot of work toward the same interface with JSPWiki.  I use this API
in the linkage between my blog and the wiki on this site.  Now that
I&#39;ve drifted away from XmlRpc a bit and am more in favor of simpler
REST-ish web service APIs, I&#39;d like to see something more toward that
end.  Seems like a lot of people are discovering or rediscovering
wikis since the introduction of Sam Ruby&#39;s wiki for Atom/Echo/Pie
work, so it&#39;s interesting to see a lot of things come up again like
grousing about APIs and mutant wiki-format offshoots and standards.&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Sat, 06 Sep 2003 17:50:45 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/09/06/wiki-apis/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/09/06/wiki-apis/</guid>
        </item><item>
          <title>White Hat Worms and robots.txt?</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;Or maybe it&#39;s time to release our own Defender.A worm which could invasively close down the relevant &quot;holes&quot; in Internet security. A defensive worm could use standard intrusion tactics for benign result. For example, it could worm it&#39;s way into Windows XP computers and get the owner&#39;s permission to turn their firewalls on. It could survey open TCP/IP ports and offer to close them.

Source: Superworm To Storm The Net On 9/11 (via KurzweilAI)



So, anger is my first reaction to the idea of any unwelcome visitors on any of my machines, well intentioned or not.  I’m sure that there aren’t many who wouldn’t feel the same way.  But, although a lot of us try to keep up on patches and maintain decent security, there’s the “great unwashed masses” who just want to “do email“.

&lt;p&gt;On one hand, it’s easy to say, “Tough.  Learn the care &amp;amp; feeding of your equipment.”  Yeah, as if that will help or get any response from all the people who’ve bought into &lt;span class=&quot;caps&quot;&gt;AOL&lt;/span&gt; and have been reassured for years that computers are friendly and easy beasts (despite their intuitions to the contrary).  Hell, I’d bet that, more often than not, the same person who gets regular oil changes and tune-ups for the car has no idea how to do the equivalent for a computer (or that it even needs it).  Cars have been positioned differently than computers.  No one expects a Spanish Inquisition when they live in a virtual preschool of a user interface with large and colorful buttons and happy smiling faces.  They know there’s some voodoo going on underneath, but the UI tells them that it’s nothing to worry about (until &lt;a href=&quot;http://www.decafbad.com/blog/geek/not_working.html&quot;&gt;it isn’t working&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Now if the problem was just that stupid users ended up with broken computers, there’d be no problem.  But, like cars with problems waiting to happen (like worn down tires), their users become a hazard to others.  Unlike cars, however, the problems of stupid users’ computers are contagious and self-replicating: every tire blowout becomes a 1000 car pileup.&lt;/p&gt;

&lt;p&gt;It’s like everyone sits on their recliners watching TV in their houses; not even realizing that there are doors to lock; not even hearing the intruders rummaging through the fridge in the kitchen; and certainly not knowing that there’s a guy sleeping on the sofa at night working by day to let his army of clones into the neighbor’s houses.&lt;/p&gt;

&lt;p&gt;So, about what about vigilante “white hat” worms?  Wouldn’t it be nice if there was a guy wandering the neighborhood locking door for the ignorant?  Wouldn’t it be nice if there was a truck driver on the road that forced cars with bald tires off to the side for free tire replacement?  Okay, maybe that’s a bit whacky, but then again, people with bald tires aren’t causing 1000 car pileups.&lt;/p&gt;

&lt;p&gt;I’m thinking that “white hat” virii and worms are one of the only things that will work, since I’m very pessimistic about the user culture changing to be more responsible.  Though, what about a compromise?  Install a service or some indicator on every network-connected machine, somewhat like &lt;a href=&quot;http://www.robotstxt.org/wc/robots.html&quot;&gt;robots.txt&lt;/a&gt; , which tells friendly robots where they‘re welcome and where they‘re not.  Set this to maximum permissiveness for white hat worms as a default.  The good guys infect, fix, and self-destruct unless this indicator tells them to stay out.  Then, all of us who want to take maintenance into our own hands can turn away the friendly assistance of white hat worms.  It’s an honor system, but the white hats should be the honorable ones anyway.  The ones which ignore the no-worms-allowed indicator are hostile by definition.&lt;/p&gt;

&lt;p&gt;So, then, the internet develops an immune system.  Anyone can release a white hat worm as soon as they find an exploit to be nullified, and I’m sure there are lots of geeks out there who’d jump at the chance to play with worms and virii in a constructive way.  And if you want to opt-out of the system, go for it.  Hell…  think of this on a smaller scale as a next-gen anti-virus software.  Instead of internet-wide, just support &lt;span class=&quot;caps&quot;&gt;P2P&lt;/span&gt; networks between installations of your anti-virus product.  When it’s time to close a hole, infect your network with a vaccinating update.  I doubt this would work as well as a fully open system, but might have less controversy.&lt;/p&gt;

&lt;p&gt;Anyway, it’s a whacky idea to a whacky problem that just might work.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Fri, 05 Sep 2003 15:10:53 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/09/05/superworm/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/09/05/superworm/</guid>
        </item><item>
          <title>Litany against meetings, courtesy of purl</title>
          
          
          <pubDate>Thu, 04 Sep 2003 14:50:58 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/09/04/litany-meetings/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/09/04/litany-meetings/</guid>
        </item><item>
          <title>Jibot and purl, distant cousins?</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;What the [#joiito](/tag/joiito) bot knows. I&#39;m dumping it out dynamically with the Twisted webserver, which is all Python too.

Source: Epeus&#39; epigone - Kevin Marks weblog



While the #joiito bot is looking pretty keen, I keep wondering if anyone hacking on it has seen Infobot ?  It’s the brains behind purl, the bot serving [#perl](/tag/perl) channels on a few IRC networks.  Jibot seems to have some funky punctuation-based commands, but purl accepts commands in formulatic english and even picks a few things up from normal channel chatter.  When I look at Kevin Marks’ dump of Jibot’s brains, I can’t help but think of the gigantic factoid packs available for Infobot.&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Thu, 04 Sep 2003 12:57:55 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/09/04/jibot-and-purl/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/09/04/jibot-and-purl/</guid>
        </item><item>
          <title>Another BookmarkBlogger in Python</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;I haven&#39;t been paying attention to my referrers as much lately,
but I probably should.  Because, when I do, I find things like
another implementation
of BookmarkBlogger in Python, this one by
David Edmondson.
His version has many fewer requirements, using only core Python
libraries as far as I can see.  One of these which I hadn&#39;t any idea
existed is
plistlib,
&quot;a tool to generate and parse MacOSX .plist files&quot;.  When I get
another few round tuits, I&#39;ll likely tear out all the XPath use
in my version and replace it with this.  Bummer.  And here I thought
I was all clever using the XPaths like that in Python :)&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Wed, 03 Sep 2003 14:59:31 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/09/03/another-bmblogger/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/09/03/another-bmblogger/</guid>
        </item><item>
          <title>ChangeLog to RSS web service</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;Hanging out on joiito on IRC today,
I read Ecyrd asking
around about any tools to present GNU-style changelogs
as an RSS feed.  I couldn&#39;t find any, but I did find
this changelog parser, apparently
by Jonathan Blandford.  So,
when I had a few free minutes, I took some parts I had laying around, along
with this parser, and made this:

  - Changelog for JSPWiki

 Source code for cl2rss





This is at the &quot;it works&quot; stage.  It needs much work in what it presents
in an RSS feed, so feel free to suggest changes!&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Tue, 02 Sep 2003 17:44:18 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/09/02/cl-to-rss/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/09/02/cl-to-rss/</guid>
        </item><item>
          <title>Using web services and XSLT to scrape RSS from HTML</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;After tinkering a bit with
web services and XSLT-based scraping
last week for generating RSS from HTML, I ripped out some work I was
doing for a Java-based scraper I&#39;d started
working on last year and
threw together a kit of XSLT files that does most everything I was trying
to do.

I&#39;m calling this kit XslScraper, and there&#39;s further blurbage and download links
avaiable in the Wiki.  Check it out.  I&#39;ve got shell scripts to run the stuff
from as a cron job, and CGI scripts to run it all from web services.

For quick gratification, check out these feeds:

  - The Nation (using Bill Humphries&#39; XSL) 

  - KurzweilAI.net

  - J-List -- You&#39;ve got a friend in Japan!

  - New JOBS at the University of Michigan (By Job Family)&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Tue, 02 Sep 2003 04:22:53 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/09/02/xsl-scraper/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/09/02/xsl-scraper/</guid>
        </item>
    </channel>
  </rss>