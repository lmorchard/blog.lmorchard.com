[
  {
    "comments_archived": true,
    "date": "2003-09-29T17:48:28.000Z",
    "layout": "post",
    "title": "Dynamic polling times for news aggregators, II",
    "wordpress_id": 484,
    "wordpress_slug": "dynamic-polling-freq-too",
    "wordpress_url": "http://www.decafbad.com/blog/?p=484",
    "year": "2003",
    "month": "09",
    "day": "29",
    "isDir": false,
    "slug": "dynamic-polling-freq-too",
    "postName": "2003-09-29-dynamic-polling-freq-too",
    "parentPath": "./content/posts/archives/2003",
    "path": "2003/09/29/dynamic-polling-freq-too",
    "summary": "Okay, so that thing with the SQL I did Friday?\nI'm not exactly sure what I was thinking with it.  I was doing something\nthat seems really odd now, trying to collect counts of new items together\nby hour, then averaging those hourly counts across a week.  Instead, I'm\ntrying this now:\n\n\nSELECT\n  source,\n  'update_period' AS name,\n  round(min(24,max(1,(max(1,(iso8601_to_epoch(max(created)) -\n    max(now() - (7*24*60*60), iso8601_to_epoch(min(created)))) /\n   (60*60))) / count(id))),2) AS value\nFROM\n  items\nWHERE\n  created >= epoch_to_iso8601(now() - (7*24*60*60)) \nGROUP BY\n  source\n\n\nThis bit of SQL, though still ugly, is much simpler.  This leaves out\nthe subselect, which I think I might have been playing with in order\nto build a little graph display of new items over time by source.  What\nthe above does now is to get an average time between new items for the\npast week, with a minimum of an hour, and a maximum of a day.  This\nseems to be working much better.\n\n\n\nAn alternate algorithm I've been playing with was suggested in\na comment\nby Gnomon,\ninspired by TCP/IP's Additive Increase / Multiplicative Decrease.\nWith this, I subtract an hour from the time between polls when a\npoll finds new items, and then multiply by 2 every time a poll\ncomes up with nothing new.\n\n\n\nUsing the average of new items over time lessens my pummeling\nof servers per hour, but the second approach is even lighter\non polling since it's biased toward large leaps backing off\nfrom polling when new items are not found.  I'll likely be trading\noff between the two to see which one seems to work best.\n\n\n\nHoping that, after playing a bit, I'll settle on one and my\naggregator will play much nicer with feeds, especially once\nI get the HTTP client usage to correctly use things like\nlast-modified headers and ETags.  There's absolutely no reason\nfor a news aggregator to poll a feed every single hour of a day,\nunless you're monitoring a feed that's mostly quiet, except\nfor emergencies.  In that case, well, a different polling\nalgorithm is needed, or maybe an instant messaging or pub/sub\narchitecture is required.\n\n\n\nUpdate: As Gnomon\nhas corrected me in comments, I've got the AIMD algorithm mixed up.\nWhat I really should be doing is making quick jumps up in polling\nfrequency in response to new items (multiplicative decrease of\npolling period) and creeping away in response to no new items\n(additive increase of polling period).  As he notes, this approach\nshould make an aggregator jump to attention when clumps of new\nposts come in, and gradually get bored over periods of silence.\nI've adjusted my code and will be tinkering with it.\n\n\n\nAlso, although Gnomon makes\na good point that bloggers and their posting habits are not easily\nsubject to statistical analysis,\nI've further refined my little SQL query to catch sources\nwhich haven't seen any updates during the week (or ever):\n\n\nSELECT \n  id as source,\n  'update_period' AS name,\n  round(min(24,max(1,coalesce(update_period,24)))) AS value\nFROM sources\nLEFT JOIN (\n     SELECT\n      source AS source_id,\n            (iso8601_to_epoch(max(created)) -\n              max(\n                now()-(7*24*60*60),\n                iso8601_to_epoch(min(created))\n              )\n            ) / (60*60) / count(id)\n        AS update_period\n    FROM items\n    WHERE created >= epoch_to_iso8601(now() - (7*24*60*60)) \n    GROUP BY source\n) ON sources.id=source_id\n\n\nAlso, in case anyone's interested, I've checked all the above\ninto CVS.  This beastie's far from ready for prime time, but it\nmight be interesting to someone.",
    "prevPostPath": "2003/10/01/mailbucket-feeds",
    "nextPostPath": "2003/09/26/dynamic-feed-scan-times"
  },
  {
    "comments_archived": true,
    "date": "2003-09-26T02:45:29.000Z",
    "layout": "post",
    "title": "Dynamic feed polling times for news aggregators",
    "wordpress_id": 483,
    "wordpress_slug": "dynamic-feed-scan-times",
    "wordpress_url": "http://www.decafbad.com/blog/?p=483",
    "year": "2003",
    "month": "09",
    "day": "25",
    "isDir": false,
    "slug": "dynamic-feed-scan-times",
    "postName": "2003-09-25-dynamic-feed-scan-times",
    "parentPath": "./content/posts/archives/2003",
    "path": "2003/09/26/dynamic-feed-scan-times",
    "summary": "Today, my aggregator got\nthe following SQL worked into its feed poll scheduling machinery:\n\nSELECT id as source,\n       'update_period' as name,\n       max(1, 1/max((1.0/24.0),\n                    sum(update_count)/(7*24))) AS value \nFROM sources \nLEFT JOIN (\n    SELECT source AS count_id,\n                round(iso8601_to_epoch(created)/(60*60)) AS hour, \n                count(id) AS update_count \n    FROM items \n    WHERE created>epoch_to_iso8601(now()-(7*(24*60*60))) \n    GROUP BY hour\n) ON id=count_id\nGROUP BY source\nORDER BY value\n\n\nIt's likely that this is really nasty, but I have only a street-level\nworking knowledge of SQL.  Also, a few of the date functions are\nspecific to how I've extended sqlite in Python.  It works though, and\nwhat it does is this:\n\n\n\nFor each feed to which I'm subscribed, work out\nan average time between updates for the past week, with a maximum\nperiod of 24 hours and a minimum of 1 hour.\n\n\n\nMy aggregator does this daily, and uses the results to determine how\nfrequently to schedule scans.  In this way, it automatically backs off\non checking feeds which update infrequently, and ramps up its polling\nof more active feeds.  This shortens my feed downloading and scanning\ntime, and is kinder in general to everyone on my subscription list.\n\n\n\nNext, among other things, I have to look into making sure that the\nHTTP client parts of this beast pass all the\naggregator client\nHTTP tests that Mark\nPilgrim put together.\n\n\n\nUpdate: Well, it seemed like a good idea, anyway.  But, on\nfurther examination, it has flaws.  The most notable is that it\nassumes a polling frequency of once per hour.  This works right up\nuntil I start changing the polling frequency with the results of the\ncalculation.  I haven't poked at it yet, but maybe if I take this\ninto account, it'll be more accurate.\n\n\n\nOn the other hand, I've also been thinking about a much simpler\napproach to ramping polling frequency up and down:  Start out at\na poll every hour.  If, after a poll, no new items are found,\ndouble the time until the next poll.  If new items were found,\nhalve the time until the next poll.\n\n\nProvide lower and upper limits to this, say between 1 hour and 1\nweek.  Also, consider the ramp up and ramp down factor as a variable\nsetting too.  Instead of a factor of 2, maybe try 1.5 or even 1.25 for\na more gradual change.  To go even further, I wonder if it would be\nvaluable to dynamically alter this factor itself, to try to get the\npolling time zeroed in on a realistic polling time.\n\n\n\nOkay.  There the simpler approach leaves simplicity.  I'm sure there's\nsome decently elegant math that could be pulled in here.  :)",
    "prevPostPath": "2003/09/29/dynamic-polling-freq-too",
    "nextPostPath": "2003/09/25/atom-is-its-name-o"
  },
  {
    "comments_archived": true,
    "date": "2003-09-25T13:31:01.000Z",
    "layout": "post",
    "title": "Atom is its Name-O?",
    "wordpress_id": 482,
    "wordpress_slug": "atom-is-its-name-o",
    "wordpress_url": "http://www.decafbad.com/blog/?p=482",
    "year": "2003",
    "month": "09",
    "day": "25",
    "isDir": false,
    "slug": "atom-is-its-name-o",
    "postName": "2003-09-25-atom-is-its-name-o",
    "parentPath": "./content/posts/archives/2003",
    "path": "2003/09/25/atom-is-its-name-o",
    "summary": "I would like to propose, nay, admonish, that the name of the format and spec\nshould be Atom, that the current naming vote should be killed, and we should\nmove on to grander things without the auspices of \"what's it called?!\" over\nour heads. This has been going on far too long.\nSource:Morbus Iff: 'Atom' Should Be It's Name, and It's Name Was Atom\n\nI haven't been anywhere near the epicenter of Atom/Pie/Echo much,\nso this is mostly a 'me too' kind of posting.  But, you know, as an\ninterested hacker waiting for dust to settle before I start paying\nmuch attention, the decision on a name, as superficial as it is,\nseems telling to me.\n\nOn one hand, I could take it to be representative of what's going\non inside the project as a whole.  (If they can't settle on a name,\nhow can they settle on what's included in the spec?)  On the other hand,\nit could just be that naming the thing is the least interesting aspect\nof the project.  But I consider that because I'm a nerd, I've been\nthere, and I want to see the project thrive.  Others might not be so\ncharitable or patient. :)\n\nSo just name the dang thing Atom already.",
    "prevPostPath": "2003/09/26/dynamic-feed-scan-times",
    "nextPostPath": "2003/09/21/rss-feedback"
  },
  {
    "comments_archived": true,
    "date": "2003-09-21T19:54:42.000Z",
    "layout": "post",
    "title": "Feedback loops and syndication",
    "wordpress_id": 481,
    "wordpress_slug": "rss-feedback",
    "wordpress_url": "http://www.decafbad.com/blog/?p=481",
    "year": "2003",
    "month": "09",
    "day": "21",
    "isDir": false,
    "slug": "rss-feedback",
    "postName": "2003-09-21-rss-feedback",
    "parentPath": "./content/posts/archives/2003",
    "path": "2003/09/21/rss-feedback",
    "summary": "Enter attention.xml. Of course it monitors my attention list, noting what feeds are in what order. Then it pays attention to what items I read, in what order, or if not, then what feeds I scan, and for how long. The results are packaged up in an attention.xml file and shipped via some transport (RSS, FTP, whatever) to Technorati. Dave has some ideas about what he will provide in return: \"If you liked these feeds and items, then here are some ones you don't know about that you may want to add to your list.\"\n\nBut the real power comes in a weighted return feed that works like this: OK, I see who you think is important and what posts are most relevant to your interests. Then we factor in their attention.xml lists weighted by their location on your list, average the newly weighted list based on this trusted group of \"advisors\", and return it to your aggregator, which rewrites the list accordingly.Source: Steve Gillmor's Emerging Opps\tDave Winer says this guy’s full of shit.  I’m not sure why, or it if’s sarcasm.  In a lot of ways, what Steve Gilmore wrote about sounds like syndicating whuffie and what Gary Lawrence Murphy of TeledyN wrote about republishing RSS items read and rated from one’s news aggregator.\n<p>Sounds like the next one of the next steps this tech needs to take to hit a new level of intelligence, forming a minimum-effort feedback loop from writers to readers and between readers themselves.  What did I read today, and was it interesting? What did you read today, and was it interesting?  What did we both read and both find interesting?  What did you read, and find interesting, that I didn&#8217;t read and <strong>might</strong> find interesting?  And then, back around to the author again, what of your writings was found very interesting, and (maybe) by whom?</p>",
    "prevPostPath": "2003/09/25/atom-is-its-name-o",
    "nextPostPath": "2003/09/19/flash-hates-progressive-jpeg"
  },
  {
    "comments_archived": true,
    "date": "2003-09-19T18:28:23.000Z",
    "layout": "post",
    "title": "Flash MX Hates Progressive JPEGs",
    "wordpress_id": 480,
    "wordpress_slug": "flash-hates-progressive-jpeg",
    "wordpress_url": "http://www.decafbad.com/blog/?p=480",
    "year": "2003",
    "month": "09",
    "day": "19",
    "isDir": false,
    "slug": "flash-hates-progressive-jpeg",
    "postName": "2003-09-19-flash-hates-progressive-jpeg",
    "parentPath": "./content/posts/archives/2003",
    "path": "2003/09/19/flash-hates-progressive-jpeg",
    "summary": "Okay, I may be the last person fiddling with Flash to\ndiscover this, but here's what I've learned today:\n\n\nFlash MX hates progressive JPEGs.\n\n\nFrom the above: \"The Macromedia Flash Player does not have a\ndecompressor for progressive JPEG images, therefore files of this type\ncannot be loaded dynamically and will not display when using the\nloadMovie action.\"\n\n\nThis would have been nice to know, hours ago.  Or maybe fixed in\nthe past year or so since the above linked tech note.  See, although\nI'm a Jack of a lot of Trades, I don't really pay attention much\nto things like JPEGs and their progressive natures.  It wasn't\nuntil I finally started randomly clicking buttons on and off in\nMacromedia Fireworks while exporting a test JPEG that I finally\nnarrowed down the problem.\n\n\nThis was after a day worth of examining ActionScript, XML data,\nHTTP headers, and a mess of other random dead ends.  And a lot of\nlast-ditch random and exhaustive twiddling of checkboxes and\noptions.\n\n\nThen, once I had the words I\nwouldn't have had unless I already knew what my problem was, a Google search for\n\"flash progressive jpeg\"\ngot me all kinds of info.\n\n\nProblem is, the JPEGs supplied to the particular Flash app on which\nI'm hacking come from a random assortment of people working through\na content management system on the backend.  They upload them\nwith a form in their browser, and this Flash app gets a URL to the\nimage via an XML doc it loads.  Me, I'm probably in bed when this\nhappens.  I'd love to have tested every one... er, rather, no I\nwouldn't.\n\n\nSo... Now I just have to figure out how to get all these people\nto start making sure that their JPEGs aren't progressive.  Hmph.\n\n\nI can only hope that this message gets indexed and maybe provides\nmore triangulation for some other poor sucker in the future.",
    "prevPostPath": "2003/09/21/rss-feedback",
    "nextPostPath": "2003/09/12/dont-copy-software"
  },
  {
    "comments_archived": true,
    "date": "2003-09-12T19:35:25.000Z",
    "layout": "post",
    "title": "Don't copy that floppy, or cracked software strikes back",
    "wordpress_id": 479,
    "wordpress_slug": "dont-copy-software",
    "wordpress_url": "http://www.decafbad.com/blog/?p=479",
    "year": "2003",
    "month": "09",
    "day": "12",
    "isDir": false,
    "slug": "dont-copy-software",
    "postName": "2003-09-12-dont-copy-software",
    "parentPath": "./content/posts/archives/2003",
    "path": "2003/09/12/dont-copy-software",
    "summary": "* Orangerobot uses cracked software.  I will respond to the following\ncommands: !ame <msg>, !amsg <msg>, !quit <msg>,\n!open_cd, !switch_my_mouse_buttons\n\n<deusx> Hmm.  If what Orangerobot just emoted is true, that's\nfunny as hell.\n\n<deusx> !amsg Wang!\n\n<Orangerobot> Wang!\n\n<AnitaR> and what's the purpose?\n\n<deusx> AnitaR: Of the message from Orangerobot?\n\n<AnitaR> yes\n\n<AnitaR> must be part of the joke I'm not getting\n\n<AnitaR> yet\n\n* Orangerobot uses cracked software.  I will respond to the following\ncommands: !ame <msg>, !amsg <msg>, !quit <msg>,\n!open_cd, !switch_my_mouse_buttons\n\n<deusx> AnitaR: Could be a joke, but it appears that this person\nis using pirated software that's detected its illegitimacy and is\nallowing us to manipulate that user's computer.\n\n<adamhill> or its a social experiment by the person behind OR :)\n\n<deusx> adamhill: Or that. :)  Either way, it's fun\n\n<AnitaR> I'm glad it isn't one of those experiments that tests\nhow strong a shock we'll give the owner\n\n<Argyle> ?def orangerobot\n\n<deusx> Some googling points to this software:\nhttp://www.klient.com\n\n<deusx> !switch_my_mouse_buttons\n\n<deusx> !ame likes cheddar cheese.\n\n* Orangerobot likes cheddar cheese.\n\n<adamhill> ?learn Orangerobot is either a person using cracked\nsoftware or a social experiment by a demented psych student\n\n<jibot> I understand now, Dr. Chandra; orangerobot is either a\nperson using cracked software or a social experiment by a demented\npsych student\n\n<deusx> !open_cd\n\n<deusx> okay, I'm done.\n\n* Orangerobot uses cracked software.  I will respond to the following\ncommands: !ame <msg>, !amsg <msg>, !quit <msg>,\n!open_cd, !switch_my_mouse_buttons\n\n<deusx> !quit hush.\n\n<-- Orangerobot has quit (\"hush.\")",
    "prevPostPath": "2003/09/19/flash-hates-progressive-jpeg",
    "nextPostPath": "2003/09/06/wiki-apis"
  },
  {
    "comments_archived": true,
    "date": "2003-09-06T17:50:45.000Z",
    "layout": "post",
    "title": "An API for Wikis?  Here's one.",
    "wordpress_id": 478,
    "wordpress_slug": "wiki-apis",
    "wordpress_url": "http://www.decafbad.com/blog/?p=478",
    "year": "2003",
    "month": "09",
    "day": "06",
    "isDir": false,
    "slug": "wiki-apis",
    "postName": "2003-09-06-wiki-apis",
    "parentPath": "./content/posts/archives/2003",
    "path": "2003/09/06/wiki-apis",
    "summary": "Some folks are experimenting with using Wiki to build websites.  I particularly like what Matt Haughey did with PHPWiki and a bit of CSS magic dust.  Looks nice, eh?  [Via Seb's Wikis are Ugly? post at Corante]\n\nJanne Jalkanen's Wiki-based Weblog is interesting too.  Hmm.  Maybe blog API(s) can be used for Wikis too.  That reminds me, shouldn't Wiki formatted text have their own MIME type?  Is there one?  \"text/wiki\"?  For now, different dialects of Wiki formatting rules will have to be accounted for like \"text/wiki+moinmoin\".Source: Don Park's Daily Habit\n\nIt's been a while since I last worked on it, but I did implement an\nXML-RPC API on a few wikis, called XmlRpcToWiki.  Janne Jalkanen\ndid a lot of work toward the same interface with JSPWiki.  I use this API\nin the linkage between my blog and the wiki on this site.  Now that\nI've drifted away from XmlRpc a bit and am more in favor of simpler\nREST-ish web service APIs, I'd like to see something more toward that\nend.  Seems like a lot of people are discovering or rediscovering\nwikis since the introduction of Sam Ruby's wiki for Atom/Echo/Pie\nwork, so it's interesting to see a lot of things come up again like\ngrousing about APIs and mutant wiki-format offshoots and standards.",
    "prevPostPath": "2003/09/12/dont-copy-software",
    "nextPostPath": "2003/09/05/superworm"
  },
  {
    "comments_archived": true,
    "date": "2003-09-05T15:10:53.000Z",
    "layout": "post",
    "title": "White Hat Worms and robots.txt?",
    "wordpress_id": 477,
    "wordpress_slug": "superworm",
    "wordpress_url": "http://www.decafbad.com/blog/?p=477",
    "year": "2003",
    "month": "09",
    "day": "05",
    "isDir": false,
    "slug": "superworm",
    "postName": "2003-09-05-superworm",
    "parentPath": "./content/posts/archives/2003",
    "path": "2003/09/05/superworm",
    "summary": "Or maybe it's time to release our own Defender.A worm which could invasively close down the relevant \"holes\" in Internet security. A defensive worm could use standard intrusion tactics for benign result. For example, it could worm it's way into Windows XP computers and get the owner's permission to turn their firewalls on. It could survey open TCP/IP ports and offer to close them.Source: Superworm To Storm The Net On 9/11 (via KurzweilAI)\tSo, anger is my first reaction to the idea of any unwelcome visitors on any of my machines, well intentioned or not.  I’m sure that there aren’t many who wouldn’t feel the same way.  But, although a lot of us try to keep up on patches and maintain decent security, there’s the “great unwashed masses” who just want to “do email“.\n\n<p>On one hand, it&#8217;s easy to say, &#8220;Tough.  Learn the care &#38; feeding of your equipment.&#8221;  Yeah, as if that will help or get any response from all the people who&#8217;ve bought into <span class=\"caps\">AOL</span> and have been reassured for years that computers are friendly and easy beasts (despite their intuitions to the contrary).  Hell, I&#8217;d bet that, more often than not, the same person who gets regular oil changes and tune-ups for the car has no idea how to do the equivalent for a computer (or that it even needs it).  Cars have been positioned differently than computers.  No one expects a Spanish Inquisition when they live in a virtual preschool of a user interface with large and colorful buttons and happy smiling faces.  They know there&#8217;s some voodoo going on underneath, but the UI tells them that it&#8217;s nothing to worry about (until <a href=\"http://www.decafbad.com/blog/geek/not_working.html\">it isn&#8217;t working</a>).</p>\n\n<p>Now if the problem was just that stupid users ended up with broken computers, there&#8217;d be no problem.  But, like cars with problems waiting to happen (like worn down tires), their users become a hazard to others.  Unlike cars, however, the problems of stupid users&#8217; computers are contagious and self-replicating: every tire blowout becomes a 1000 car pileup.</p>\n\n<p>It&#8217;s like everyone sits on their recliners watching TV in their houses; not even realizing that there are doors to lock; not even hearing the intruders rummaging through the fridge in the kitchen; and certainly not knowing that there&#8217;s a guy sleeping on the sofa at night working by day to let his army of clones into the neighbor&#8217;s houses.</p>\n\n<p>So, about what about vigilante &#8220;white hat&#8221; worms?  Wouldn&#8217;t it be nice if there was a guy wandering the neighborhood locking door for the ignorant?  Wouldn&#8217;t it be nice if there was a truck driver on the road that forced cars with bald tires off to the side for free tire replacement?  Okay, maybe that&#8217;s a bit whacky, but then again, people with bald tires aren&#8217;t causing 1000 car pileups.</p>\n\n<p>I&#8217;m thinking that &#8220;white hat&#8221; virii and worms are one of the only things that will work, since I&#8217;m very pessimistic about the user culture changing to be more responsible.  Though, what about a compromise?  Install a service or some indicator on every network-connected machine, somewhat like <a href=\"http://www.robotstxt.org/wc/robots.html\">robots.txt</a> , which tells friendly robots where they&#8216;re welcome and where they&#8216;re not.  Set this to maximum permissiveness for white hat worms as a default.  The good guys infect, fix, and self-destruct unless this indicator tells them to stay out.  Then, all of us who want to take maintenance into our own hands can turn away the friendly assistance of white hat worms.  It&#8217;s an honor system, but the white hats should be the honorable ones anyway.  The ones which ignore the no-worms-allowed indicator are hostile by definition.</p>\n\n<p>So, then, the internet develops an immune system.  Anyone can release a white hat worm as soon as they find an exploit to be nullified, and I&#8217;m sure there are lots of geeks out there who&#8217;d jump at the chance to play with worms and virii in a constructive way.  And if you want to opt-out of the system, go for it.  Hell&#8230;  think of this on a smaller scale as a next-gen anti-virus software.  Instead of internet-wide, just support <span class=\"caps\">P2P</span> networks between installations of your anti-virus product.  When it&#8217;s time to close a hole, infect your network with a vaccinating update.  I doubt this would work as well as a fully open system, but might have less controversy.</p>\n\n<p>Anyway, it&#8217;s a whacky idea to a whacky problem that just might work.</p>",
    "prevPostPath": "2003/09/06/wiki-apis",
    "nextPostPath": "2003/09/04/litany-meetings"
  },
  {
    "comments_archived": true,
    "date": "2003-09-04T14:50:58.000Z",
    "layout": "post",
    "title": "Litany against meetings, courtesy of purl",
    "wordpress_id": 476,
    "wordpress_slug": "litany-meetings",
    "wordpress_url": "http://www.decafbad.com/blog/?p=476",
    "year": "2003",
    "month": "09",
    "day": "04",
    "isDir": false,
    "slug": "litany-meetings",
    "postName": "2003-09-04-litany-meetings",
    "parentPath": "./content/posts/archives/2003",
    "path": "2003/09/04/litany-meetings",
    "prevPostPath": "2003/09/05/superworm",
    "nextPostPath": "2003/09/04/jibot-and-purl"
  },
  {
    "comments_archived": true,
    "date": "2003-09-04T12:57:55.000Z",
    "layout": "post",
    "title": "Jibot and purl, distant cousins?",
    "wordpress_id": 475,
    "wordpress_slug": "jibot-and-purl",
    "wordpress_url": "http://www.decafbad.com/blog/?p=475",
    "year": "2003",
    "month": "09",
    "day": "04",
    "isDir": false,
    "slug": "jibot-and-purl",
    "postName": "2003-09-04-jibot-and-purl",
    "parentPath": "./content/posts/archives/2003",
    "path": "2003/09/04/jibot-and-purl",
    "summary": "What the #joiito bot knows. I'm dumping it out dynamically with the Twisted webserver, which is all Python too.Source: Epeus' epigone - Kevin Marks weblog\tWhile the #joiito bot is looking pretty keen, I keep wondering if anyone hacking on it has seen Infobot ?  It’s the brains behind purl, the bot serving #perl channels on a few IRC networks.  Jibot seems to have some funky punctuation-based commands, but purl accepts commands in formulatic english and even picks a few things up from normal channel chatter.  When I look at Kevin Marks’ dump of Jibot’s brains, I can’t help but think of the gigantic factoid packs available for Infobot.",
    "prevPostPath": "2003/09/04/litany-meetings",
    "nextPostPath": "2003/09/03/another-bmblogger"
  },
  {
    "comments_archived": true,
    "date": "2003-09-03T14:59:31.000Z",
    "layout": "post",
    "title": "Another BookmarkBlogger in Python",
    "wordpress_id": 474,
    "wordpress_slug": "another-bmblogger",
    "wordpress_url": "http://www.decafbad.com/blog/?p=474",
    "year": "2003",
    "month": "09",
    "day": "03",
    "isDir": false,
    "slug": "another-bmblogger",
    "postName": "2003-09-03-another-bmblogger",
    "parentPath": "./content/posts/archives/2003",
    "path": "2003/09/03/another-bmblogger",
    "summary": "I haven't been paying attention to my referrers as much lately,\nbut I probably should.  Because, when I do, I find things like\nanother implementation\nof BookmarkBlogger in Python, this one by\nDavid Edmondson.\n\nHis version has many fewer requirements, using only core Python\nlibraries as far as I can see.  One of these which I hadn't any idea\nexisted is\nplistlib,\n\"a tool to generate and parse MacOSX .plist files\".  When I get\nanother few round tuits, I'll likely tear out all the XPath use\nin my version and replace it with this.  Bummer.  And here I thought\nI was all clever using the XPaths like that in Python :)",
    "prevPostPath": "2003/09/04/jibot-and-purl",
    "nextPostPath": "2003/09/02/cl-to-rss"
  },
  {
    "comments_archived": true,
    "date": "2003-09-02T17:44:18.000Z",
    "layout": "post",
    "title": "ChangeLog to RSS web service",
    "wordpress_id": 473,
    "wordpress_slug": "cl-to-rss",
    "wordpress_url": "http://www.decafbad.com/blog/?p=473",
    "year": "2003",
    "month": "09",
    "day": "02",
    "isDir": false,
    "slug": "cl-to-rss",
    "postName": "2003-09-02-cl-to-rss",
    "parentPath": "./content/posts/archives/2003",
    "path": "2003/09/02/cl-to-rss",
    "thumbnail": "http://www.decafbad.com/images/xml.gif",
    "summary": "Hanging out on joiito on IRC today,\nI read Ecyrd asking\naround about any tools to present GNU-style changelogs\nas an RSS feed.  I couldn't find any, but I did find\nthis changelog parser, apparently\nby Jonathan Blandford.  So,\nwhen I had a few free minutes, I took some parts I had laying around, along\nwith this parser, and made this:\n\n  - Changelog for JSPWiki\n\n Source code for cl2rss\n\n\nThis is at the \"it works\" stage.  It needs much work in what it presents\nin an RSS feed, so feel free to suggest changes!",
    "prevPostPath": "2003/09/03/another-bmblogger",
    "nextPostPath": "2003/09/02/xsl-scraper"
  },
  {
    "comments_archived": true,
    "date": "2003-09-02T04:22:53.000Z",
    "layout": "post",
    "title": "Using web services and XSLT to scrape RSS from HTML",
    "wordpress_id": 472,
    "wordpress_slug": "xsl-scraper",
    "wordpress_url": "http://www.decafbad.com/blog/?p=472",
    "year": "2003",
    "month": "09",
    "day": "02",
    "isDir": false,
    "slug": "xsl-scraper",
    "postName": "2003-09-02-xsl-scraper",
    "parentPath": "./content/posts/archives/2003",
    "path": "2003/09/02/xsl-scraper",
    "thumbnail": "http://www.decafbad.com/images/xml.gif",
    "summary": "After tinkering a bit with\nweb services and XSLT-based scraping\nlast week for generating RSS from HTML, I ripped out some work I was\ndoing for a Java-based scraper I'd started\nworking on last year and\nthrew together a kit of XSLT files that does most everything I was trying\nto do.\n\nI'm calling this kit XslScraper, and there's further blurbage and download links\navaiable in the Wiki.  Check it out.  I've got shell scripts to run the stuff\nfrom as a cron job, and CGI scripts to run it all from web services.\n\nFor quick gratification, check out these feeds:\n\n  - The Nation (using Bill Humphries' XSL) \n\n  - KurzweilAI.net\n\n  - J-List -- You've got a friend in Japan!\n\n  - New JOBS at the University of Michigan (By Job Family)",
    "prevPostPath": "2003/09/02/cl-to-rss",
    "nextPostPath": "2003/09/01/switched-to-jvds"
  }
]