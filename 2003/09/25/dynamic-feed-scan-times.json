{"comments_archived":true,"date":"2003-09-25T00:00:00.000Z","layout":"post","title":"Dynamic feed polling times for news aggregators","wordpress_id":483,"wordpress_slug":"dynamic-feed-scan-times","wordpress_url":"http://www.decafbad.com/blog/?p=483","url":"/2003/09/25/dynamic-feed-scan-times/","summary":"<p>Today, &lt;a href=<a href=\"http://www.decafbad.com/cvs/dbagg/&quot;&gt;my\">http://www.decafbad.com/cvs/dbagg/&quot;&gt;my</a> aggregator got\nthe following SQL worked into its <a href=\"http://www.decafbad.com/cvs/dbagg/lib/dbagg/scan.py?rev=HEAD&amp;content-type=text/vnd.viewcvs-markup\">feed poll scheduling machinery</a>:</p>\n\n<pre>SELECT id as source,\n       &apos;update_period&apos; as name,\n       max(1, 1/max((1.0/24.0),\n                    sum(update_count)/(7*24))) AS value \nFROM sources \nLEFT JOIN (\n    SELECT source AS count_id,\n                round(iso8601_to_epoch(created)/(60*60)) AS hour, \n                count(id) AS update_count \n    FROM items \n    WHERE created&gt;epoch_to_iso8601(now()-(7*(24*60*60))) \n    GROUP BY hour\n) ON id=count_id\nGROUP BY source\nORDER BY value</pre>\n\n<p>\nIt&apos;s likely that this is really nasty, but I have only a street-level\nworking knowledge of SQL.  Also, a few of the date functions are\nspecific to how I&apos;ve <a href=\"http://pysqlite.sourceforge.net/documentation/pysqlite/node10.html#SECTION004231000000000000000\">extended sqlite in Python</a>.  It works though, and\nwhat it does is this:\n</p>\n\n<p>\nFor each feed to which I&apos;m subscribed, work out\nan average time between updates for the past week, with a maximum\nperiod of 24 hours and a minimum of 1 hour.\n</p>\n\n<p>\nMy aggregator does this daily, and uses the results to determine how\nfrequently to schedule scans.  In this way, it automatically backs off\non checking feeds which update infrequently, and ramps up its polling\nof more active feeds.  This shortens my feed downloading and scanning\ntime, and is kinder in general to everyone on my subscription list.\n</p>\n\n<p>\nNext, among other things, I have to look into making sure that the\nHTTP client parts of this beast pass all the\n<a href=\"http://diveintomark.org/tests/client/http/\">aggregator client\nHTTP tests</a> that <a href=\"http://diveintomark.org/\">Mark\nPilgrim</a> put together.\n</p>\n\n<p>\n<b>Update</b>: Well, it seemed like a good idea, anyway.  But, on\nfurther examination, it has flaws.  The most notable is that it\nassumes a polling frequency of once per hour.  This works right up\nuntil I start changing the polling frequency with the results of the\ncalculation.  I haven&apos;t poked at it yet, but maybe if I take this\ninto account, it&apos;ll be more accurate.\n</p>\n\n<p>\nOn the other hand, I&apos;ve also been thinking about a much simpler\napproach to ramping polling frequency up and down:  Start out at\na poll every hour.  If, after a poll, no new items are found,\ndouble the time until the next poll.  If new items were found,\nhalve the time until the next poll.</p>\n\n<p>\nProvide lower and upper limits to this, say between 1 hour and 1\nweek.  Also, consider the ramp up and ramp down factor as a variable\nsetting too.  Instead of a factor of 2, maybe try 1.5 or even 1.25 for\na more gradual change.  To go even further, I wonder if it would be\nvaluable to dynamically alter this factor itself, to try to get the\npolling time zeroed in on a realistic polling time.\n</p>\n\n<p></p><p>\nOkay.  There the simpler approach leaves simplicity.  I&apos;m sure there&apos;s\nsome decently elegant math that could be pulled in here.  :)\n</p>\n","path":"2003/09/25/dynamic-feed-scan-times","content":"<p>Today, &lt;a href=<a href=\"http://www.decafbad.com/cvs/dbagg/&quot;&gt;my\">http://www.decafbad.com/cvs/dbagg/&quot;&gt;my</a> aggregator</a> got\nthe following SQL worked into its <a href=\"http://www.decafbad.com/cvs/dbagg/lib/dbagg/scan.py?rev=HEAD&content-type=text/vnd.viewcvs-markup\">feed poll scheduling machinery</a>:</p>\n\n<pre>SELECT id as source,\n       'update_period' as name,\n       max(1, 1/max((1.0/24.0),\n                    sum(update_count)/(7*24))) AS value \nFROM sources \nLEFT JOIN (\n    SELECT source AS count_id,\n                round(iso8601_to_epoch(created)/(60*60)) AS hour, \n                count(id) AS update_count \n    FROM items \n    WHERE created>epoch_to_iso8601(now()-(7*(24*60*60))) \n    GROUP BY hour\n) ON id=count_id\nGROUP BY source\nORDER BY value</pre>\n\n<p>\nIt&#39;s likely that this is really nasty, but I have only a street-level\nworking knowledge of SQL.  Also, a few of the date functions are\nspecific to how I&#39;ve <a href=\"http://pysqlite.sourceforge.net/documentation/pysqlite/node10.html#SECTION004231000000000000000\">extended sqlite in Python</a>.  It works though, and\nwhat it does is this:\n</p>\n\n<p>\nFor each feed to which I&#39;m subscribed, work out\nan average time between updates for the past week, with a maximum\nperiod of 24 hours and a minimum of 1 hour.\n</p>\n\n<p>\nMy aggregator does this daily, and uses the results to determine how\nfrequently to schedule scans.  In this way, it automatically backs off\non checking feeds which update infrequently, and ramps up its polling\nof more active feeds.  This shortens my feed downloading and scanning\ntime, and is kinder in general to everyone on my subscription list.\n</p>\n\n<p>\nNext, among other things, I have to look into making sure that the\nHTTP client parts of this beast pass all the\n<a href=\"http://diveintomark.org/tests/client/http/\">aggregator client\nHTTP tests</a> that <a href=\"http://diveintomark.org/\">Mark\nPilgrim</a> put together.\n</p>\n\n<p>\n<b>Update</b>: Well, it seemed like a good idea, anyway.  But, on\nfurther examination, it has flaws.  The most notable is that it\nassumes a polling frequency of once per hour.  This works right up\nuntil I start changing the polling frequency with the results of the\ncalculation.  I haven&#39;t poked at it yet, but maybe if I take this\ninto account, it&#39;ll be more accurate.\n</p>\n\n<p>\nOn the other hand, I&#39;ve also been thinking about a much simpler\napproach to ramping polling frequency up and down:  Start out at\na poll every hour.  If, after a poll, no new items are found,\ndouble the time until the next poll.  If new items were found,\nhalve the time until the next poll.</p>\n\n<p>\nProvide lower and upper limits to this, say between 1 hour and 1\nweek.  Also, consider the ramp up and ramp down factor as a variable\nsetting too.  Instead of a factor of 2, maybe try 1.5 or even 1.25 for\na more gradual change.  To go even further, I wonder if it would be\nvaluable to dynamically alter this factor itself, to try to get the\npolling time zeroed in on a realistic polling time.\n</p>\n\n<p><p>\nOkay.  There the simpler approach leaves simplicity.  I&#39;m sure there&#39;s\nsome decently elegant math that could be pulled in here.  :)\n</p>\n<!--more-->\nshortname=dynamic_feed_scan_times</p>\n<div id=\"comments\" class=\"comments archived-comments\">\n            <h3>Archived Comments</h3>\n\n        <ul class=\"comments\">\n\n        <li class=\"comment\" id=\"comment-221087768\">\n            <div class=\"meta\">\n                <div class=\"author\">\n                    <a class=\"avatar image\" rel=\"nofollow\" \n                       href=\"http://24.102.209.201/weblogs/ben/\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=588bdfdda82be46c638d6956c55ebc38&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\n                    <a class=\"avatar name\" rel=\"nofollow\" \n                       href=\"http://24.102.209.201/weblogs/ben/\">Gnomon</a>\n                </div>\n                <a href=\"#comment-221087768\" class=\"permalink\"><time datetime=\"2003-09-26T09:56:09\">2003-09-26T09:56:09</time></a>\n            </div>\n            <div class=\"content\">Why not just go the TCP/IP route - Additive Increase / Multiplicative Decrease? Start by setting the check-interval to one hour (or whatever). For each feed, if a new post is found, cut the check-interval for that feed by half; if no new post is found, increase the check-interval by an hour.\n\nIt&#39;s not optimal, and it won&#39;t automagically zero in on the predicted post times of individual feeds, but it strikes a nice balance between bandwidth politeness, update rapidity and conceptual simplicity. The constant values (initial check-interval, check-interval increment, check-interval multiplier) can be tweaked for different behavioural styles.</div>\n\n<pre><code>    &lt;/li&gt;\n\n    &lt;li class=&quot;comment&quot; id=&quot;comment-221087769&quot;&gt;\n        &lt;div class=&quot;meta&quot;&gt;\n            &lt;div class=&quot;author&quot;&gt;\n                &lt;a class=&quot;avatar image&quot; rel=&quot;nofollow&quot; \n                   href=&quot;http://www.decafbad.com/blog&quot;&gt;&lt;img src=&quot;http://www.gravatar.com/avatar.php?gravatar_id=2ac2cffd36ada8c734b90e02a1e5c1ac&amp;amp;size=32&amp;amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png&quot;/&gt;&lt;/a&gt;\n                &lt;a class=&quot;avatar name&quot; rel=&quot;nofollow&quot; \n                   href=&quot;http://www.decafbad.com/blog&quot;&gt;l.m.orchard&lt;/a&gt;\n            &lt;/div&gt;\n            &lt;a href=&quot;#comment-221087769&quot; class=&quot;permalink&quot;&gt;&lt;time datetime=&quot;2003-09-26T11:04:24&quot;&gt;2003-09-26T11:04:24&lt;/time&gt;&lt;/a&gt;\n        &lt;/div&gt;\n        &lt;div class=&quot;content&quot;&gt;Hmm...  I knew that this was something that someone had already handled somewhere.  :)  I&#39;m just not all that familiar with the details of TCP/IP, but this sounds pretty much like a workable approach I&#39;d like to go with.&lt;/div&gt;\n\n    &lt;/li&gt;\n\n    &lt;li class=&quot;comment&quot; id=&quot;comment-221087770&quot;&gt;\n        &lt;div class=&quot;meta&quot;&gt;\n            &lt;div class=&quot;author&quot;&gt;\n                &lt;a class=&quot;avatar image&quot; rel=&quot;nofollow&quot; \n                   href=&quot;http://www.decafbad.com/blog&quot;&gt;&lt;img src=&quot;http://www.gravatar.com/avatar.php?gravatar_id=2ac2cffd36ada8c734b90e02a1e5c1ac&amp;amp;size=32&amp;amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png&quot;/&gt;&lt;/a&gt;\n                &lt;a class=&quot;avatar name&quot; rel=&quot;nofollow&quot; \n                   href=&quot;http://www.decafbad.com/blog&quot;&gt;l.m.orchard&lt;/a&gt;\n            &lt;/div&gt;\n            &lt;a href=&quot;#comment-221087770&quot; class=&quot;permalink&quot;&gt;&lt;time datetime=&quot;2003-09-26T11:07:00&quot;&gt;2003-09-26T11:07:00&lt;/time&gt;&lt;/a&gt;\n        &lt;/div&gt;\n        &lt;div class=&quot;content&quot;&gt;Oh!  For a second I thought that what you&#39;re suggesting was basically what I was already working on with my multiplying/dividing by a factor...  But you&#39;re talking about something that ADDS to increase and MULTIPLIES to decrease, which is something much more biased to back off than ramp up, which seems very polite to me.  Yay!&lt;/div&gt;\n\n    &lt;/li&gt;\n\n    &lt;/ul&gt;\n\n    &lt;/div&gt;\n</code></pre>"}