{"comments_archived":true,"date":"2003-08-23T00:00:00.000Z","layout":"post","title":"Scraping HTML with web services","wordpress_id":466,"wordpress_slug":"rss-scrape-urls","wordpress_url":"http://www.decafbad.com/blog/?p=466","url":"/2003/08/23/rss-scrape-urls/","summary":"<p>After <a href=\"http://www.decafbad.com/blog/geek/rss_scrape_xsl.html\">checking out</a> <a href=\"http://www.whump.com/moreLikeThis/date/21/08/2003\">Bill Humphries&#x2019; approach</a> to scraping yesterday, I recalled the various things <a href=\"http://udell.roninhouse.com/bytecols/2001-08-15.html\">Jon Udell has written</a> about <span class=\"caps\">URL</span>-as-command-line and the various places I&#x2019;ve seen the <a href=\"http://www.w3.org/2001/05/xslt\"><span class=\"caps\">W3C XSLT </span>Servlet</a> used in <span class=\"caps\">XSLT</span> tinkering.  I also remembered that there&#x2019;s an <a href=\"http://cgi.w3.org/cgi-bin/tidy\"><span class=\"caps\">HTML </span>Tidy service</a> offered by <span class=\"caps\">W3C</span> as well.</p>\n\n<pre><code>&lt;p&gt;So&amp;#8230;  these are all URLs.  I figured I could pull together the site &lt;span class=&quot;caps&quot;&gt;URL&lt;/span&gt;, &lt;a href=&quot;http://www.whump.com/dropbox/nationrss/nation.xsl&quot;&gt;Bill&amp;#8217;s &lt;span class=&quot;caps&quot;&gt;XSLT&lt;/span&gt;&lt;/a&gt;, the tidy service, and the &lt;span class=&quot;caps&quot;&gt;XSLT&lt;/span&gt; service, and have a whole lot of scraping going on right in my browser or via wget or curl.  Here are the steps in how I composed the &lt;span class=&quot;caps&quot;&gt;URL&lt;/span&gt;:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=&quot;http://www.thenation.com&quot;&gt;http://www.thenation.com&lt;/a&gt;&lt;/li&gt;\n</code></pre><li><a href=\"http://cgi.w3.org/cgi-bin/tidy?docAddr=http%3A%2F%2Fwww.thenation.com\">http://cgi.w3.org/cgi-bin/tidy?docAddr=http%3A%2F%2Fwww.thenation.com</a></li>\n<li><a href=\"http://www.w3.org/2000/06/webdata/xslt?xslfile=http%3A%2F%2Fwww.whump.com%2Fdropbox%2Fnationrss%2Fnation.xsl&amp;xmlfile=http%3A%2F%2Fcgi.w3.org%2Fcgi-bin%2Ftidy%3FdocAddr%3Dhttp%253A%252F%252Fwww.thenation.com&amp;transform=Submit\">http://www.w3.org/2000/06/webdata/xslt?<br>xslfile=http%3A%2F%2Fwww.whump.com%2Fdropbox%2Fnationrss%2Fnation.xsl&amp;<br>xmlfile=http%3A%2F%2Fcgi.w3.org%2Fcgi-bin%2Ftidy%3F<br>docAddr%3Dhttp%253A%252F%252Fwww.thenation.com&amp;transform=Submit</a></li>\n\n\n    <p>Unfortunately, this doesn&#x2019;t work.  In particular, step #2 fails, the Tidy service reporting a failure in processing the original <span class=\"caps\">HTML</span>.  I imagine, had that worked, the whole process at step #3 would be producing <span class=\"caps\">RSS</span>.  On my command line, <span class=\"caps\">HTML </span>Tidy works fine, so I&#x2019;ve been thinking of throwing together my own web interface to that program and seeing if that works.</p>\n\n    <p>If it works, this with the addition of a cache at each stage could allow for what I think is a pretty nifty, all web-based means of scraping news items from web sites.  </p>\n\n        <p>What would really be nice for apps like this is a better way to express the URLs-within-URLs without escaping and escaping and escaping and...  Thinking some very lightweight scripting here, or some LISP-ish expressions would help.</p>\n","path":"2003/08/23/rss-scrape-urls","content":"<p>After <a href=\"http://www.decafbad.com/blog/geek/rss_scrape_xsl.html\">checking out</a> <a href=\"http://www.whump.com/moreLikeThis/date/21/08/2003\">Bill Humphries&#8217; approach</a> to scraping yesterday, I recalled the various things <a href=\"http://udell.roninhouse.com/bytecols/2001-08-15.html\">Jon Udell has written</a> about <span class=\"caps\">URL</span>-as-command-line and the various places I&#8217;ve seen the <a href=\"http://www.w3.org/2001/05/xslt\"><span class=\"caps\">W3C XSLT </span>Servlet</a> used in <span class=\"caps\">XSLT</span> tinkering.  I also remembered that there&#8217;s an <a href=\"http://cgi.w3.org/cgi-bin/tidy\"><span class=\"caps\">HTML </span>Tidy service</a> offered by <span class=\"caps\">W3C</span> as well.</p>\n\n<pre><code>&lt;p&gt;So&amp;#8230;  these are all URLs.  I figured I could pull together the site &lt;span class=&quot;caps&quot;&gt;URL&lt;/span&gt;, &lt;a href=&quot;http://www.whump.com/dropbox/nationrss/nation.xsl&quot;&gt;Bill&amp;#8217;s &lt;span class=&quot;caps&quot;&gt;XSLT&lt;/span&gt;&lt;/a&gt;, the tidy service, and the &lt;span class=&quot;caps&quot;&gt;XSLT&lt;/span&gt; service, and have a whole lot of scraping going on right in my browser or via wget or curl.  Here are the steps in how I composed the &lt;span class=&quot;caps&quot;&gt;URL&lt;/span&gt;:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=&quot;http://www.thenation.com&quot;&gt;http://www.thenation.com&lt;/a&gt;&lt;/li&gt;\n</code></pre><li><a href=\"http://cgi.w3.org/cgi-bin/tidy?docAddr=http%3A%2F%2Fwww.thenation.com\">http://cgi.w3.org/cgi-bin/tidy?docAddr=http%3A%2F%2Fwww.thenation.com</a></li>\n<li><a href=\"http://www.w3.org/2000/06/webdata/xslt?xslfile=http%3A%2F%2Fwww.whump.com%2Fdropbox%2Fnationrss%2Fnation.xsl&xmlfile=http%3A%2F%2Fcgi.w3.org%2Fcgi-bin%2Ftidy%3FdocAddr%3Dhttp%253A%252F%252Fwww.thenation.com&transform=Submit\">http://www.w3.org/2000/06/webdata/xslt?<br />xslfile=http%3A%2F%2Fwww.whump.com%2Fdropbox%2Fnationrss%2Fnation.xsl&#38;<br />xmlfile=http%3A%2F%2Fcgi.w3.org%2Fcgi-bin%2Ftidy%3F<br />docAddr%3Dhttp%253A%252F%252Fwww.thenation.com&#38;transform=Submit</a></li>\n</ol>\n\n    <p>Unfortunately, this doesn&#8217;t work.  In particular, step #2 fails, the Tidy service reporting a failure in processing the original <span class=\"caps\">HTML</span>.  I imagine, had that worked, the whole process at step #3 would be producing <span class=\"caps\">RSS</span>.  On my command line, <span class=\"caps\">HTML </span>Tidy works fine, so I&#8217;ve been thinking of throwing together my own web interface to that program and seeing if that works.</p>\n\n    <p>If it works, this with the addition of a cache at each stage could allow for what I think is a pretty nifty, all web-based means of scraping news items from web sites.  </p>\n\n        <p>What would really be nice for apps like this is a better way to express the URLs-within-URLs without escaping and escaping and escaping and...  Thinking some very lightweight scripting here, or some LISP-ish expressions would help.</p>\n<!--more-->\nshortname=rss_scrape_urls\n\n<div id=\"comments\" class=\"comments archived-comments\">\n            <h3>Archived Comments</h3>\n\n        <ul class=\"comments\">\n\n        <li class=\"comment\" id=\"comment-221086837\">\n            <div class=\"meta\">\n                <div class=\"author\">\n                    <a class=\"avatar image\" rel=\"nofollow\" \n                       href=\"http://www.whump.com/moreLikeThis/\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=622548e3f303e03297375ab20ddcb696&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\n                    <a class=\"avatar name\" rel=\"nofollow\" \n                       href=\"http://www.whump.com/moreLikeThis/\">Bill Humphries</a>\n                </div>\n                <a href=\"#comment-221086837\" class=\"permalink\"><time datetime=\"2003-08-24T16:08:56\">2003-08-24T16:08:56</time></a>\n            </div>\n            <div class=\"content\">Does the w3c Tidy service support the force output option? That&#39;s what I had to do with command line Tidy to get something well formed from The Nation&#39;s home page.</div>\n\n        </li>\n\n<pre><code>    &lt;li class=&quot;comment&quot; id=&quot;comment-221086838&quot;&gt;\n        &lt;div class=&quot;meta&quot;&gt;\n            &lt;div class=&quot;author&quot;&gt;\n                &lt;a class=&quot;avatar image&quot; rel=&quot;nofollow&quot; \n                   href=&quot;http://www.decafbad.com/blog&quot;&gt;&lt;img src=&quot;http://www.gravatar.com/avatar.php?gravatar_id=2ac2cffd36ada8c734b90e02a1e5c1ac&amp;amp;size=32&amp;amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png&quot;/&gt;&lt;/a&gt;\n                &lt;a class=&quot;avatar name&quot; rel=&quot;nofollow&quot; \n                   href=&quot;http://www.decafbad.com/blog&quot;&gt;l.m.orchard&lt;/a&gt;\n            &lt;/div&gt;\n            &lt;a href=&quot;#comment-221086838&quot; class=&quot;permalink&quot;&gt;&lt;time datetime=&quot;2003-08-24T20:07:03&quot;&gt;2003-08-24T20:07:03&lt;/time&gt;&lt;/a&gt;\n        &lt;/div&gt;\n        &lt;div class=&quot;content&quot;&gt;Unfortunately, it seems that the W3C service only offers an indentation option&lt;/div&gt;\n\n    &lt;/li&gt;\n\n    &lt;li class=&quot;comment&quot; id=&quot;comment-221086840&quot;&gt;\n        &lt;div class=&quot;meta&quot;&gt;\n            &lt;div class=&quot;author&quot;&gt;\n                &lt;a class=&quot;avatar image&quot; rel=&quot;nofollow&quot; \n                   href=&quot;http://www.whump.com/moreLikeThis/&quot;&gt;&lt;img src=&quot;http://www.gravatar.com/avatar.php?gravatar_id=622548e3f303e03297375ab20ddcb696&amp;amp;size=32&amp;amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png&quot;/&gt;&lt;/a&gt;\n                &lt;a class=&quot;avatar name&quot; rel=&quot;nofollow&quot; \n                   href=&quot;http://www.whump.com/moreLikeThis/&quot;&gt;Bill Humphries&lt;/a&gt;\n            &lt;/div&gt;\n            &lt;a href=&quot;#comment-221086840&quot; class=&quot;permalink&quot;&gt;&lt;time datetime=&quot;2003-08-25T02:13:11&quot;&gt;2003-08-25T02:13:11&lt;/time&gt;&lt;/a&gt;\n        &lt;/div&gt;\n        &lt;div class=&quot;content&quot;&gt;It&#39;s tempting to take the script, and offer it as a service myself, with the force output option. However, I&#39;d need to wrap an authorization service in front of it so it doesn&#39;t kill my bandwidth.&lt;/div&gt;\n\n    &lt;/li&gt;\n\n    &lt;/ul&gt;\n\n    &lt;/div&gt;\n</code></pre>"}