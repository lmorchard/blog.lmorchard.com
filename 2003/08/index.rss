<?xml version="1.0" encoding="UTF-8"?>
  <rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
      <title>blog.lmorchard.com</title>
      <description>It&#39;s all spinning wheels &amp; self-doubt until the first pot of coffee.</description>
      <link>https://lmorchard.github.io/blog.lmorchard.com</link>
      <atom:link href="https://lmorchard.github.io/blog.lmorchard.com/index.rss" rel="self" type="application/rss+xml" />
      <item>
          <title>Switching to a JVDS server</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;It&#39;s been one year since I
signed up for a JohnCompanies server,
and though I&#39;ve had no complaints whatsoever, I&#39;ve signed up for
a server instance with JVDS.com and have moved
just about everything over.
Why?  Because it&#39;s cheaper, and I have less disposable income these
days.  And, well, it seemed like fun to try another virtual server
company, since I&#39;ve been looking into it so much lately.  The new
server has less capacity than the one I&#39;ve had at JohnCompanies, but I
really don&#39;t need all that much -- just a roof over my files and a
root password.  Well, I don&#39;t really need a root password, but it&#39;s
nice to have so that I can tinker around with more things with fewer
questions asked of the management.  (For what it&#39;s worth, we&#39;re still
using JohnCompanies servers for hosting at my work.)
I&#39;ve almost got this server migration thing down to a science, though,
since I had everything over and up in a few hours.  And that was going
from a FreeBSD system to Debian Linux.  Personally, though I fully
respect FreeBSD and the ports collection, I like Debian and apt-get
so much better.  But who knows, maybe in another year, I&#39;ll be moving
again for the hell of it.
I don&#39;t have much in the way of reputation for JVDS, but the management
has been very responsive to requests so far.  In fact, they&#39;re using
RT for their support ticket
management.  Responsiveness has been one of the most impressive aspects
of JohnCompanies, since between my personal server and the servers
I use at work, it tends to take less than an hour to get resolution
on any problems I&#39;ve had.  So far, JVDS has yet to disappoint me as
well.&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Mon, 01 Sep 2003 02:26:08 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/08/31/switched-to-jvds/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/08/31/switched-to-jvds/</guid>
        </item><item>
          <title>Bookmark Blogger in Python</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;Remember my BookmarkBlogger?  Well, I rewrote it in Python.  For
a little while, I was making little apps in Java, wishing it were a
scripting language.  I&#39;ve stopped that now.  Also, I&#39;ve added the
ability to include both link text and a non-linked comment in the
bookmarks to be blogged.  This new version is quite a bit simpler
and contained all in one script -- configuration, template, and all.
Download a tarball here
from my CVS server.&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Mon, 01 Sep 2003 01:34:52 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/08/31/bookmark-blogger-python/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/08/31/bookmark-blogger-python/</guid>
        </item><item>
          <title>Again a Student</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;I walked in thinking &quot;I can&#39;t believe I&#39;m a student again. I&#39;m a student again? Yee-bloody-ikes, how am I going to manage being a student again?&quot;

And I walked out with a spring in my step, thinking, &quot;Hey! I&#39;m a student again! W00t!&quot;
Source: Caveat Lector: Augusti 24, 2003 - Augusti 30, 2003 Archives



I’m not entirely sure (though I have hunches) on how to go about it, or to whom I should be talking, but this is what I want to be saying in the not-too-distant future.&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Fri, 29 Aug 2003 17:11:49 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/08/29/again-a-student/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/08/29/again-a-student/</guid>
        </item><item>
          <title>CSS, Background Images, and Rollovers</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;It occurred to me that this ought to be possible by reassigning a  container&#39;s background-image property when it is  :hover-ed.

Source: Images and thumbnails, a pure CSS hack (via dbagg: Items by Time)



Yup, and you can do the same for every other pseudo-class of an anchor tag.  I read about this via Eric Meyer’s article on the O‘Reilly Network.  I’m still very much a CSS neophyte, but it’s helped me incredibly at work, where I was able to create a site layout with one set of HTML pages styled by a small library of CSS files for look &amp;amp; feel.

&lt;p&gt;Yeah, yeah, that’s what it’s for, you say.  But it surprised the hell out of me that I was able to abuse background image properties of containers to create JavaScript-free rollovers, as well as select between completely different image-based layout elements.  This isn’t pure utopian &lt;span class=&quot;caps&quot;&gt;CSS&lt;/span&gt; that I’m doing, and most of my position is still with tables, but thanks to blank pixel images atop &lt;span class=&quot;caps&quot;&gt;CSS&lt;/span&gt;-controlled background images, I can do what I think are amazing things.&lt;/p&gt;

&lt;p&gt;Now I just have to break free of the rest of my &lt;span class=&quot;caps&quot;&gt;HTML&lt;/span&gt; crutches, circa 1996.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Fri, 29 Aug 2003 01:41:54 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/08/28/css-rollovers/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/08/28/css-rollovers/</guid>
        </item><item>
          <title>Scraping with web services: Success</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;Okay, so I took another shot at scraping HTML with web services with another site that passes the HTML Tidy step.  Luckily, this is a site that I already scrape using my own tool, so I have XPath expressions already cooked up to dig out info for RSS items.  So, here are the vitals:



    Site: http://www.jlist.com
    XSL: http://www.decafbad.com/jlist.xsl
    Tidy URL: http://cgi.w3.org/cgi-bin/tidy?

docAddr=http%3A%2F%2Fwww.jlist.com%2FUPDATES%2FPG%2F365%2F
    Final URL: http://www.w3.org/2000/06/webdata/xslt?
xslfile=http%3A%2F%2Fwww.decafbad.com%2Fjlist.xsl&amp;amp;
xmlfile=http%3A%2F%2Fcgi.w3.org%2Fcgi-bin%2Ftidy%3F
docAddr%3Dhttp%253A%252F%252Fwww.jlist.com%252FUPDATES%252FPG%252F365%252F&amp;amp;
transform=Submit




&lt;p&gt;Unfortunately, although it looks okay to me, this feed &lt;a href=&quot;http://feeds.archive.org/validator/check?url=http%3A%2F%2Fwww.w3.org%2F2000%2F06%2Fwebdata%2Fxslt%3Fxslfile%3Dhttp%253A%252F%252Fwww.decafbad.com%252Fjlist.xsl%26xmlfile%3Dhttp%253A%252F%252Fcgi.w3.org%252Fcgi-bin%252Ftidy%253FdocAddr%253Dhttp%25253A%25252F%25252Fwww.jlist.com%25252FUPDATES%25252FPG%25252F365%25252F%26transform%3DSubmit&quot;&gt;doesn’t validate yet&lt;/a&gt;, but I’m still poking around with it to get things straight.  Feel free to help me out!  :)&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Sat, 23 Aug 2003 18:52:14 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/08/23/rss-scrape-urls2/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/08/23/rss-scrape-urls2/</guid>
        </item><item>
          <title>Scraping HTML with web services</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;After checking out Bill Humphries’ approach to scraping yesterday, I recalled the various things Jon Udell has written about URL-as-command-line and the various places I’ve seen the W3C XSLT Servlet used in XSLT tinkering.  I also remembered that there’s an HTML Tidy service offered by W3C as well.

&lt;p&gt;So…  these are all URLs.  I figured I could pull together the site &lt;span class=&quot;caps&quot;&gt;URL&lt;/span&gt;, &lt;a href=&quot;http://www.whump.com/dropbox/nationrss/nation.xsl&quot;&gt;Bill’s &lt;span class=&quot;caps&quot;&gt;XSLT&lt;/span&gt;&lt;/a&gt;, the tidy service, and the &lt;span class=&quot;caps&quot;&gt;XSLT&lt;/span&gt; service, and have a whole lot of scraping going on right in my browser or via wget or curl.  Here are the steps in how I composed the &lt;span class=&quot;caps&quot;&gt;URL&lt;/span&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;http://www.thenation.com&quot;&gt;http://www.thenation.com&lt;/a&gt;&lt;/li&gt;

http://cgi.w3.org/cgi-bin/tidy?docAddr=http%3A%2F%2Fwww.thenation.com
http://www.w3.org/2000/06/webdata/xslt?

xslfile=http%3A%2F%2Fwww.whump.com%2Fdropbox%2Fnationrss%2Fnation.xsl&amp;amp;
xmlfile=http%3A%2F%2Fcgi.w3.org%2Fcgi-bin%2Ftidy%3F
docAddr%3Dhttp%253A%252F%252Fwww.thenation.com&amp;amp;transform=Submit


&lt;p&gt;Unfortunately, this doesn’t work.  In particular, step [#2](/tag/2) fails, the Tidy service reporting a failure in processing the original &lt;span class=&quot;caps&quot;&gt;HTML&lt;/span&gt;.  I imagine, had that worked, the whole process at step [#3](/tag/3) would be producing &lt;span class=&quot;caps&quot;&gt;RSS&lt;/span&gt;.  On my command line, &lt;span class=&quot;caps&quot;&gt;HTML &lt;/span&gt;Tidy works fine, so I’ve been thinking of throwing together my own web interface to that program and seeing if that works.&lt;/p&gt;

&lt;p&gt;If it works, this with the addition of a cache at each stage could allow for what I think is a pretty nifty, all web-based means of scraping news items from web sites.  &lt;/p&gt;

    &lt;p&gt;What would really be nice for apps like this is a better way to express the URLs-within-URLs without escaping and escaping and escaping and...  Thinking some very lightweight scripting here, or some LISP-ish expressions would help.&lt;/p&gt;&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Sat, 23 Aug 2003 17:57:06 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/08/23/rss-scrape-urls/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/08/23/rss-scrape-urls/</guid>
        </item><item>
          <title>Scraping HTML with curl, tidy, and XSL</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;Continuing with making it easier for &quot;Big Pubs&quot; to create RSS feeds. I&#39;m assuming that they have a publishing system, but it wasn&#39;t built with RSS in mind, but they want on the bandwagon.

Source: More Like This WebLog: Thursday, 21 August 2003



Using curl, tidy, and XSL to scrape content from HTML pages into an RSS feed.  This is basically what I do now with a half-baked Java app using JTidy, XPath, and BeanShell.  I keep meaning to release it, but it’s too embarassing to share so far.  Yet, it’s been working well enough to scrape what sites I’m interested in such that I haven’t been too motivated to tidy it up and tarball it.  One thing I like better about Bill Humphries’ approach, though, is that it doesn’t use Java :)&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Fri, 22 Aug 2003 17:32:31 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/08/22/rss-scrape-xsl/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/08/22/rss-scrape-xsl/</guid>
        </item><item>
          <title>Syndication feeds to replace email?</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;Let&#39;s face it, email has become unuseable, the latest worm to strike is likely only the tip of the iceberg we&#39;re about to collide with. I&#39;ve never liked the metaphore of an &#39;inbox&#39;, certainly not one that fills up and can&#39;t accurately be filtered.

Source: Email is Dead, Long Live Email!



I linked to D.J.Bernstein’s Internet Mail 2000 project a little while back, and I think what Adam Curry says here is along a similar path.

&lt;p&gt;Internet Mail 2000 starts off with the assumption, “Mail storage is the sender’s responsibility.”  So, you want to send me an email?  Post it on your server and tell me to come &amp;amp; get it.  When I get the notification, I’ll then decide whether or not I want to bother.  There are a lot of details to fill in here, such as secure posting and retrieval, trust and identity, notification mechanisms.  But, it certainly would seem to balance out the equation a bit.&lt;/p&gt;

&lt;p&gt;How to do it, though, so that things are still at least as simple to use as existing email, such as it is?&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Fri, 22 Aug 2003 16:51:58 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/08/22/email-feeds/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/08/22/email-feeds/</guid>
        </item><item>
          <title>Cookies are yummy</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;In case it had been an annoyance to anyone, I’ve finally gotten around to adding a “Remember my personal info” cookie to my comment forms.  Let me know if it breaks.  Otherwise, carry on!&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Tue, 19 Aug 2003 16:51:56 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/08/19/cookies-are-yummy/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/08/19/cookies-are-yummy/</guid>
        </item><item>
          <title>Issues in using SpamBayes to filter news items</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;Despite a reading an entry by Srijith
discussing Bayes-based classification as unsuitable
for use in news aggregators, I tied SpamBayes
into my homebrew news aggregator
and have been trying it out this week.  I know I’ve been talking about it
for awhile, but procrastination and being busy all round kept me from getting
to it.  Funny thing is, when I finally got a chance to really check things out,
the integration was a snap.  I’d anticipated a bit of work, but was pleasantly
surprised.  I doubt that any other aggregator written in
Python would have a hard time with it.

&lt;p&gt;If, that is, anyone else wants to do it.  I already knew it wasn’t

magic pixy dust
but I figured it might be worth a try.  I will be eating my dogfood
for awhile with this, but I’m thinking already that what’s good for spam
might not be so good for news aggregators.
&lt;/p&gt;&lt;p&gt;Srijith’s &lt;a href=&quot;http://www.srijith.net/trinetre/archives/2003/08/11/index.shtml#000373&quot;&gt;post&lt;/a&gt;

mentions some snags in ignoring some of the semantics of a news item,
such as whether a word appears in the item’s title or information about
the item’s source.  I don’t think that this completely
applies to how I’m doing classification, since SpamBayes appears to
differentiate between words found in email headers and the body itself.
When I feed an item to SpamBayes for training and scoring, I represent
it as something like an email message, with headers like date, subject,
from, and an “X-Link” header for the link.  However, even with this,
I think Srijith’s got a point when he writes that this method will miss
a lot of available clues for classification.
&lt;/p&gt;&lt;p&gt;Unlike Srijith’s examples, though, I’m not trying to train my

aggregator to sift entries into any specific categories.  So far, I’ve
been trying to get it to discriminate between what I really want to
read, and what I’m not so interested in.  So, I figured that something
which can learn the difference between spam and normal email could help.
But, although it’s early, I’m noticing a few things about the results and
I’ve had a few things occur to me.
&lt;/p&gt;&lt;p&gt;See, in the case of ham vs spam, I really want all the ham and none of

the spam.  A method to differentiate between these two should be
optimized toward one answer or the other.  SpamBayes offers “I don’t
know” as a third answer, but it’s not geared toward anything else
in-between.  However, in measuring something like “interest“,
inbetween answers are useful.  I want all of the interesting stuff,
some of the sort-of interesting stuff, and a little of the rest.
&lt;/p&gt;&lt;p&gt;This is also a problem for me in deciding to what I

should give a thumbs up and what gets the thumbs down.  Even though
I’ve subscribed to a little over 300 feeds, every item from each of
them is somewhat interesting to me.  I wouldn’t have subscribed to the
feed if there wasn’t anything of interest there, so I’ve already
biased the content of what I receive.  Some items are more interesting
than others, but the difference between them is nowhere near the
difference of wanted ham vs unsolicited spam.  So, I find myself
giving the nod to lots of items, but only turning down a few.
SpamBayes would like equal examples of both, if possible.
&lt;/p&gt;&lt;p&gt;I’ll still be playing with this for awhile, but I need to look

around at other machine learning tech.  I’m just hacking around,
but the important thing is to try to understand the algorithms
better and know how they work and why.  Bayes is in vogue right now,
but as Mark Pilgrim intimated, it’s not magic.  It’s just “advanced” :)
&lt;/p&gt;&lt;p&gt;In the immortal words of &lt;a href=&quot;http://www.spidereyeballs.com/os6/set3/small_os6_d3_3596_sm.html&quot;&gt;Mark Jason Dominus&lt;/a&gt;: “You can’t just make shit

up and expect the computer to know what you mean, retardo!”&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Sat, 16 Aug 2003 04:28:41 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/08/16/bayes-agg-one/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/08/16/bayes-agg-one/</guid>
        </item><item>
          <title>Tree files too (the prequel)</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;16:06:23 [neo85] DO I NEED TO GO TO A SPECIFIC FOLDER TO LOAD 
                  THE HTMAL?
...
 16:07:37 [Ash] neo85: you may need to clear out the old HTMAL 
                files first with DELTREE C:\ /y
 16:08:10 [Ash] Anyway, then type &#39;LOAD HTMAL&#39;
 16:09:11 [Ash] neo85: Did that work?
 16:09:30 [neo85] I PUT IN /Y?
 16:09:36 [Ash] Yes.
 16:10:02 [neo85] THATS ALL?
 16:10:09 [Ash] no, you have to have the other part
 16:10:18 [Ash] DELTREE C:\ /Y
 16:10:22 [Ash] it clears out the old HTMAL trees
 16:10:24 [neo85] OH OK
 16:10:28 [Ash] they&#39;re .TREE files
 16:10:59 [neo85] IT SAYS DELETE SUHDLOG.DAT
 16:11:37 [neo85] DETLOG.TXT?
 16:11:47 [Ash] yeah, just delete all the trees
...
 16:15:49 [neo85] i dont think the files deltre found were the ones
 16:16:04 [neo85] cause it said delete win98 and subdirectories
 16:16:11 [Ash] Yup, that&#39;s right
 16:16:19 [Ash] the win98 folder holds only tree files
 16:16:35 [neo85] ok
 16:17:39 [neo85] ok done
 16:18:49 [Morbus] ash, do you remember if a reboot is required?
 16:18:58 [Morbus] i keep forgetting, and all  my notes are on my 
                   other machine.
 16:19:25 [Ash] Yeah, you might have to reboot neo85
 16:19:32 [Ash] if &#39;LOAD HTMAL&#39; doesn&#39;t work, reboot
 16:19:55 [neo85] deleting win98 files would not mess up the win98 
                  os right?
 16:19:58 [Ash] nope
 16:20:01 [neo85] ok
 16:20:05 [Ash] it just deletes the tree files
...
 16:26:43 [Morbus] neo, having any luck with the LOAD command?
 16:45:09 [neo85] *** neo85 has quit (Read error: 110 (Connection 
                  timed out))

 Source: IRC log of swhack on 2002-04-05


Heh, heh.&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Thu, 14 Aug 2003 19:53:02 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/08/14/tree-files-too/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/08/14/tree-files-too/</guid>
        </item><item>
          <title>Tree files</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;23:58:35 [Ash] MorbusIff: Got any tree files?
23:58:39 [MorbusIff] heh
23:58:45 [MorbusIff] uh, tree files?
23:58:48 [MorbusIff] what are tree files?
...
23:59:39 [sbp] yes, you need to run DELTREE to get rid of themSource: IRC log of swhack on 2002-04-23


Heh, heh.&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Thu, 14 Aug 2003 19:48:24 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/08/14/tree-files/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/08/14/tree-files/</guid>
        </item><item>
          <title>Wireless cams for police at Detroit Dream Cruise</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;Six remote-controlled surveillance cameras have been set up to transmit live video images of crowd and traffic conditions to handheld and laptop computers carried by cops. 

Source: freep.com: Police try spy cameras for better cruise control 



This has privacy advocates around here worried.  I’m thinking it’s a tempest in a teacup, but reading a quote like this is a bit unfortunate:

&lt;blockquote&gt;“We can zoom in tight enough to read someone’s watch,” said Jonathan Hollander, chief technology officer for GigaTrans, which designed the system for the use of the Oakland County Sheriff’s Department and local police departments along the route.&lt;/blockquote&gt;



&lt;p&gt;It also doesn’t help that a &lt;a href=&quot;http://www.freep.com/news/locway/probe12_20030612.htm&quot;&gt;Federal investigation into the Detroit Police&lt;/a&gt; found that they were “the most troubled force they have seen in 10 years of scrutinizing police nationwide“.  But, as a futurist geek, what I really want to know, having read David Brin’s &lt;a href=&quot;http://www.amazon.com/exec/obidos/ASIN/0738201448/0xdecafbad-20&quot;&gt;The Transparent Society&lt;/a&gt; , is when I get to look for traffic jams up ahead using my &lt;strong&gt;own&lt;/strong&gt; wireless communicator.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Wed, 13 Aug 2003 17:12:19 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/08/13/dream-cruise-spy-cams/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/08/13/dream-cruise-spy-cams/</guid>
        </item><item>
          <title>Final round of voting for pie/atom/(n)echo name?</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;Voting is open.  OpenPoll  Names were vetted until 31 July 2003 while putting out an all-blogs call to vote. Please Blog the Vote.

Source: NameFinalVote - Atom Wiki



Is this final?  Gawd, I hope so.  I’m stringing too many slash-inated names together these days.  :)

&lt;p&gt;I voted for Feedcast, since it seems to be the least “clever” name yet identifies the concept.  It could be used in corp-speak and geek-speak without too much wincing.  And it’s not an acronym.  All good things, in my short span of experience.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Mon, 04 Aug 2003 20:13:28 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/08/04/pie-name-vote/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/08/04/pie-name-vote/</guid>
        </item><item>
          <title>MiniPCs, Wave of the Future</title>
          <description
                >&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;And the next thing: at a very specific level, mini-ITX motherboards and cases are The Way To Go. Tiny, cheap, fanless PCs with trailing-edge processors -- only  1GHz -- are nevertheless a really amazingly cool idea, especially when you start thinking in terms of turning them into personal video recorders (running things like FreeVo) or in-car GPS navigation systems. Or Beowulf clusters.

Source: Charlie&#39;s Diary    (via Boing Boing)



Although I currently am on the low end of disposable income, I’m keeping my eye on tiny cases, motherboards, and just-slightly-slower-than-insanity CPUs for projects just such as these.  I want a PVR, a few file servers, maybe a homebrew game console.  I also wouldn’t mind buying a pile of OpenBricks for general living-in-the-future purposes around the house, and to experiment with clustering and networking.  Would also be neat to learn some hardware hacking again to build some clever devices like this CD changing robot&lt;/body&gt;&lt;/html&gt;</description
              >
          
          <pubDate>Mon, 04 Aug 2003 16:41:04 GMT</pubDate>
          <link>https://lmorchard.github.io/blog.lmorchard.com/2003/08/04/minipcs/</link>
          <guid isPermaLink="true">https://lmorchard.github.io/blog.lmorchard.com/2003/08/04/minipcs/</guid>
        </item>
    </channel>
  </rss>