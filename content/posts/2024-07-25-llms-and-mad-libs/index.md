---
title: Large Language Mad Libs
tags:
  - ai
  - llm
  - writing
---

**TL;DR**: Might Large Langeuage Models (LLMs) be like Chinese Rooms that generate new completions of old Mad Libs?

<!--more-->

Here are some thoughts I'm entertaining while developing my practical understanding of LLMs:

Think about words and symbols. A symbol (or word) encapsulates a concept with great economy. We continually invent new symbols to encompass ever more complex conceptual patterns - most often composed, recursively, of previously invented symbols. Some symbols, if you could "double-click" to expand their full context, would result in entire library shelves of explanation.

Some conceptual patterns are too enormous to fit in the working memory of even the most brilliant human brains. Folks can still discover them and invent symbols to capture them - but the process drags as soon as we have to start using external memory prostheses like writing or social interaction. Sometimes, the invention of a new symbol takes multiple lifetimes.

What if we had an automated mechanical process that could tease out new symbols from a given soup of symbolic representation? What if we could grant it super-human working memory and processing speed? And then, what if we let it rip on the largest corpus of human language we can assemble? Could such a process uncover patterns and invent symbols that humans might otherwise never have the time or capacity to pursue?

This is how I'm thinking about Large Language Models - i.e. as something close to the above process. The frustrating thing is that, as far as I understand the current state of the art, LLMs stop just short of the whole shebang. Patterns entrained into LLMs do not get encapsulated as symbols we can directly access and share. They're represented as weights in a neural network that we can't interpret.

But, we can - with mixed success - access the nascent symbols in LLMs through the completion of text prompts engineered to activate parts of that neural network. They're not thinking, per se, but rather shunting into incredibly complex Mad Libs templates of symbolic representations based on the past thoughts of countless humans.

Now, I think it'd be great if, somehow, a future generation of machine learning could close the loop on symbol invention in a way that's legible to us. I don't even really know how that'd work - I imagine if I did, I'd be on my way to my first billion.

But still, I think this capacity to pattern-match and finish the apparent trajectory of your thoughts is not an entirely useless parlor trick. When it's firing on all cylinders, it can be second best to having a head sloshing full of rich symbols.

Like, you ever talk to someone who can complete your sentences because they're so tuned into your wavelength? Someone who can already effectively quote your conclusion, less than a paragraph into your TED Talk? LLMs seem to be getting pretty good at doing that with rather high-conceptual sophistication.

We're currently wasting LLMs on things like the production of slop to maximize the saleable harvest of human attention. This, along with cynical attempts to replace human effort & creativity. But, I can imagine a future where this stuff settles more into helping us think better and faster.

Maybe LLMs can help us get to conclusions more efficiently by clearing away intervening steps that countless humans have already taken, even if you can't quite access the discrete concepts along the way? Maybe, as a converational intermediary, LLMs could help conjure up that feeling of having a conversation with someone who's already on your wavelength by suggesting and filling in implicit context and connections along the way?

As far as I understand LLMs so far - and that's admittedly a process in motion - I don't really expect them to lead to any kind of Artificial General Intelligence. That reads like complete hype to me. But, if we can clear away the crud and mania, I can see them as a useful tool for helping us think and communicate more effectively by reaching shared context and conclusions more efficiently.
