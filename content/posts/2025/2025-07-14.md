8<--- { "draft": false, "title": "Miscellanea for 2025-07-14", "time": "23:59:00-07:00", "type": "miscellanea", "slug": "miscellanea", "tags": [ "miscellanea" ] }

- Hello world!
- Spent a chunk Sunday doing [inadvisable things](https://switch.hacks.guide/) with my Nintendo Switch to back up all my downloaded games and game saves.
	- I think this risks getting the console banned from Nintendo's online services? But, the gist I've gotten from Reddit [and elsewhere](https://nx.eiphax.tech/ban.html) is that they seem to reserve that hammer for folks who actually pirate, hack, and cheat in multiplayer games.
	- Me, I just want to dump stuff to my NAS for later restoration if Something Bad happens. I did this with my 3DS, back when the eShop was on its way out. I've since been able to wipe and restore all my games on there without Nintendo's help. That makes me happy.
	- Since the Switch 2 is out, I figure the Switch 1 is on the verge of end-of-life. So, now seems like a good time to jailbreak the thing, even if I risk it getting cut off from the mothership.

8<--- { "draft": true, "title": "On Mental Models vs LLM Context in Programming", "slug": "mental-model-metr", "tags": [ "llms", "codegen", "ai" ], "time": "10:30:15-07:00" }

On that METR paper [on which I commented](https://blog.lmorchard.com/2025/07/10/ai-tools-slowdown/), last week, [John Whiles writes](https://johnwhiles.com/posts/mental-models-vs-ai-tools):

> We know that the programmers in Metr's study are all people with extremely well developed mental models of the projects they work on. And we also know that the LLMs they used had no real access to those mental models. The developers could provide chunks of that mental model to their AI tools - but doing so is a slow and lossy process that will never truly capture the theory of the program that exists in their minds. By offloading their software development work to an LLM they hampered their unique ability to work on their codebases effectively.

This analysis jibes well with me. This goes back to what I think is the hot issue in AI-assisted coding tools right now - i.e. "[context management](https://docs.anthropic.com/en/docs/build-with-claude/context-windows)". In my experience so far, these tools operate best when they're given well-constructed context and description of the work to be done.

But, yeah, that takes time and effort that's often orthogonal (or at least at an oblique angle) to the task of writing the code. That is, writing documentation and composing an explicit plan - getting the wordless thought-forms expressed in roughly appropriate word-symbols. And this, for a bot that may or may not actually do the needful, once supplied with the context.

I could definitely see how - for folks who can manage the context in their own heads as abstract notions - just writing the code is faster. LLMs aren't mind-readers and their context windows are not a real match for human headspace.

> It's common for engineers to end up working on projects which they don't have an accurate mental model of. Projects built by people who have long since left the company for pastures new. It's equally common for developers to work in environments where little value is placed on understanding systems, but a lot of value is placed on quickly delivering changes that mostly work. In this context, I think that AI tools have more of an advantage. They can ingest the unfamiliar codebase faster than any human can, and can often generate changes that will essentially work.

So, this is what I think is the ironic part: Onboarding engineers often have difficulty getting to the point of forming accurate mental models because the veteran engineers who *had* accurate mental models never wrote much down. So, onboarding engineers have to start approximately from scratch. ðŸ™ƒ

Practically speaking, LLMs are pretty good at transforming symbols of code into roughly appropriate word-symbols that likely resemble what could have been inspired by the wordless thought-forms in the head of the original programmer - had they ever bothered to write them down. That is what LLMs are for, after all: symbol transformation and sequence completion.

> Okay, so if you don't have a mental model of a program, then maybe an LLM could improve your productivity. However, we agreed earlier that the main purpose of writing software is to build a mental model. If we outsource our work to the LLM are we still able to effectively build the mental model? I doubt it
> 
> So should you avoid using these tools? Maybe. If you expect to work on a project long term, want to truly understand it, and wish to be empowered to make changes effectively then I think you should just write some code yourself. If on the other hand you are just slopping out slop at the slop factory, then install cursor and crack on - yolo.

I think this is the core good advice - and also why I think this stuff works pretty well for greenfield explorations, too: Do you need to get in and do something quickly? Prove out a concept before investing more time? Want to grind through some backlog tedium? In these cases, an LLM-powered coding assistant can work like a power tool, mechanically grinding through what probably ends up being mad-libs boilerplate work that doesn't demand the highest in human creativity.

If you need help figuring out where to even start with a new project, an LLM can be pretty great at mapping out the big points of interest in a code base to get you started. From there, 

If you need to do some big thinking about architecture, tricky requirements, and fussy constraints - that's a job for human brains. LLMs can advise by surfacing what other folks have written about conventions and best practices in the past - but you'll need to verify all of that for yourself anyway, given that LLMs can hallucinate and confabulate at the edges. 
