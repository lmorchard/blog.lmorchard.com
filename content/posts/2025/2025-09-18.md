8<--- { "draft": false, "title": "Miscellanea for 2025-09-18", "time": "23:59:00-07:00", "type": "miscellanea", "slug": "miscellanea", "tags": [ "miscellanea" ] }

- Hello world!
- I should write something here, just to get something written.
- I seem to join discord servers aspirationally like I used to buy domain names
- Just found this [Obsidian Kindle Plugin](https://github.com/hadynz/obsidian-kindle-plugin) that does what it says on the tin.
	- I've read a lot on a Kindle over the past 10 years at least.
	- But, my pace has accelerated lately since I've reliably carved out 30 minutes every weekday morning to ride an exercise bike and read while I'm doing it.
	- The highlight feature always seemed neat but pointless, because I never really looked into how to uplift that data into some other system.
	- But, this plugin did a nice job of pulling down all the highlights and notes from books I've read on my Kindle.
	- So now, I have nice markdown files for all my books, ready to copy/paste into blog posts if I manage to motivate myself to do it.
- Apropos of that plugin, I just finished reading [*Co-Intelligence: Living and Working with AI* by Ethan Mollick](https://www.penguinrandomhouse.com/books/741805/co-intelligence-by-ethan-mollick/)—maybe I should try collecting some highlights and thoughts around that?
	- Probably worth its own standalone post, rather than trying to clutter up a miscellanea post like this.
	- Off the top of my head, I thought this book was pretty great. Neither selling doom nor hype, rather realistic about where AI was at, circa April 2024.
	- That said, it all still applies now in September 2025, but it's worth noting that this stuff is still moving fast—I don't think we've quite hit the flattening of the [S-curve](https://en.wikipedia.org/wiki/Sigmoid_function) yet. 
		- Though, I do think we will find that flattening—and will have found that this stuff isn't a hockey stick.

8<--- { "draft": false, "title": "This engine has no horses", "slug": "ai-engine-horses", "tags": [ "ai", "llms" ], "time": "11:34:02-07:00" }

When explaining what LLMs are (and aren't), I've been using this analogy: AI isn't thinking, just like the horsepower in your car isn't horses—but it can still get you places.

An engine doesn't have horses inside. But, we needed to relate it to something folks understood at the time. Weirdly, now that we ride in cars more than on horses, the metaphor has lost its explanatory power—yet we keep using it. Likewise, we'll probably be talking about "thinking" in AI for awhile yet, likely long past any literal usefulness.

I appreciate this bit in [_Co-Intelligence: Living and Working with AI_ by Ethan Mollick](https://www.penguinrandomhouse.com/books/741805/co-intelligence-by-ethan-mollick/): "I'm about to commit a sin. And not just once, but many, many times. For the rest of this book, I am going to anthropomorphize AI." He's right to frame it as a 'sin'—and also right to do it anyway.

Engaging with an LLM isn't truly a conversation. But, when we use conversational patterns, we effectively mine the model for responses that are often useful.

If you play-act with the machine, it offers back thinking-like responses that aren't actually thinking—but nonetheless can still produce interesting connections and notions that resemble real information. With care from both AI developers and users, they might even match correct information that would be hard to find otherwise.

But you still need to keep your wits about you and remember you're not riding a horse. If you pass out on a horse, she still knows the way home. If you pass out in a car, you end up in a ditch. A horse knows to avoid the cliff; a car just follows your lead.