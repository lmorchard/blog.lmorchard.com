8<--- { "title": "Miscellanea for 2025-05-13", "time": "23:59:00-07:00", "type": "miscellanea", "slug": "miscellanea", "tags": [ "miscellanea" ] }

- Hello world!
- I'm in a weird place with this current AI wave in the tech industry.
	- The way LLMs have been trained is dubious at best in most cases.
	- The hype about their capabilities is often overinflated and hyperbolic.
	- And, of course, I'm feeling that my ongoing paycheck currently depends on my having deep literacy with these technologies while abiding all the unpleasantries.
	- Like, yeah yeah, Upton Sinclair, “[It is difficult to get a man to understand something, when his salary depends on his not understanding it.](https://en.wikiquote.org/wiki/Upton_Sinclair)”
	- But, I do understand it rather decently, distasteful parts and all - and my mortgage doesn't care. And, really, there's no bottom to the qualms I might have about working in tech in general if I fully committed to looking down. So, what's one more ethical violation under capitalism?
	- None of that's a justification. It's an entirely compromised position with which I'm continually uncomfortable.
	- All that said, it's also hyperbolic to say LLMs are entirely useless.
	- If we lived in a proper society, we could train these models in full consent and with proper reciprocal remuneration to the folks whose intellectual produce was fed into the machines.
	- The closest we get is that access to the models can be rather cheap and often run on your own hardware. [Not quite Open Source, even if they have assumed the label "open source"](https://www.technologyreview.com/2024/03/25/1090111/tech-industry-open-source-ai-definition-problem/) - but still rather tinkering friendly a lot of the time.
	- Once you do have access to LLMs, if you're careful and judicious and honest and savvy about it, they can be put to interesting use in labor saving devices for programming and even creative work.
	- An enormous as-yet-unanswered question is whether the value of their use outweighs all the costs. My hunch is they might, even if they don't measure up precisely to the heights of hype. A market correction lies ahead, more likely than not.
	- I don't think that LLMs think or that they're going to condense AGI out of the GPU heat exhaust ether. I do think they represent very powerful [Chinese rooms](https://en.wikipedia.org/wiki/Chinese_room) of probabilistic symbol manipulation that recontextualize, recapitulate, and reconstitute patterns of language.
	- Those patterns of language encode ghost traces of previous human thought and reasoning. And, using models that have managed to distill subtle rules out of enormous exemplar volumes of those patterns, we can remix and mad-lib our way to useful derivative continuations of those patterns.
	- Like, they may be "[spicy autocomplete](https://thecleverest.com/gpt3-is-just-spicy-autocomplete/)", but that spice is really potent. Maybe not like [what they have in Dune](https://dune.fandom.com/wiki/Spice_Melange), but maybe like [what they used to cross oceans to find](https://en.wikipedia.org/wiki/Nutmeg#Colonial_era).
	- (Of course, the Spice Melange came into use after the [Butlerian Jihad](https://dune.fandom.com/wiki/Butlerian_Jihad) destroyed all thinking machines. And, nutmeg is so common we sprinkle it on lattes in the fall. So, these comparisons are admittedly all very confusing.)
	- Anyway, there is a lot of power in having mechanisms by which to compute your way from encoded human intention to really quite close approximations of human-desired results.
	- Like, when I ask a coding assistant to write a function with english words and it continues from there to produce computer code that really very closely matches what I would have written - and in less time than it would have taken me to consult Stack Overflow and write it - that's pretty great actually?
	- Yeah, yeah, hallucinations and all that. But, if I keep my hand steady on the tiller, my personal experience has been that the confabulations aren't as big a showstopper as some folks claim. And they're increasingly easy to mitigate with context, guidance, and advances in model quality.
	
	