8<--- { "title": "Miscellanea for 2025-05-13", "time": "23:59:00-07:00", "type": "miscellanea", "slug": "miscellanea", "tags": [ "miscellanea" ] }

- Hello world!
- I'm in a weird place with this current AI wave in the tech industry. Drafting up some thoughts, maybe they'll turn into a post? I started just riffing here, but the riffing kept expanding, so I think I should give it some time to cook.

8<--- { "title": "What I'm thinking about AI and LLMs", "slug": "thinking-about-llms", "time": "15:15:00-07:00", "type": "entry", "tags": [ "ai", "llms", "ml", "dev", "career" ] }

I'm in a weird place with this current AI wave in the tech industry. I'm almost afraid to write about it. I feel like a good chunk of folks would tar & feather me, if I wrote anything but a complete denunciation. Another chunk of folks, I already blocked back when crypto & NFTs were all the rage. I still feel like writing something, though, if only to bounce it off the screen for myself.

<!-- more -->

The way LLMs have been trained [is problematic at best](https://allenpike.com/2024/llms-trained-on-internet). The hype surrounding LLM capabilities is often hyperbolic. If we lived in [a proper society](https://en.wiktionary.org/wiki/Fully_Automated_Luxury_Gay_Space_Communism), we could train these things with full consent and reciprocal remuneration to folks whose intellectual produce was fed into the machines. And we could be [normal about LLMs as a technology](https://www.fastly.com/blog/can-we-be-normal-about-ai-now-that-deepseek-happened).

Instead, we get what looks like [an extractive regime that disrupts actually generative ecosystems fueling it](https://blog.lmorchard.com/2024/03/11/dance-for-the-bots/). The best shrug toward that is that some LLMs are [almost, but not quite, entirely unlike "open source"](https://www.technologyreview.com/2024/03/25/1090111/tech-industry-open-source-ai-definition-problem/) (with apologies [to Douglas Adams](https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#Not_entirely_unlike)).

That said, unless I change careers, my continuing paycheck seems contingent on developing deep AI literacy while abiding the unpleasantries. Like, yeah, Upton Sinclair wrote, “[It is difficult to get a man to understand something, when his salary depends on his not understanding it.](https://en.wikiquote.org/wiki/Upton_Sinclair)” The thing is that I do understand it rather decently - and frankly my mortgage doesn't care.

On the other hand, it's worth noting that I've long been fascinated with intelligence - both artificial and natural. As a kid, I loved [Tron](https://en.wikipedia.org/wiki/Tron) and [Automan](https://en.wikipedia.org/wiki/Automan) and [Knight Rider](https://en.wikipedia.org/wiki/Knight_Rider). I read Douglas Hofstadter's [_Gödel, Escher, Bach_](https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach) in early high school - and then again in college, because my brain literally needed to grow to accommodate it. I also loved Marvin Minsky's [_Society of Mind_](https://en.wikipedia.org/wiki/Society_of_Mind) - which seems especially salient with agentic tech being all the rage. I double-majored in Computer Science and Psychology. I really wanted to major in [Cognitive Science](https://en.wikipedia.org/wiki/Cognitive_science). But, I decided to bail out and graduate to get a dot-com job, because the advanced math made me surly.

All of which is to say that I am admittedly susceptible to enthusiasm toward this wave of technology. Maybe even wishfully biased, though I'm trying to stay grounded.

So, maybe it's my bias talking, but I think it's also hyperbolic to say LLMs are entirely useless. If you're careful and judicious and honest and savvy, LLMs can be put to interesting use as labor saving devices and thought amplifiers. That's a lot of caveats, I know.

An enormous question is whether the value outweighs the costs. My hunch is it will, eventually, even if LLMs don't measure up precisely to the hype. A market correction lies ahead, more likely than not. Another question is whether, by-and-large, we'll use LLMs honestly and carefully. My hunch is we might, eventually. But, things are decidedly wild right now.

In the meantime, it's worth it to me, personally, to understand this stuff as fully as I can. So, here's what I understand so far: I don't think that LLMs think. I also don't think they're going to condense an AGI genie out of the ether of GPU exhaust. I do think the are very powerful [Chinese rooms](https://en.wikipedia.org/wiki/Chinese_room) of probabilistic symbol manipulation that recontextualize, recapitulate, and reconstitute patterns of language.

Those patterns of language roughly map to human thought & reasoning. I think that models that have managed to distill subtle rules out of mind-bogglingly enormous volumes of exemplar language. This enables them to remix and [Mad-Libs](https://en.wikipedia.org/wiki/Mad_Libs) their way to producing continuations that, for many practical purposes, compare favorably to what a real thinking person might have written.

Like, they may be "[spicy autocomplete](https://thecleverest.com/gpt3-is-just-spicy-autocomplete/)", but that spice is really potent. Not like [what they have in Dune](https://dune.fandom.com/wiki/Spice_Melange), but maybe like [what they used to cross oceans to find](https://en.wikipedia.org/wiki/Nutmeg#Colonial_era). Of course, the Spice Melange came into use after the [Butlerian Jihad](https://dune.fandom.com/wiki/Butlerian_Jihad) destroyed all thinking machines. And, nutmeg was bound up in a lot of colonial fuckery for what I now just sprinkle blithely on eggnog once a year. But, I digress.

There is utility in computing your way from encoded human intention to approximations of desired results. Or, translating to executable commands that can produce said results. It's even more powerful when such a system can accept intentions encoded just like humans converse with each other - or with themselves.

For instance, when I ask a coding assistant to write a function using English words and it continues from there to produce computer code that very closely matches what I would have written anyway - and in less time than consulting docs and Stack Overflow - that's pretty great actually?

Sure, "hallucinations" are a thing. But, if I keep my hand steady on the tiller, my personal experience has been that the confabulations aren't as big a showstopper as some folks claim. If you provide a lot of context, narrow the range of acceptable outputs, and test the results - you can keep the thing pretty well on task. 

This, by the way, is what I think is key to good LLM use: the results have to be tested & verified - ideally via automated evaluation, but otherwise via human inspection.

And then there's the latest waves of [agentic AI](https://en.wikipedia.org/wiki/Agentic_AI). They're not magic, either. It just turns out that some of what we do - especially in building software - can be patently rote: Iterate on some code, build & test, observe the results. If there are errors, fix them. If the errors don't make sense, hit up Google and then fix. If the results match expectations, move onto the next task. Lather, rinse, repeat.

With guiding prompts and a rich set of context, an LLM can spicily autocomplete from previous code & results to new code & commands that someone like you would likely have produced, in order to move the work forward. And if you [turn off your targeting computer](https://www.starwars.com/video/use-the-force-luke) (ironic, I know) and let the system run those commands directly, it can effectively emulate an iterative process.

The LLM doesn't even really "know" what it's doing. It's just that an enormous number of humans have done this before and written about it. So, an LLM can [Mad-Libs](https://en.wikipedia.org/wiki/Mad_Libs) its way toward a reconstituted flow appropriate to current contextual conditions.

That said, I don't think [pure vibe coding](https://simonwillison.net/2025/Mar/19/vibe-coding/) is a great idea past some initial tentative exploration and goofing around. Not yet, maybe not ever. Again, [keep your hand steady on the tiller](https://simonwillison.net/2025/Mar/19/vibe-coding/#using-llms-for-code-responsibly-is-not-vibe-coding) and take responsibility for the output of the tools.

To sum up, I think there's a *there* there, in LLMs and agents. There's a bunch I didn't cover here and a bunch that I'm still thinking through. It's a fraught and compromised *there*, to be sure. I guess that places me on a *side* for some folks? So it goes.

In another universe - one wherein I wasn't such a decently paid computer dork - I might sit it out until the dust settles. Maybe carpent a few dining tables and chairs. But, here & now, I am who I am and this stuff honestly fascinates me. So, I'm likely to keep tinkering and write some more about it here in the near future.
