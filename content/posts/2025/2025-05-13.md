8<--- { "title": "Miscellanea for 2025-05-13", "time": "23:59:00-07:00", "type": "miscellanea", "slug": "miscellanea", "tags": [ "miscellanea" ] }

- Hello world!
- I'm in a weird place with this current AI wave in the tech industry. Drafting up some thoughts, maybe they'll turn into a post? I started just riffing here, but the riffing kept expanding, so I think I should give it some time to cook.
- And, indeed, I went ahead and posted a separate entry on [what I'm thinking about AI and LLMs](https://blog.lmorchard.com/2025/05/13/thinking-about-llms/)! Maybe too many words that no one will read, but I wanted to get it out of my head for future noodling.
- [feedsmith](https://github.com/macieklamberski/feedsmith): "Robust and fast parser and generator for RSS, Atom, JSON Feed, and RDF feeds, with support for Podcast, iTunes, Dublin Core, and OPML files."
	- Well, that's relevant to my interests. Might be worth replacing my half-baked RSS template on this blog with that, at least.

8<--- { "title": "What I'm thinking about AI and LLMs", "slug": "thinking-about-llms", "time": "15:15:00-07:00", "type": "entry", "tags": [ "ai", "llms", "ml", "dev", "career" ] }

I'm in a weird place with this current AI wave in the tech industry. I feel like a good chunk of folks would tar & feather me if I wrote anything but a complete denunciation, while another chunk I already blocked during the crypto & NFT craze. I still feel like writing something, though, if only to bounce it off the screen for myself.

<!--more-->

The way LLMs have been trained [is problematic at best](https://allenpike.com/2024/llms-trained-on-internet). In [an ideal world](https://en.wiktionary.org/wiki/Fully_Automated_Luxury_Gay_Space_Communism), we could train these systems with full consent and reciprocal compensation for the intellectual produce they consume. And we could be [normal about LLMs as a technology](https://www.fastly.com/blog/can-we-be-normal-about-ai-now-that-deepseek-happened). Instead, we have [an extractive regime disrupting the very ecosystems fueling it](https://blog.lmorchard.com/2024/03/11/dance-for-the-bots/).

That said, my continuing paycheck seems contingent on developing AI literacy while abiding these unpleasantries. As Upton Sinclair wrote, "[It is difficult to get a man to understand something, when his salary depends on his not understanding it.](https://en.wikiquote.org/wiki/Upton_Sinclair)" The thing is, I do understand it rather decently - but my mortgage doesn't care, and I can't retire for another 30 years at least.

I've long been fascinated with both artificial and natural intelligence.As a kid, I loved [Tron](https://en.wikipedia.org/wiki/Tron) and [Automan](https://en.wikipedia.org/wiki/Automan) and [Knight Rider](https://en.wikipedia.org/wiki/Knight_Rider). I read Hofstadter's [_Gödel, Escher, Bach_](https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach) and Minsky's [_Society of Mind_](https://en.wikipedia.org/wiki/Society_of_Mind) in high school, double-majored in Computer Science and Psychology in college. I almost majored in [Cognitive Science](https://en.wikipedia.org/wiki/Cognitive_science), but the advanced math made me surly. So I'm admittedly susceptible to enthusiasm toward this technology, though I'm trying to stay grounded.

I think it's hyperbolic to say LLMs are entirely useless. Used carefully, judiciously, and honestly, they can function as labor-saving devices and thought amplifiers. That's a lot of caveats, I know. The enormous question is whether the value outweighs the costs. My hunch is it will, eventually, even if LLMs don't measure up to the hype. A market correction likely lies ahead.

That said, here's what I understand so far: LLMs aren't thinking, nor will they conjure an AGI genie from GPU exhaust. They're powerful [Chinese rooms](https://en.wikipedia.org/wiki/Chinese_room) that manipulate symbols probabilistically, recontextualizing and reconstituting patterns of language that roughly map to human thought.

They may be "[spicy autocomplete](https://thecleverest.com/gpt3-is-just-spicy-autocomplete/)", but the spice is potent. There's real utility in computing from encoded human intention to approximations of desired results, especially when the system can accept intentions expressed conversationally.

When a coding assistant produces code that closely matches what I would have written anyway—and faster than consulting docs and Stack Overflow—that's valuable. Yes, "hallucinations" happen, but with sufficient context, narrowed outputs, and result testing, they're manageable. That's the key to good LLM use: results must be tested and verified, ideally via automation but otherwise through human inspection.

Even [agentic AI](https://en.wikipedia.org/wiki/Agentic_AI) isn't magic. It turns out some of what we do—especially in building software—can be patently rote. With guiding prompts and rich context, an LLM can effectively emulate an iterative development process. It doesn't "know" what it's doing; it's just that countless humans have done this before and written about it. So, it can effectively emulate an iterative process by [Mad-Libs](https://en.wikipedia.org/wiki/Mad_Libs)'ing its way through with current context.

That said, I don't think [pure vibe coding](https://simonwillison.net/2025/Mar/19/vibe-coding/) is wise beyond initial exploration. Keep a steady hand on the tiller and take responsibility for the output.

In sum, I think there's a _there_ there with LLMs and agents—fraught and compromised though it may be. In another universe, I might wait until the dust settles, maybe build furniture instead. But here and now, this stuff genuinely fascinates me, so I'll likely keep tinkering and writing about it.