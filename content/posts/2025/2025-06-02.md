8<--- { "draft": false, "title": "Miscellanea for 2025-06-02", "time": "23:59:00-07:00", "type": "miscellanea", "slug": "miscellanea", "tags": [ "miscellanea" ] }

- Hello world!
- Jotted down a couple posts today on AI stuff that aren't particularly revelatory.
	- If anything, they're just me trying to think out loud and clarify.
	- I'm probably going to try writing more stuff like this, if only to be Wrong on the Internet and lure someone in to correct me. ðŸ˜…

8<--- { "draft": false, "title": "The Bomb Still Works: On LLM Denial and Magical Thinking", "slug": "the-bomb-still-works", "tags": [ "llms", "ai", "ml" ], "time": "12:20:52-07:00" }

I found myself in a frustrating argument with someone convinced that LLMs are pure vaporwareâ€”incapable of real work. Their reasoning? Since LLMs were trained on stolen material, the results they produce can't actually exist.

Not that the results should be considered illegitimate or taintedâ€”but that they're literally impossible. That the training data's questionable origins somehow prevents the technology from functioning at all.

I couldn't convince them otherwise. But, life isn't fair and **both things can be true simultaneously**: the origin of something can be problematic _and_ the results can be real.

This analogy kept coming to mind: If someone steals materials to build a bomb and successfully builds it, they have a functioning bomb. The theft doesn't retroactively prevent the bomb from existing or reduce its explosive capability. Proving the theft might help with future bombs or justify going after the bomb-maker, but it doesn't cause the current bomb to magically self-dismantle.

This seems obvious to meâ€”embarrassingly so. Yet I keep encountering this form of reasoning about LLMs, and it strikes me as a particular kind of denial.

There's something almost magical in the thinking: that moral illegitimacy can somehow negate physical reality. That if we disapprove strongly enough of how something was created, we can wish away its actual capabilities.

**The ethical questions around LLM training data are important and deserve serious discussion.** But pretending the technology doesn't work because we don't like how it was built isn't engaging with realityâ€”it's a form of wishful thinking that prevents us from dealing effectively with the situation we actually face.

Whether we like it or not, the bomb has been built. Now we need to figure out what to do about it.

8<--- { "draft": false, "title": "Why Prompt Engineering Isn't Just Good Writing", "slug": "prompt-engineering-is-real", "tags": [ "ai", "llms", "promptengineering" ], "time": "12:12:15-07:00" }

Someone told me that prompt engineering isn't realâ€”that it's just techbros rebranding "good writing" and "using words well." I disagree, and here's why:

Prompt engineering fundamentally differs from writing for human audiences because **LLMs aren't people**. When done rigorously, prompt engineering relies on automated evaluations and measurable metrics at a scale impossible with human communication. While we do test human-facing content through focus groups and A/B testing, the scale and precision (such as it is) here are entirely different.

The "engineering" aspect involves systematic tinkeringâ€”sometimes by humans tweaking language, sometimes by LLMs themselvesâ€”to activate specific emergent behaviors in models. Some of these techniques come from formal research; others are educated hunches that prove effective through testing.

**Effective prompts often resemble terrible writing.** The ritual forms, repetitions, and structural patterns that improve LLM performance would make a professional editor cringe. Yet they produce measurable improvements in evaluation metrics.

Consider adversarial prompts: they're often stuffed with tokens that are nonsense to humans but exploit specific model quirks. Here, the goal is explicitly to use language in ways that _aren't_ human-legible, making attacks harder to detect during review.

Good writing skills can help someone pick up prompt engineering faster, but mastering it requires learning to use words and grammar in weird, counterintuitive ways that are frankly sometimes horrifying.

All-in-all, prompt engineering may still be somewhat hand-wavy as a discipline, but it's definitely realâ€”and definitely not just rebranded writing advice.
