8<--- { "draft": false, "title": "Miscellanea for 2025-07-10", "time": "23:59:00-07:00", "type": "miscellanea", "slug": "miscellanea", "tags": [ "miscellanea" ] }

- Hello world!
- Trying to decide if this is a "[still alive](https://www.youtube.com/watch?v=Y6ljFaKRTrI)" or a "[beware, I live!](https://www.youtube.com/watch?v=CC2iqlvifSU)" kind of day?
- Either way, I've been down a hole of busy-ness for the past few weeks and been wanting to climb out to emit some reports here.
- Still trying to find a good balance while spinning plates across multiple projects. But, I've gotten a lot done without quite going entirely insane.
- Feel like I've suddenly become a cyborg over the past few weeks. Been working across multiple instances of Claude Code all day, every day.
- I've gotten well acquainted with Anthropic's usage limits, having managed to reliably burn through my Max plan allowances every 5 hours or so.
- Also feels like I've picked a side in the war against Skynet in some folks' estimation. Except, to me, it feels like working with a concussed version of the main computer from the USS Enterprise (NCC-1701-D).

8<--- { "draft": true, "title": "2025-07-10 @ 16:26:50", "slug": "", "tags": [ "" ], "time": "16:26:50-07:00" }

[Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity - METR](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/):

  > We conduct a randomized controlled trial \(RCT\) to understand how early-2025 AI tools affect the productivity of experienced open-source developers working on their own repositories. Surprisingly, we find that when developers use AI tools, they take 19% longer than without‚ÄîAI makes them slower.

This piece has been making the rounds today. I see a lot of skeptics pointing at it with an "ah hah!" and a smug nod. For me, this directly contradicts my experience over the past month of having been thrown into the deep end with a selection of AI coding floaties‚ÄîI'm finding I'm getting around in the pool just fine.

Maybe I've just ingested too much of the kool-aid. But I'd like to think I'm not entirely suffering from delusions. A few interesting things from reading the article, sort of quoted out of order as my brain comes up with things:

> we recruited 16 experienced developers from large open-source repositories (averaging 22k+ stars and 1M+ lines of code) that they‚Äôve contributed to for multiple years.

That's a rather small sample and a very specific scenario, isn't it?

A difference for my anecdotal situation (an even smaller sample) is that I've been engaged in a lot of new work. Greenfield or relatively young projects, though with technologies chosen specifically for both my experience with them and their overall maturity. But, decidedly *not* projects with over 1.1m lines of code. Maybe that's something?

> We do not claim that our developers or repositories represent a majority or plurality of software development work

Seems like this could be stated more emphatically? Though, I guess folks will do what they do when seeing headlines like this.

> We caution readers against overgeneralizing on the basis of our results. The slowdown we observe does not imply that current AI tools do not often improve developer‚Äôs productivity‚Äîwe find evidence that the high developer familiarity with repositories and the size and maturity of the repositories both contribute to the observed slowdown, and these factors do not apply in many software development settings. For example, our results are consistent with small greenfield projects or development in unfamiliar codebases seeing substantial speedup from AI assistance.

Ah, maybe this double-negative-enriched bit from the "Key Caveats" section basically jibes with my experience?

> Cursor does not sample many tokens from LLMs, it may not use optimal prompting/scaffolding, and domain/repository-specific training/finetuning/few-shot learning could yield positive speedup

The wording here is a bit tangled, but I *think* I get what they mean?

The bit about "may not use optimal prompting/scaffolding" is significant. 

When I've just charged into a rambling chat and autocomplete session with like Cursor, things steer into the ditch early and often.

>We expect that AI systems that have higher fundamental reliability, lower latency, and/or are better elicited (e.g. via more inference compute/tokens, more skilled prompt-ing/scaffolding, or explicit fine-tuning on repositories) could speed up developers in our setting (i.e. experienced open-source developers on large repositories).

They don't really get into *how* the developers used the tools in the study. Anecdotally (again), my experience has been that preparing context is key. 

I think this is significant: When I've worked with Claude Code through [a multi-step process](https://blog.lmorchard.com/2025/06/07/semi-automatic-coding/) of describing the problem; asking the agent to prompt *me* with a series of clarifying questions; reviewing the problem and considering a solution; breaking the solution down into parts; and then asking the agent to methodically execute the solution‚Äîthat's a yielded a lot of reliable success for me.

Tools and models and patterns of use‚Äîand various permutations of all the above‚Äîyield very different results these days. Things are far from stable or deterministic. I think it's a mistake to generalize from one combination‚Äîi.e. "primarily Cursor Pro with Claude 3.5/3.7 Sonnet"‚Äîto a statement on the entire market of such tools.

And then there's the waiting:

> All else equal, faster AI generations would result in developers being slowed down less. Qualitatively, a minority of developers note that they spend significant time waiting on AI to generate code. One developer notes that for ‚Äúlarger refactorings, [AI generation] takes a couple of minutes‚Äù. Another developer notes that when waiting on AI generations, he ‚Äúspends time on Twitter‚Äù. However, not all developers feel majorly affected by this time, for example, one developer notes that he was ‚Äúnever waiting for more than like 20 seconds.‚Äù

This is where my single-participant experience diverges, too: I don't really wait that much, because I've been working on multiple projects across several teams. When one agent instance is churning away on a request, I cycle to the next IDE window and see how things are going there. I'm spinning many plates.

Granted, this demands a lot of context switching and I will admit that's tiring. But, it also feels like it's helping me overcome a lot of ADHD activation energy barriers.

There've been days when I just sit there staring at the IDE window, poking my brain with a stick saying "c'mon, do something" and nothing happens for an hour or more. I'm not planning my next move, I'm just dissociating. My executive function often doesn't, like, function. At all.

Maybe it's the cycling novelty in all this that gets me going? I have liked task switching between prosing and coding. I enjoy spewing out a bunch of words to describe a problem, and then finding that the LLM appears to have "read" all of it‚Äîas evidenced by it echoing the intent back to me in either code or with follow-on questions. I enjoy finding that, while I was in another window, new things have happened in windows in the background for me to review.

I've also found that agents like Claude Code are mostly reliable at taking care of drudgery tasks. Those can seize me up for tens of minutes at a time. But usually, I can just tell the bot to do it, while I turn to more interesting stuff.

I guess what I'm saying is that *my own brain* makes me wait long periods of time before it starts generating useful results. So maybe this is just par for the course? üòÖ



8<--- { "draft": false, "title": "Building a Breakout clone with Claude", "slug": "breakout-with-claude", "tags": [ "codegen", "ai", "llms", "claude", "gamedev" ], "time": "11:11:01-07:00" }

To make steps toward showing and telling about my Claude Code workflow, I built a browser-based Breakout game with Phaser 3. The [repository](https://github.com/lmorchard/claude-breakout-clone) captures the full development process so far‚Äîprompts, commands, and session transcripts.

Along with the basic game, [I added a multi-ball power-up](https://github.com/lmorchard/claude-breakout-clone/tree/main/docs/dev-sessions/2025-07-05-1336-multiball) to demonstrate iterative development. The game itself isn't particularly novel, but the documented development process might be useful for others exploring AI-assisted coding workflows.

At some point, this will turn into a show-and-tell presentation for co-workers and maybe a follow-up to last month's blog post on ["Baby steps into semi-automatic coding"](https://blog.lmorchard.com/2025/06/07/semi-automatic-coding/).

This took all of a couple hours on a Saturday afternoon on the couch watching TV, but I kind of want to keep going with it. It's rather addictive to just kind of riff on ideas and get them into the game with quick little iterations.

8<--- { "draft": false, "title": "Progress on Pebbling Club", "slug": "progress-on-pebbling-club", "tags": [ "pebblingclub", "codegen", "claude" ], "time": "11:54:03-07:00" }

I've been procrastinating getting back to it, but I finally threw some hours into a substantial overhaul of my [Pebbling Club](https://github.com/lmorchard/pebbling-club) web link sharing project‚Äîthe first real efforts since December! Migrated [from SQLite to Postgres](https://github.com/lmorchard/pebbling-club/pull/239), [switched to uv](https://github.com/lmorchard/pebbling-club/pull/238) for dependency management, and moved deployment from fly.io to my basement machine running Docker Compose.

I built [my own git-push deployment post-receive hook](https://github.com/lmorchard/pebbling-club/blob/main/docker/compose/post-receive) because I'm a masochist‚Äîer, I mean I wanted complete control over the deployment process. It's nice watching your own server rebuild containers when you push to main, even if cloud platforms would be more practical.

The development environment [became a hybrid](https://github.com/lmorchard/pebbling-club/pull/240): Docker Compose for stable services, Honcho + Procfile for active development. Added Flower for Celery monitoring and experimented with Prometheus and Grafana metrics. (But, then, I [reverted django-prometheus](https://github.com/lmorchard/pebbling-club/pull/255) because it doesn't work at all like I thought it did.)

I got several useful features working: [RSS feed reading](https://github.com/lmorchard/pebbling-club/commits/main/pebbling_apps/feeds), [duplicate URL detection through normalized hashing](https://github.com/lmorchard/pebbling-club/pull/249), ActivityStreams-inspired [import](https://github.com/lmorchard/pebbling-club/pull/250)/[export](https://github.com/lmorchard/pebbling-club/pull/248), and a [Netscape Bookmarks HTML export](https://github.com/lmorchard/pebbling-club/pull/247) (for fun). Built [a link inbox](https://github.com/lmorchard/pebbling-club/pull/250) that currently handles RSS feeds, with in-progress work to add [Mastodon timeline integration](https://github.com/lmorchard/pebbling-club/pull/254) and plans for Bluesky.

Along the way, I wanted to see how far I could get with Claude Code and make tweaks to my overall process. If nothing else, the bot helped me get past the barrier of activation energy to get some things done that I've put off for most of a year. The bot wrote a bunch of just-fine code‚Äîand where it was wrong, the wrongness motivated me to get it fixed and done myself.
