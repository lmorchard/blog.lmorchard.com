{
  "type": "aside",
  "title": "Beyond Text-only AI",
  "time": "15:28:39-07:00",
  "slug": "beyond-text-only-ai",
  "tags": [
    "ux",
    "llm",
    "ai"
  ],
  "attachments": [],
  "year": "2025",
  "month": "05",
  "day": "19",
  "isDir": false,
  "date": "2025-05-19T22:28:39.000Z",
  "postName": "2025-05-19",
  "html": "<p>Fatih Kadir Akın, \"<a href=\"https://blog.fka.dev/blog/2025-05-16-beyond-text-only-ai-on-demand-ui-generation-for-better-conversational-experiences/\">Beyond Text-Only AI: On-Demand UI Generation for Better Conversational Experience</a>\":</p>\n<blockquote>\n<p>What if AI systems could generate precisely the right interface components when needed, without requiring developers to build every possible UI variation in advance? This post explores my work on a new prototype that enables LLMs to create dynamic, interactive UI components on demand, transforming how users interact with AI systems and MCP servers.</p>\n</blockquote>\n<p>This is where LLMs are very interesting to me, where applications with them are most able to escape hype into practical building. A thing they're good at is taking some of the vague stuff we hand-wave at like \"you know, that thing with the days in squares on it\" and jumping to \"you mean a calendar?\" But then, skip straight from that to presenting a calendar for input as part of the workflow.</p>\n<p>I'm finding LLMs most handy when they work in smaller universes. Give it a relatively small inventory of UI components, properly described with context, and they can be great at picking which one to use and when. Hard to hallucinate when there's not a wide range for wandering, easy to mitigate what hallucinations might occur when you can automatically kick it back into bounds. Meanwhile, the workflow can feel more smooth and demand less fiddly navigation from a user. That seems like a really promising glimmer to me.</p>\n",
  "body": "\nFatih Kadir Akın, \"[Beyond Text-Only AI: On-Demand UI Generation for Better Conversational Experience](https://blog.fka.dev/blog/2025-05-16-beyond-text-only-ai-on-demand-ui-generation-for-better-conversational-experiences/)\":\n\n> What if AI systems could generate precisely the right interface components when needed, without requiring developers to build every possible UI variation in advance? This post explores my work on a new prototype that enables LLMs to create dynamic, interactive UI components on demand, transforming how users interact with AI systems and MCP servers.\n\nThis is where LLMs are very interesting to me, where applications with them are most able to escape hype into practical building. A thing they're good at is taking some of the vague stuff we hand-wave at like \"you know, that thing with the days in squares on it\" and jumping to \"you mean a calendar?\" But then, skip straight from that to presenting a calendar for input as part of the workflow.\n\nI'm finding LLMs most handy when they work in smaller universes. Give it a relatively small inventory of UI components, properly described with context, and they can be great at picking which one to use and when. Hard to hallucinate when there's not a wide range for wandering, easy to mitigate what hallucinations might occur when you can automatically kick it back into bounds. Meanwhile, the workflow can feel more smooth and demand less fiddly navigation from a user. That seems like a really promising glimmer to me.",
  "parentPath": "./content/posts/2025",
  "path": "2025/05/19/beyond-text-only-ai",
  "prevPostPath": "2025/05/19/gamedev-reading",
  "prevPostTitle": "I like (reading about) making video games",
  "nextPostPath": "2025/05/19/foggy-blogging",
  "nextPostTitle": "Foggy Blogging"
}