
carl svensson, [Is Winter Coming?](https://www.datagubbe.se/winter/):

> Even when it comes to something like LLM-assisted programming, where a highly skilled developer can maybe, sometimes, somewhat gain a performance boost, the most pertinent question isn't if it can be done at all - but rather if what can be done good enough can also be done profitably. The number of GPUs and the amount of increasingly expensive energy required remains as unclear as the time frame needed to accomplish it. 

I don't disagree that this is where things are right now. Personally, I'm getting utility from these tools - mainly for things that I could do myself, but the LLM lets me carve off a few rounds of googling and Stack Overflow browsing. That's valuable. Probably not as valuable as investors are hoping, though.

I see some glimmers of potential in letting the agents rip in full-auto. But, I do also find that ofttimes they're [developer slot machines](https://prototypr.io/note/vibe-coding-cursor-windsurf-slot-machine) that need careful tending to keep them from going off into the weeds, going into loops, and going off on quests for which I never asked.

I'm seeing the most value in these tools when they're given a lot of context and kept constrained to limited tasks in a tight loop with adult supervision. I've got a hunch this will improve over time, but who knows? We could hit a wall and see winter soon?

I'd really like it if this stuff would settle down and be [normal](https://www.fastly.com/blog/can-we-be-normal-about-ai-now-that-deepseek-happened). But, there's probably going to be a crash before that happens.