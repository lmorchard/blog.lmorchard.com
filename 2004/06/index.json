[
  {
    "comments_archived": true,
    "date": "2004-06-28T23:17:36.000Z",
    "excerpt": "Lately, my iTunes has been playing radioio Rock almost exclusively lately, but one thing that peeves me is that I don't seem to see the current song while the stream's playing.  Instead, the radioio site offers a pop-up window that displays the last few songs in the playlist.  However, I'm usually somewhere off in another window or a shell and don't really feel like popping over to a browser and navigating to the playlist just to see what this song is.  So, I wrote myself a little mini-scraper script...",
    "layout": "post",
    "tags": [
      "hacks"
    ],
    "title": "A mini-scraper for the playlist at radioio Rock",
    "wordpress_id": 531,
    "wordpress_slug": "radioiorock-scraper",
    "wordpress_url": "http://www.decafbad.com/blog/?p=531",
    "year": "2004",
    "month": "06",
    "day": "28",
    "isDir": false,
    "slug": "radioiorock-scraper",
    "postName": "2004-06-28-radioiorock-scraper",
    "html": "<p>Lately, my iTunes has been playing <a href=\"http://www.radioio.com/radioiorock.php?stream=radioioRock\">radioio Rock</a> almost exclusively lately, but one thing that peeves me is that I don&#39;t seem to see the current song while the stream&#39;s playing.  Instead, the <a href=\"http://www.radioio.com/\">radioio</a> site offers a pop-up window that displays the last few songs in the playlist.  However, I&#39;m usually somewhere off in another window or a shell and don&#39;t really feel like popping over to a browser and navigating to the playlist just to see what this song is.</p>\n<p>So, I wrote myself a little mini-scraper script:</p>\n<pre><code>#!/bin/sh\n\ncurl -s &#39;http://player.radioio.com/player.php?b=614&amp;#38;stream=radioioRock&#39; | \\\n    tidy -asxml --wrap 300 -q -f /dev/null | \\\n    xml sel -t -m &quot;//*[@class=&#39;leadrock&#39;]&quot; -v &#39;.&#39; -n \\\n        -o &#39;    [http://www.radioio.com&#39; -v &#39;../@href&#39; -o &#39;]&#39; -n </code></pre>\n<p>The output looks something like this:</p>\n<pre><code>[06/29 11:01:08] deusx@Caffeina2:~ % radioio\n\nVast - I Need To Say Goodbye\n    [http://www.radioio.com...]\nCure - The End Of The World\n    [http://www.radioio.com...]\nSeachange - Avs Co 10\n    [http://www.radioio.com...]\nPixies - Bam Thwok\n    [http://www.radioio.com...]\nDeath Cab For Cutie - The New Year\n    [http://www.radioio.com...]\nLovethugs - Drawing The Curtains\n    [http://www.radioio.com...]</code></pre>\n<p>Oh yeah, and to run this script, you will need these tools:</p>\n<ul>\n<li><a href=\"http://curl.haxx.se/\">curl</a></li>\n<li><a href=\"http://tidy.sourceforge.net/\">HTML Tidy</a> </li>\n<li><a href=\"http://xmlstar.sourceforge.net/\">XMLStarlet</a></li>\n</ul>\n<p>Personally, I like the included URLs (which I edited here for length) since they launch a search for CDs by the artist.  However, you can cut the output down to just the artist/title by removing the final line of the script and the backslash at the end of the line before.</p>\n<p>If you like a different <a href=\"http://www.radioio.com/\">radioio</a> station, say <a href=\"http://www.radioio.com/radioioeclectic.php?stream=radioioEclectic\">radioio Eclectic</a>, you can change <code>stream=radioioRock</code> to <code>stream=radioioEclectic</code> in the URL above and change <code>class=&#39;leadrock&#39;</code> to <code>class=&#39;leadeclectic&#39;</code>.  I could have parameterized these, but I&#39;m lazy, and that was the whole point!</p>\n<p>ShareAndEnjoy!</p>\n<!--more-->\n<p>shortname=radioiorock_scraper</p>\n<div id=\"comments\" class=\"comments archived-comments\"><h3>Archived Comments</h3>\n<ul class=\"comments\">\n<li class=\"comment\" id=\"comment-221087229\">\n<div class=\"meta\">\n<div class=\"author\">\n<a class=\"avatar image\" rel=\"nofollow\" \nhref=\"http://www.bytecloud.com\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=7388f3fd9cfa9436c5282542aaccf4fd&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\n<a class=\"avatar name\" rel=\"nofollow\" \nhref=\"http://www.bytecloud.com\">Mike Carter</a>\n</div>\n<a href=\"#comment-221087229\" class=\"permalink\"><time datetime=\"2004-07-01T07:39:45\">2004-07-01T07:39:45</time></a>\n</div>\n<div class=\"content\">I was listening to some streams today for the first time in a while and I was thinking how annoying it was not to know the track names.  Thanks for posting this, it may come in handy!</div>\n</li>\n<li class=\"comment\" id=\"comment-221087231\">\n<div class=\"meta\">\n<div class=\"author\">\n<a class=\"avatar image\" rel=\"nofollow\" \nhref=\"http://cpe000103c34069-cm014300001653.cpe.net.cable.rogers.com/weblogs/ben/\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=871196de9b27ed994c30428eed59073c&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\n<a class=\"avatar name\" rel=\"nofollow\" \nhref=\"http://cpe000103c34069-cm014300001653.cpe.net.cable.rogers.com/weblogs/ben/\">Gnomon</a>\n</div>\n<a href=\"#comment-221087231\" class=\"permalink\"><time datetime=\"2004-07-01T18:31:58\">2004-07-01T18:31:58</time></a>\n</div>\n<div class=\"content\">How is this the first time I've ever found out about XML Starlet? It's magnificent!\nThe application is cool, too, but /man/ - I'm going to have fun with XML Starlet. Thanks so very much for the pointer, Les!</div>\n</li>\n<li class=\"comment\" id=\"comment-221087232\">\n<div class=\"meta\">\n<div class=\"author\">\n<a class=\"avatar image\" rel=\"nofollow\" \nhref=\"http://www.radioio.com/\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=0cfcb3aefaca714528c009a68810e8ec&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\n<a class=\"avatar name\" rel=\"nofollow\" \nhref=\"http://www.radioio.com/\">Jesse</a>\n</div>\n<a href=\"#comment-221087232\" class=\"permalink\"><time datetime=\"2005-01-23T11:50:41\">2005-01-23T11:50:41</time></a>\n</div>\n<div class=\"content\">I was searching around with blowsearch.com today (which btw blows heheh) and found this entry.  This earl might be of interest to ya:\nhttp://69.28.133.51:1040/7.html\nThe db for searchplay and our players is only updated every 30 seconds, 7.html is near reatime right from our playback software.  :)\nWe used to have title streaming on our mp3 streams but are in transition of server and encoding software atm while we prepare to roll out our own software, and will be adding new formats such as heaac.  :)\nToo much to talk about, we have a LOT of stuff planned for 2005-2006 that we've been working on (and patent pending) for years now.  Definately going to be exciting time for net radio.  Thanks for your interest Leslie.\njesse\nchief tinkerer of toys,\nradioio</div>\n</li>\n</ul>\n</div>\n",
    "body": "Lately, my iTunes has been playing [radioio Rock][rock] almost exclusively lately, but one thing that peeves me is that I don't seem to see the current song while the stream's playing.  Instead, the [radioio][radioio] site offers a pop-up window that displays the last few songs in the playlist.  However, I'm usually somewhere off in another window or a shell and don't really feel like popping over to a browser and navigating to the playlist just to see what this song is.\r\n\r\nSo, I wrote myself a little mini-scraper script:\r\n\r\n    #!/bin/sh\r\n    \r\n    curl -s 'http://player.radioio.com/player.php?b=614&#38;stream=radioioRock' | \\\r\n        tidy -asxml --wrap 300 -q -f /dev/null | \\\r\n        xml sel -t -m \"//*[@class='leadrock']\" -v '.' -n \\\r\n            -o '    [http://www.radioio.com' -v '../@href' -o ']' -n \r\n\r\nThe output looks something like this:\r\n\r\n    [06/29 11:01:08] deusx@Caffeina2:~ % radioio\r\n    \r\n    Vast - I Need To Say Goodbye\r\n        [http://www.radioio.com...]\r\n    Cure - The End Of The World\r\n        [http://www.radioio.com...]\r\n    Seachange - Avs Co 10\r\n        [http://www.radioio.com...]\r\n    Pixies - Bam Thwok\r\n        [http://www.radioio.com...]\r\n    Death Cab For Cutie - The New Year\r\n        [http://www.radioio.com...]\r\n    Lovethugs - Drawing The Curtains\r\n        [http://www.radioio.com...]\r\n\r\nOh yeah, and to run this script, you will need these tools:\r\n\r\n* [curl][curl]\r\n* [HTML Tidy][tidy] \r\n* [XMLStarlet][xmlstarlet]\r\n\r\nPersonally, I like the included URLs (which I edited here for length) since they launch a search for CDs by the artist.  However, you can cut the output down to just the artist/title by removing the final line of the script and the backslash at the end of the line before.\r\n\r\nIf you like a different [radioio][radioio] station, say [radioio Eclectic][eclectic], you can change `stream=radioioRock` to `stream=radioioEclectic` in the URL above and change `class='leadrock'` to `class='leadeclectic'`.  I could have parameterized these, but I'm lazy, and that was the whole point!\r\n\r\nShareAndEnjoy!\r\n\r\n[curl]: http://curl.haxx.se/\r\n[tidy]: http://tidy.sourceforge.net/\r\n[xmlstarlet]: http://xmlstar.sourceforge.net/\r\n[rock]: http://www.radioio.com/radioiorock.php?stream=radioioRock\r\n[radioio]: http://www.radioio.com/\r\n[eclectic]: http://www.radioio.com/radioioeclectic.php?stream=radioioEclectic\r\n<!--more-->\r\nshortname=radioiorock_scraper\r\n\r\n<div id=\"comments\" class=\"comments archived-comments\">\r\n            <h3>Archived Comments</h3>\r\n            \r\n        <ul class=\"comments\">\r\n            \r\n        <li class=\"comment\" id=\"comment-221087229\">\r\n            <div class=\"meta\">\r\n                <div class=\"author\">\r\n                    <a class=\"avatar image\" rel=\"nofollow\" \r\n                       href=\"http://www.bytecloud.com\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=7388f3fd9cfa9436c5282542aaccf4fd&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\r\n                    <a class=\"avatar name\" rel=\"nofollow\" \r\n                       href=\"http://www.bytecloud.com\">Mike Carter</a>\r\n                </div>\r\n                <a href=\"#comment-221087229\" class=\"permalink\"><time datetime=\"2004-07-01T07:39:45\">2004-07-01T07:39:45</time></a>\r\n            </div>\r\n            <div class=\"content\">I was listening to some streams today for the first time in a while and I was thinking how annoying it was not to know the track names.  Thanks for posting this, it may come in handy!</div>\r\n            \r\n        </li>\r\n    \r\n        <li class=\"comment\" id=\"comment-221087231\">\r\n            <div class=\"meta\">\r\n                <div class=\"author\">\r\n                    <a class=\"avatar image\" rel=\"nofollow\" \r\n                       href=\"http://cpe000103c34069-cm014300001653.cpe.net.cable.rogers.com/weblogs/ben/\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=871196de9b27ed994c30428eed59073c&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\r\n                    <a class=\"avatar name\" rel=\"nofollow\" \r\n                       href=\"http://cpe000103c34069-cm014300001653.cpe.net.cable.rogers.com/weblogs/ben/\">Gnomon</a>\r\n                </div>\r\n                <a href=\"#comment-221087231\" class=\"permalink\"><time datetime=\"2004-07-01T18:31:58\">2004-07-01T18:31:58</time></a>\r\n            </div>\r\n            <div class=\"content\">How is this the first time I've ever found out about XML Starlet? It's magnificent!\r\n\r\nThe application is cool, too, but /man/ - I'm going to have fun with XML Starlet. Thanks so very much for the pointer, Les!</div>\r\n            \r\n        </li>\r\n    \r\n        <li class=\"comment\" id=\"comment-221087232\">\r\n            <div class=\"meta\">\r\n                <div class=\"author\">\r\n                    <a class=\"avatar image\" rel=\"nofollow\" \r\n                       href=\"http://www.radioio.com/\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=0cfcb3aefaca714528c009a68810e8ec&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\r\n                    <a class=\"avatar name\" rel=\"nofollow\" \r\n                       href=\"http://www.radioio.com/\">Jesse</a>\r\n                </div>\r\n                <a href=\"#comment-221087232\" class=\"permalink\"><time datetime=\"2005-01-23T11:50:41\">2005-01-23T11:50:41</time></a>\r\n            </div>\r\n            <div class=\"content\">I was searching around with blowsearch.com today (which btw blows heheh) and found this entry.  This earl might be of interest to ya:\r\nhttp://69.28.133.51:1040/7.html\r\nThe db for searchplay and our players is only updated every 30 seconds, 7.html is near reatime right from our playback software.  :)\r\n\r\nWe used to have title streaming on our mp3 streams but are in transition of server and encoding software atm while we prepare to roll out our own software, and will be adding new formats such as heaac.  :)\r\n\r\nToo much to talk about, we have a LOT of stuff planned for 2005-2006 that we've been working on (and patent pending) for years now.  Definately going to be exciting time for net radio.  Thanks for your interest Leslie.\r\n\r\njesse\r\nchief tinkerer of toys,\r\nradioio</div>\r\n            \r\n        </li>\r\n    \r\n        </ul>\r\n    \r\n        </div>\r\n    ",
    "parentPath": "../blog.lmorchard.com/posts/archives/2004",
    "path": "2004/06/28/radioiorock-scraper",
    "summary": "<p>Lately, my iTunes has been playing <a href=\"http://www.radioio.com/radioiorock.php?stream=radioioRock\">radioio Rock</a> almost exclusively lately, but one thing that peeves me is that I don&apos;t seem to see the current song while the stream&apos;s playing.  Instead, the <a href=\"http://www.radioio.com/\">radioio</a> site offers a pop-up window that displays the last few songs in the playlist.  However, I&apos;m usually somewhere off in another window or a shell and don&apos;t really feel like popping over to a browser and navigating to the playlist just to see what this song is.</p>\n<p>So, I wrote myself a little mini-scraper script:</p>\n<pre><code>#!/bin/sh\n\ncurl -s &apos;http://player.radioio.com/player.php?b=614&amp;#38;stream=radioioRock&apos; | \\\n    tidy -asxml --wrap 300 -q -f /dev/null | \\\n    xml sel -t -m &quot;//*[@class=&apos;leadrock&apos;]&quot; -v &apos;.&apos; -n \\\n        -o &apos;    [http://www.radioio.com&apos; -v &apos;../@href&apos; -o &apos;]&apos; -n </code></pre>\n<p>The output looks something like this:</p>\n<pre><code>[06/29 11:01:08] deusx@Caffeina2:~ % radioio\n\nVast - I Need To Say Goodbye\n    [http://www.radioio.com...]\nCure - The End Of The World\n    [http://www.radioio.com...]\nSeachange - Avs Co 10\n    [http://www.radioio.com...]\nPixies - Bam Thwok\n    [http://www.radioio.com...]\nDeath Cab For Cutie - The New Year\n    [http://www.radioio.com...]\nLovethugs - Drawing The Curtains\n    [http://www.radioio.com...]</code></pre>\n<p>Oh yeah, and to run this script, you will need these tools:</p>\n<ul>\n<li><a href=\"http://curl.haxx.se/\">curl</a></li>\n<li><a href=\"http://tidy.sourceforge.net/\">HTML Tidy</a> </li>\n<li><a href=\"http://xmlstar.sourceforge.net/\">XMLStarlet</a></li>\n</ul>\n<p>Personally, I like the included URLs (which I edited here for length) since they launch a search for CDs by the artist.  However, you can cut the output down to just the artist/title by removing the final line of the script and the backslash at the end of the line before.</p>\n<p>If you like a different <a href=\"http://www.radioio.com/\">radioio</a> station, say <a href=\"http://www.radioio.com/radioioeclectic.php?stream=radioioEclectic\">radioio Eclectic</a>, you can change <code>stream=radioioRock</code> to <code>stream=radioioEclectic</code> in the URL above and change <code>class=&apos;leadrock&apos;</code> to <code>class=&apos;leadeclectic&apos;</code>.  I could have parameterized these, but I&apos;m lazy, and that was the whole point!</p>\n<p>ShareAndEnjoy!</p>\n",
    "prevPostPath": "2004/07/06/wishofthemonthclub3",
    "nextPostPath": "2004/06/28/wishofthemonthclub2"
  },
  {
    "comments_archived": true,
    "date": "2004-06-28T01:44:51.000Z",
    "excerpt": "Here's the next installment of the Wish-of-the-Month Club.  You can revisit the first part, too, if you've missed it.  I'd meant to post it within a week of the first part, so apologies all around to anyone who has been tapping a foot waiting for it.  Enjoy!",
    "layout": "post",
    "tags": [
      "hacks",
      "xml"
    ],
    "title": "Wish-of-the-Month Club, Part 2 of 3",
    "wordpress_id": 530,
    "wordpress_slug": "wishofthemonthclub2",
    "wordpress_url": "http://www.decafbad.com/blog/?p=530",
    "year": "2004",
    "month": "06",
    "day": "27",
    "isDir": false,
    "slug": "wishofthemonthclub2",
    "postName": "2004-06-27-wishofthemonthclub2",
    "html": "<p><i>Here&#39;s the next installment of the Wish-of-the-Month Club.  You can <a href=\"http://www.decafbad.com/blog/2004/06/16/wishofthemonthclub1\">revisit the first part</a>, too, if you&#39;ve missed it.  I&#39;d meant to post it within a week of the first part, so apologies all around to anyone who has been tapping a foot waiting for it.  Enjoy!</i></p>\n<h3 id=\"paging-through-wishes\">Paging Through Wishes</h3>\n<p>Some ready-made files are available for this section:</p>\n<ul>\n<li><a href=\"http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes-ex2.xsl\"><code>wishes-ex2.xsl</code></a>: The second iteration of the stylesheet in development.</li>\n</ul>\n<p>Now we&#39;ve got a way to make queries against Amazon Web Services, not entirely unlike what you might be used to if you tinker with MySQL databases on a regular basis.  At this point, though, we still have a bit of refining to make to this query.  If you take a look at the data produced by the query in its current state, and compare that to what you see on wishlists in your browser, you should notice some things missing.</p>\n<p>If you look at <a href=\"http://www.amazon.com/exec/obidos/registry/1QWYI6P2JF3Q5\" title=\"Buy me something, will ya?\">my wishlist</a>, you&#39;ll notice that items span several pages when visited by browser.  As it turns out, AWS queries work in a similar fashion--each query returns only a limited number of items (about 10), and an additional parameter supplied to further queries is required to step through further pages of results.  So, using what we&#39;ve built so far will only get us to the first page of wishlist items; to get all of the items, we&#39;ll need a way to step through all of the pages.</p>\n<p>In playing with this, I experienced a bit of hairpulling frustration:  The AWS documentation, under &quot;Generating Additional Product Results&quot;, claims that XML returned by the service will supply a count of the total pages available for a given query.  And although I see this element present in other types of searches, the <code>TotalPages</code> element is absent when querying on wishlists.  This may be a bug, or it may be an undocumented change in the service--either way, it was a surprise and leaves me with no official way to know how many pages I need to ask for in order to have a complete set of data.  </p>\n<p>With some further tinkering, though, I figured out a workaround: If a query is made for a page number beyond the final page, the XML returned will be a duplicate of the final page.  Once I see a duplicate item appear, I know it&#39;s time to stop paging through results.  This is completely undocumented behavior, and could break at any time (ie. if Amazon decided to start issuing an error for a page index out of bounds), but it&#39;ll work for now.</p>\n<p>This calls for reworking the <code>processWishlist</code> template.  For a given wishlist, it will need to iterate through a sequence of page numbers, requesting XML from AWS for each, stopping when the first duplicate page is found.  Since XSLT is heavily steeped in functional programming concepts, this sort of <a href=\"http://www.dpawson.co.uk/xsl/sect2/N4806.html\" title=\"Iteration in XSLT\">iteration in XSLT</a> is best done <a href=\"http://www-106.ibm.com/developerworks/xml/library/x-xslrecur/\" title=\"Use recursion effectively in XSL\">with recursion</a>:</p>\n<pre><code>  &lt;xsl:template name=&quot;processWishlist&quot;&gt;\n\n    &lt;xsl:param name=&quot;wishlist&quot; /&gt;              &lt;!-- Wishlist ID --&gt;\n    &lt;xsl:param name=&quot;max&quot;   select=&quot;50&quot; /&gt;     &lt;!-- Arbitrary upper loop limit --&gt;\n    &lt;xsl:param name=&quot;curr_page&quot; select=&quot;1&quot; /&gt;  &lt;!-- Curr page # --&gt;\n    &lt;xsl:param name=&quot;prev_first_asin&quot; /&gt;       &lt;!-- Keeping track of repeats --&gt;</code></pre>\n<p>The first modification to this template is the addition of three parameters:</p>\n<ul>\n<li><code>max</code> provides an arbitrary upper limit to the number of pages through which this template will iterate.</li>\n<li><code>curr_page</code> contains the number of the page to be requested in this iteration.</li>\n<li><code>prev_first_asin</code> will contain the ASIN number of the first item from the previous iteration&#39;s page of results.</li>\n</ul>\n<p>Next, we modify the URL used to query for wishlist data:</p>\n<pre><code>    &lt;!-- Fetch the wishlist products --&gt;\n    &lt;xsl:variable name=&quot;details&quot; select=&quot;document(concat(\n                  &#39;http://xml.amazon.com/onca/xml3?&#39;,\n                  &#39;t=&#39;,$associate,&#39;&amp;amp;&#39;,\n                  &#39;dev-t=&#39;,$devtoken,&#39;&amp;amp;&#39;,\n                  &#39;WishlistSearch=&#39;,$wishlist,&#39;&amp;amp;&#39;,\n                  &#39;type=lite&amp;amp;f=xml&amp;amp;&#39;,\n                  &#39;page=&#39;,$curr_page))//Details&quot; /&gt;</code></pre>\n<p>The only addition here beyond the previous version is the <code>page</code> parameter in the URL.  Not much mystery here--this parameter specifies which page of results we want.  Now, let&#39;s build the loop:</p>\n<pre><code>    &lt;!-- Snag the first item Asin --&gt;\n    &lt;xsl:variable name=&quot;curr_first_asin&quot; select=&quot;$details/Asin/text()&quot; /&gt;\n\n    &lt;!-- If we haven&#39;t exceeded the loop limit, and this first Asin isn&#39;t --&gt;\n    &lt;!-- a repeat of the previous loop (indicating we&#39;ve run out of new   --&gt;\n    &lt;!-- pages), then go ahead...                                         --&gt;\n    &lt;xsl:if test=&quot;(($curr_page+1) &amp;lt; $max) and\n                  (string-length($curr_first_asin) &amp;gt; 0) and\n                  not($curr_first_asin = $prev_first_asin)&quot;&gt;</code></pre>\n<p>We capture the ASIN of the first item in this page of results and check to see if we should continue.  This <code>if</code> conditional first ensures that we&#39;re not past the sanity guard for loop iterations, makes sure that we actually got a non-empty current first ASIN, then checks our current first product&#39;s ASIN against what was passed in as the previous iteration&#39;s first product&#39;s ASIN.  If this was the first time through the loop, this value should be empty and therefore wouldn&#39;t match the current ASIN.  But, if we&#39;ve gone past the end of results, the previous and current ASIN values should match, and the conditional will fail.</p>\n<p>Moving along into the body of the conditional, we copy in wishlist products filtered on a price maximum, just as before:</p>\n<pre><code>      &lt;!-- Copy products, filtering on a maximum price --&gt;\n      &lt;xsl:copy-of select=&quot;$details/OurPrice[number(substring(\n                   text(),2)) &amp;lt; $maxprice]/..&quot; /&gt;</code></pre>\n<p>Having done that, we move onto the recursive end of this template:</p>\n<pre><code>      &lt;!-- Loop by recursion to get the next page --&gt;\n      &lt;xsl:call-template name=&quot;processWishlist&quot;&gt;\n        &lt;xsl:with-param name=&quot;wishlist&quot;        select=&quot;$wishlist&quot; /&gt;\n        &lt;xsl:with-param name=&quot;max&quot;             select=&quot;$max&quot; /&gt;\n        &lt;xsl:with-param name=&quot;curr_page&quot;       select=&quot;$curr_page + 1&quot; /&gt;\n        &lt;xsl:with-param name=&quot;prev_first_asin&quot; select=&quot;$curr_first_asin&quot; /&gt;\n      &lt;/xsl:call-template&gt;\n\n    &lt;/xsl:if&gt;    \n  &lt;/xsl:template&gt;</code></pre>\n<p>Here, the template makes a recursive call back to itself, passing through the wishlist ID and the maximum iteration count.  Since variables in XSLT are immutable, meaning that their values can&#39;t be changed once they&#39;ve been set, we can&#39;t increment <code>$curr_page</code> in-place like a loop counter in other languages--so, the current page count <em>value</em> is incremented and passed to the recursive call as a parameter.  Finally, the current first item&#39;s ASIN is passed along, to become the previous ASIN for the next iteration.</p>\n<p>Note that when the conditional fails--that is, if the loop limit is passed or a duplicate page is detected--the loop ends.  In other words, nothing further happens and execution pops back up out of all the levels of recursion and the top-level template ends.  </p>\n<p>I wrote &quot;<em>when</em> the conditional fails&quot;.  This is a key point: for the loop to eventually end, this conditional <em>must</em> fail (or be made to fail) at some point, else this loop will happily progress through page requests forever.  This is the reason for the <code>$max</code> parameter limiting the number of iterations, in case something goes haywire--like, oh say, a failure of our duplicate-page detection hack as a loop ending condition.  A useful exercise for the reader might be to add some additional diagnostic code to report that the limit was hit versus a natural end to results.</p>\n<h3 id=\"random-numbers\">Random Numbers</h3>\n<p>Some ready-made files are available for this section:</p>\n<ul>\n<li><a href=\"http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes-ex3.xsl\"><code>wishes-ex3.xsl</code></a>: The third iteration of the stylesheet in development.</li>\n<li><a href=\"http://www.decafbad.com/cvs/*checkout*/hacks/wishes/random-xml\"><code>random-xml</code></a>: A Perl CGI script used as a web service to generate random numbers.</li>\n</ul>\n<p>Armed with a template that will query against the full set of items in a wishlist, we&#39;re ready to look into making a random selection from a list of products.  </p>\n<p>But first, we need to pick a random number.  Unfortunately, there doesn&#39;t appear to be any <code>random()</code> function in the XPath or XSLT standards.  There <em>is</em> a <a href=\"http://www.exslt.org/math/functions/random/index.html\"><code>math:random()</code></a> from EXSLT implemented in <code>libxslt</code>, but I seem to be having a bit of a problem getting it to produce anything other than the same sequence of numbers.  I suspect there&#39;s a problem in seeding the random number generator, but I&#39;ve yet to work out how to fix it.  (Suggestions welcome.)</p>\n<p>So, I cheated and made another workaround with a CGI script on my web server that generates random numbers in a simple XML document.  Currently, it&#39;s hosted here:</p>\n<pre><code>http://www.decafbad.com/2004/05/random-xml</code></pre>\n<p>And this is what the script looks like:</p>\n<pre><code>#!/usr/bin/perl\n\nuse strict;\nuse CGI;\n\nmy $q = new CGI();\n\nmy $min = $q-&gt;param(&#39;min&#39;) or 0;\nmy $max = $q-&gt;param(&#39;max&#39;) or 1;\nmy $int = $q-&gt;param(&#39;int&#39;);\n\nmy $num = $min + ( rand() * ($max - $min));\nif ($int) { $num = int($num); }\n\nprint $q-&gt;header(&#39;text/xml&#39;);\nprint &quot;&lt;rand&gt;$num&lt;/rand&gt;\\n&quot;;</code></pre>\n<p>This is a very simple CGI.  It accepts the parameters <code>max</code>, <code>min</code>, and <code>int</code>.  The values of these parameters determine the maximum and minimum value for the random number returned, and whether or not it should be an integer.  For example, the <a href=\"http://www.decafbad.com/2004/05/random-xml?int=1&#38;min=10&#38;max=20\" title=\"A random integer between 10 and 20, in XML\">following URL</a> should return an integer between 10 and 20:</p>\n<pre><code>http://www.decafbad.com/2004/05/random-xml?\nint=1&amp;#38;min=10&amp;#38;max=20</code></pre>\n<p>Using this as a web service in the stylesheet with the <code>document()</code> function, we can get a random number.  If you&#39;ve got web space where you can host CGI scripts, I suggest you host a copy of this script yourself, since I can&#39;t guarantee how long mine will stick around.  But, for as long at works, feel free to use the service from my server.</p>\n<p>Moving along, let&#39;s add a new named template to the stylesheet, called <code>randomWishlistProduct</code>:</p>\n<pre><code>  &lt;xsl:template name=&quot;randomWishlistProduct&quot;&gt;\n\n    &lt;xsl:param name=&quot;wishlist&quot; /&gt; &lt;!-- Wishlist ID --&gt;\n\n    &lt;!-- Gather all the products for the current wishlist --&gt;\n    &lt;xsl:variable name=&quot;products&quot;&gt;\n      &lt;xsl:call-template name=&quot;processWishlist&quot;&gt;\n        &lt;xsl:with-param name=&quot;wishlist&quot; select=&quot;$wishlist&quot; /&gt;\n      &lt;/xsl:call-template&gt;\n    &lt;/xsl:variable&gt;</code></pre>\n<p>Just like the <code>processWishlist</code> template, we start by defining the parameter <code>wishlist</code> to accept a wishlist ID.  Using this ID, we call the <code>processWishlist</code> template itself and store the complete list of products queried from the wishlist into the variable <code>$products</code>.</p>\n<pre><code>    &lt;!-- Count the products in the wishlist --&gt;\n    &lt;xsl:variable name=&quot;max_products&quot;\n                  select=&quot;count(exsl:node-set($products)/Details)&quot; /&gt;</code></pre>\n<p>This next step counts the number of products found in the wishlist.  The one tricky bit here is the use of the EXSLT function <a href=\"http://www.exslt.org/exsl/functions/node-set/index.html\"><code>exsl:node-set()</code></a>: The <code>$products</code> variable contains what&#39;s called a <a href=\"http://www.w3.org/TR/xslt#section-Result-Tree-Fragments\"><em>result tree fragment</em></a>, which is a kind of cross between XML data nodes and a plain old string.  This type of data does not normally allow the full set of XPath operators to be used on it, so first we need to use <code>exsl:node-set()</code> to turn it into a full-fledged node set.  Then we can look up the <code>Details</code> element nodes and count them.  </p>\n<pre><code>    &lt;!-- Conjure up a random index within the list of products --&gt;\n    &lt;xsl:variable name=&quot;rand_product_num&quot;\n                  select=&quot;document(concat(\n                  &#39;http://www.decafbad.com/2004/05/random-xml?&#39;,\n                  &#39;int=1&amp;amp;&#39;,\n                  &#39;min=1&amp;amp;&#39;,\n                  &#39;max=&#39;,$max_products))/rand&quot; /&gt;</code></pre>\n<p>Here is where the random number service comes in handy.  The <code>concat()</code> function is used to build the URL to the service, with parameters specifying that the number should be an integer, and should fall between 1 and the number of products in the wishlist.  The <code>document()</code> function grabs the XML document from the service, and the value is extracted from the single element the document contains.</p>\n<p>There is an alternative to this last bit, should you happen to have a properly working <code>math:random()</code> function in your XSLT processor:</p>\n<pre><code>    &lt;xsl:variable name=&quot;rand_product_num&quot; select=&quot;round( math:random() *\n                  $max_products ) + 1&quot; /&gt;</code></pre>\n<p>If you can use this instead, you&#39;ll have no need for the random number web service.  This version is obviously more concise, and doesn&#39;t require another trip out to a web service.  You might want to try it--but if you find that you keep getting the same wishlist items selected, then you&#39;ve run into the problem I found with the random number generator.</p>\n<p>Now, let&#39;s wrap this template up by selecting an item:</p>\n<pre><code>    &lt;!-- Copy the product as indexed by the random number --&gt;\n    &lt;xsl:copy-of select=&quot;exsl:node-set($products)/Details[\n                 position()=$rand_product_num]&quot; /&gt;\n\n  &lt;/xsl:template&gt;</code></pre>\n<p>Again, we need to use the <code>exsl:node-set()</code> function to turn the result tree fragment in the <code>$products</code> variable into a node set, from which we select and copy the <code>Details</code> element whose position in the data is indexed by the random number we just selected.  Just one last tweak needed to wrap up this iteration of our stylesheet.  We need to swap out the call to the <code>processWishlist</code> function at the end and replace it with a call to <code>randomWishlistProduct</code>:</p>\n<pre><code>  &lt;xsl:template match=&quot;/wishes:wishes&quot;&gt;\n\n    &lt;xsl:for-each select=&quot;//wishes:wishlist&quot;&gt;\n      &lt;wishes:wishitem&gt;\n        &lt;xsl:copy-of select=&quot;.&quot; /&gt;\n        &lt;xsl:call-template name=&quot;randomWishlistProduct&quot;&gt;\n          &lt;xsl:with-param name=&quot;wishlist&quot; select=&quot;.&quot; /&gt;\n        &lt;/xsl:call-template&gt;\n      &lt;/wishes:wishitem&gt;\n    &lt;/xsl:for-each&gt;\n\n  &lt;/xsl:template&gt;</code></pre>\n<p>After these changes, you should be able to run the stylesheet ([wishes-ex3.xsl][wishes_ex3]) and get something like the following:</p>\n<pre><code>&lt;wishes:wishitem xmlns:wishes=&quot;http://www.decafbad.com/2004/05/wishes&quot;&gt;\n    &lt;wishes:wishlist label=&quot;The Girl&quot;&gt;35OIOYWQ9XQAE&lt;/wishes:wishlist&gt;\n    &lt;Details ...&gt;...&lt;/Details&gt;\n&lt;/wishes:wishitem&gt;\n&lt;wishes:wishitem xmlns:wishes=&quot;http://www.decafbad.com/2004/05/wishes&quot;&gt;\n    &lt;wishes:wishlist label=&quot;Me&quot;&gt;1QWYI6P2JF3Q5&lt;/wishes:wishlist&gt;\n    &lt;Details ...&gt;...&lt;/Details&gt;\n&lt;/wishes:wishitem&gt;</code></pre>\n<p>This is similar to the output of the previous iteration of the stylesheet, but this time there&#39;s only one product selected at random for each wishlist.  </p>\n<h3 id=\"shopping-carts\">Shopping Carts</h3>\n<p>Some ready-made files are available for this section:</p>\n<ul>\n<li><a href=\"http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes-ex4.xsl\"><code>wishes-ex4.xsl</code></a>: The fourth iteration of the stylesheet in development.</li>\n</ul>\n<p>By this point, we&#39;ve been able to query and filter products in Amazon wishlists, and we&#39;ve selected an item at random from each wishlist we&#39;ve queried.  Now, let&#39;s enable some purchases.</p>\n<p>The AWS provides for Remote Shopping Cart functionality, whereby items can be added to an Amazon.com shopping cart programmatically.  This is about as close as we can get to automating the purchase of items selected from the wishlists--there is no API functionality for actually completing the ordering of items.  If you really think about it, this really is a good thing and <em>should</em> demand human intervention; we certainly wouldn&#39;t want this script going crazy and accidentally buying up everything on a wishlist.</p>\n<p>Documentation for the AWS Remote Shopping Cart explains that a shopping cart can be created and items added with a URL like the following:</p>\n<pre><code>http://xml.amazon.com/onca/xml3?\nShoppingCart=add&amp;#38;\nf=xml&amp;#38;\ndev-t=[Developer Token goes here]&amp;#38;\nt=[Associates ID goes here]&amp;#38;\nAsin.[ASIN goes here]=[quantity goes here]&amp;#38;\nsims=true</code></pre>\n<p>Part of this should look familiar, so we already know what to do with the developer token and the associates ID.  The last part, specifying product ASIN and quantity, can be filled out with information contained in the product records selected at random from the wishlists.  </p>\n<p>So, let&#39;s start by revising the template at the end of the stylesheet:</p>\n<pre><code>&lt;xsl:template match=&quot;/wishes:wishes&quot;&gt;\n\n    &lt;xsl:variable name=&quot;random_products&quot;&gt;      \n      &lt;xsl:for-each select=&quot;//wishes:wishlist&quot;&gt;\n        &lt;wishes:wishitem&gt;\n          &lt;xsl:copy-of select=&quot;.&quot; /&gt;\n          &lt;xsl:call-template name=&quot;randomWishlistProduct&quot;&gt;\n            &lt;xsl:with-param name=&quot;wishlist&quot; select=&quot;.&quot; /&gt;\n          &lt;/xsl:call-template&gt;\n        &lt;/wishes:wishitem&gt;\n      &lt;/xsl:for-each&gt;\n    &lt;/xsl:variable&gt;</code></pre>\n<p>Here, we&#39;ve taken what was the output of the previous iteration of the stylesheet and stuffed it into the variable <code>$random_products</code>.  Next, let&#39;s fill in the blanks and build a Remote Shopping Cart URL:</p>\n<pre><code>    &lt;xsl:variable name=&quot;shopping_cart_create_url&quot;&gt;\n      &lt;!-- Standard AWS URL --&gt;\n      &lt;xsl:text&gt;http://xml.amazon.com/onca/xml3?&lt;/xsl:text&gt;\n\n      &lt;!-- Add in the selected items --&gt;\n      &lt;xsl:for-each select=&quot;exsl:node-set($random_products)\n                            /wishes:wishitem/Details&quot;&gt;\n        &lt;xsl:text&gt;Asin.&lt;/xsl:text&gt;&lt;xsl:value-of select=&quot;Asin&quot; /&gt;\n        &lt;xsl:text&gt;=1&amp;amp;&lt;/xsl:text&gt;\n      &lt;/xsl:for-each&gt;\n\n      &lt;!-- Wrap up with the shopping cart function and required tokens --&gt;\n      &lt;xsl:text&gt;ShoppingCart=add&amp;amp;&lt;/xsl:text&gt;\n      &lt;xsl:text&gt;f=xml&amp;amp;&lt;/xsl:text&gt;\n      &lt;xsl:text&gt;dev-t=&lt;/xsl:text&gt;&lt;xsl:value-of select=&quot;$devtoken&quot; /&gt;\n      &lt;xsl:text&gt;&amp;amp;&lt;/xsl:text&gt;\n      &lt;xsl:text&gt;t=&lt;/xsl:text&gt;&lt;xsl:value-of select=&quot;$associate&quot; /&gt;\n    &lt;/xsl:variable&gt;</code></pre>\n<p>Since simple XPath doesn&#39;t allow for the looping needed for multiple items, we can&#39;t just concatenate this URL together in a <code>select</code> expression like we did with the wishlist item query.  So, we use <code>xslt:foreach</code> to build this with blocks of text using the <code>xsl:text</code> element.  We iterate though the random products chosen from wishlists and add an ASIN for each to the URL with a quantity of 1. Then, we use the <code>$devtoken</code> and <code>$associate</code> variables to fill in their respective spots.</p>\n<p>Note that this could have been written without using the <code>xsl:text</code> elements like so:</p>\n<pre><code>    &lt;xsl:variable name=&quot;shopping_cart_create_url&quot;&gt;http://xml.amazon.\n    com/onca/xml3?ShoppingCart=add&amp;amp;f=xml&amp;amp;dev-t=&lt;xsl:value-of \n    select=&quot;$devtoken&quot; /&gt;&amp;amp;t=&lt;xsl:value-of select=&quot;$associate&quot; /&gt;\n    &amp;amp;&lt;xsl:for-each select=&quot;exsl:node-set($random_products)/\n    wishes:wishitem/Details&quot;&gt;Asin.&lt;xsl:value-of select=&quot;Asin&quot; /&gt;=1\n    &amp;amp;&lt;/xsl:for-each&gt;&lt;/xsl:variable&gt;</code></pre>\n<p>This removes the clutter of all the <code>xsl:text</code> elements, but it would need to be piled all on one line in order to keep undesired whitespace from getting into the URL.  I made a small attempt at wrapping this line here, but line breaks and spaces would leave us with a non-functioning shopping cart URL.  It&#39;s up to you to decide which to use--personally, I prefer the <code>xsl:text</code> clutter for the ability to add in comments and clarify things a bit.</p>\n<p>Finally, having built the shopping cart URL, let&#39;s use it to get a shopping cart and wrap things up:</p>\n<pre><code>    &lt;xsl:variable name=&quot;shopping_cart&quot;\n                  select=&quot;document($shopping_cart_create_url)&quot; /&gt;\n\n    &lt;xsl:copy-of select=&quot;$shopping_cart&quot; /&gt;\n\n&lt;/xsl:template&gt;  </code></pre>\n<p>As an aside, this part is pushing the concept of a REST web service a bit: In the REST philosophy, requests using the GET method (which is what <code>document()</code> uses) should only return existing resources and not create new resources or cause modifications to happen.  Instead, these sorts of actions should use a POST request.  But, since we&#39;ve already accepted a few rough edges and workarounds in this project so far, we won&#39;t let a point of esoterica like that stop us.  (That and, well, this is the way Amazon designed their web service, so we&#39;ll take what we can get.)</p>\n<p>Once you run this iteration of the stylesheet ([wishes-ex4.xsl][wishes_ex4]), you should get something like this XML as output:</p>\n<pre><code>&lt;ShoppingCartResponse ...&gt;\n  ...\n  &lt;ShoppingCart&gt;\n   &lt;CartId&gt;...&lt;/CartId&gt;\n   &lt;HMAC&gt;...&lt;/HMAC&gt;\n   &lt;PurchaseUrl&gt;...&lt;/PurchaseUrl&gt;\n   &lt;Items&gt;\n    &lt;Item&gt;...&lt;/item&gt;\n    &lt;Item&gt;...&lt;/item&gt;\n   &lt;/Items&gt;\n  &lt;/ShoppingCart&gt;\n  ...\n&lt;/ShoppingCartResponse&gt;</code></pre>\n<p>The AWS documentation describes the vital elements here like so:</p>\n<ul>\n<li><code>CartId</code> - The Cart ID is the unique identifier for a given shopping cart.</li>\n<li><code>HMAC</code> - The HMAC is a security token that must be passed back to Amazon Web Services for using an existing cart.</li>\n<li><code>PurchaseUrl</code> - Use the purchase URL to transfer the remote shopping cart from your application to Amazon so that your application&#39;s users may complete their purchases.&#160; The purchase URL merges the remote shopping cart with the Amazon.com shopping cart. </li>\n</ul>\n<p>So, in short, whenever we want to do any sort of manipulation on this Remote Shopping Cart via AWS, we&#39;ll need to remember and later supply both the <code>CartId</code> and <code>HMAC</code> found in the XML returned at its creation.  And, once we&#39;re all ready to check out, the <code>PurchaseUrl</code> points to where we&#39;ll need to browse in person.</p>\n<h3 id=\"stay-tuned\">Stay Tuned!</h3>\n<p>This concludes Part 2 of the Wish-of-the-Month Club.  Following this will be the final part, where we tie everything together and start firing off monthly emails!</p>\n<!-- links -->\n\n<!--more-->\n<p>shortname=wishofthemonthclub2</p>\n",
    "body": "<i>Here's the next installment of the Wish-of-the-Month Club.  You can [revisit the first part][part1], too, if you've missed it.  I'd meant to post it within a week of the first part, so apologies all around to anyone who has been tapping a foot waiting for it.  Enjoy!</i>\r\n\r\n### Paging Through Wishes\r\n\r\nSome ready-made files are available for this section:\r\n* [`wishes-ex2.xsl`][wishes-ex2.xsl]: The second iteration of the stylesheet in development.\r\n\r\nNow we've got a way to make queries against Amazon Web Services, not entirely unlike what you might be used to if you tinker with MySQL databases on a regular basis.  At this point, though, we still have a bit of refining to make to this query.  If you take a look at the data produced by the query in its current state, and compare that to what you see on wishlists in your browser, you should notice some things missing.\r\n\r\nIf you look at [my wishlist][mywishlist], you'll notice that items span several pages when visited by browser.  As it turns out, AWS queries work in a similar fashion--each query returns only a limited number of items (about 10), and an additional parameter supplied to further queries is required to step through further pages of results.  So, using what we've built so far will only get us to the first page of wishlist items; to get all of the items, we'll need a way to step through all of the pages.\r\n\r\nIn playing with this, I experienced a bit of hairpulling frustration:  The AWS documentation, under \"Generating Additional Product Results\", claims that XML returned by the service will supply a count of the total pages available for a given query.  And although I see this element present in other types of searches, the `TotalPages` element is absent when querying on wishlists.  This may be a bug, or it may be an undocumented change in the service--either way, it was a surprise and leaves me with no official way to know how many pages I need to ask for in order to have a complete set of data.  \r\n\r\nWith some further tinkering, though, I figured out a workaround: If a query is made for a page number beyond the final page, the XML returned will be a duplicate of the final page.  Once I see a duplicate item appear, I know it's time to stop paging through results.  This is completely undocumented behavior, and could break at any time (ie. if Amazon decided to start issuing an error for a page index out of bounds), but it'll work for now.\r\n\r\nThis calls for reworking the `processWishlist` template.  For a given wishlist, it will need to iterate through a sequence of page numbers, requesting XML from AWS for each, stopping when the first duplicate page is found.  Since XSLT is heavily steeped in functional programming concepts, this sort of [iteration in XSLT][xslt_iteration] is best done [with recursion][xslt_recursion]:\r\n\r\n      <xsl:template name=\"processWishlist\">\r\n\r\n        <xsl:param name=\"wishlist\" />              <!-- Wishlist ID -->\r\n        <xsl:param name=\"max\"   select=\"50\" />     <!-- Arbitrary upper loop limit -->\r\n        <xsl:param name=\"curr_page\" select=\"1\" />  <!-- Curr page # -->\r\n        <xsl:param name=\"prev_first_asin\" />       <!-- Keeping track of repeats -->\r\n\r\nThe first modification to this template is the addition of three parameters:\r\n\r\n* `max` provides an arbitrary upper limit to the number of pages through which this template will iterate.\r\n* `curr_page` contains the number of the page to be requested in this iteration.\r\n* `prev_first_asin` will contain the ASIN number of the first item from the previous iteration's page of results.\r\n\r\nNext, we modify the URL used to query for wishlist data:\r\n\r\n        <!-- Fetch the wishlist products -->\r\n        <xsl:variable name=\"details\" select=\"document(concat(\r\n                      'http://xml.amazon.com/onca/xml3?',\r\n                      't=',$associate,'&amp;',\r\n                      'dev-t=',$devtoken,'&amp;',\r\n                      'WishlistSearch=',$wishlist,'&amp;',\r\n                      'type=lite&amp;f=xml&amp;',\r\n                      'page=',$curr_page))//Details\" />\r\n\r\nThe only addition here beyond the previous version is the `page` parameter in the URL.  Not much mystery here--this parameter specifies which page of results we want.  Now, let's build the loop:\r\n    \r\n        <!-- Snag the first item Asin -->\r\n        <xsl:variable name=\"curr_first_asin\" select=\"$details/Asin/text()\" />\r\n    \r\n        <!-- If we haven't exceeded the loop limit, and this first Asin isn't -->\r\n        <!-- a repeat of the previous loop (indicating we've run out of new   -->\r\n        <!-- pages), then go ahead...                                         -->\r\n        <xsl:if test=\"(($curr_page+1) &lt; $max) and\r\n                      (string-length($curr_first_asin) &gt; 0) and\r\n                      not($curr_first_asin = $prev_first_asin)\">\r\n      \r\nWe capture the ASIN of the first item in this page of results and check to see if we should continue.  This `if` conditional first ensures that we're not past the sanity guard for loop iterations, makes sure that we actually got a non-empty current first ASIN, then checks our current first product's ASIN against what was passed in as the previous iteration's first product's ASIN.  If this was the first time through the loop, this value should be empty and therefore wouldn't match the current ASIN.  But, if we've gone past the end of results, the previous and current ASIN values should match, and the conditional will fail.\r\n\r\nMoving along into the body of the conditional, we copy in wishlist products filtered on a price maximum, just as before:\r\n      \r\n          <!-- Copy products, filtering on a maximum price -->\r\n          <xsl:copy-of select=\"$details/OurPrice[number(substring(\r\n                       text(),2)) &lt; $maxprice]/..\" />\r\n\r\nHaving done that, we move onto the recursive end of this template:\r\n      \r\n          <!-- Loop by recursion to get the next page -->\r\n          <xsl:call-template name=\"processWishlist\">\r\n            <xsl:with-param name=\"wishlist\"        select=\"$wishlist\" />\r\n            <xsl:with-param name=\"max\"             select=\"$max\" />\r\n            <xsl:with-param name=\"curr_page\"       select=\"$curr_page + 1\" />\r\n            <xsl:with-param name=\"prev_first_asin\" select=\"$curr_first_asin\" />\r\n          </xsl:call-template>\r\n\r\n        </xsl:if>    \r\n      </xsl:template>\r\n\r\nHere, the template makes a recursive call back to itself, passing through the wishlist ID and the maximum iteration count.  Since variables in XSLT are immutable, meaning that their values can't be changed once they've been set, we can't increment `$curr_page` in-place like a loop counter in other languages--so, the current page count *value* is incremented and passed to the recursive call as a parameter.  Finally, the current first item's ASIN is passed along, to become the previous ASIN for the next iteration.\r\n\r\nNote that when the conditional fails--that is, if the loop limit is passed or a duplicate page is detected--the loop ends.  In other words, nothing further happens and execution pops back up out of all the levels of recursion and the top-level template ends.  \r\n\r\nI wrote \"*when* the conditional fails\".  This is a key point: for the loop to eventually end, this conditional *must* fail (or be made to fail) at some point, else this loop will happily progress through page requests forever.  This is the reason for the `$max` parameter limiting the number of iterations, in case something goes haywire--like, oh say, a failure of our duplicate-page detection hack as a loop ending condition.  A useful exercise for the reader might be to add some additional diagnostic code to report that the limit was hit versus a natural end to results.\r\n\r\n\r\n### Random Numbers\r\n\r\nSome ready-made files are available for this section:\r\n* [`wishes-ex3.xsl`][wishes-ex3.xsl]: The third iteration of the stylesheet in development.\r\n* [`random-xml`][random-xml]: A Perl CGI script used as a web service to generate random numbers.\r\n\r\nArmed with a template that will query against the full set of items in a wishlist, we're ready to look into making a random selection from a list of products.  \r\n\r\nBut first, we need to pick a random number.  Unfortunately, there doesn't appear to be any `random()` function in the XPath or XSLT standards.  There *is* a [`math:random()`][exsl_random] from EXSLT implemented in `libxslt`, but I seem to be having a bit of a problem getting it to produce anything other than the same sequence of numbers.  I suspect there's a problem in seeding the random number generator, but I've yet to work out how to fix it.  (Suggestions welcome.)\r\n\r\nSo, I cheated and made another workaround with a CGI script on my web server that generates random numbers in a simple XML document.  Currently, it's hosted here:\r\n\r\n    http://www.decafbad.com/2004/05/random-xml\r\n\r\nAnd this is what the script looks like:\r\n\r\n    #!/usr/bin/perl\r\n\r\n    use strict;\r\n    use CGI;\r\n\r\n    my $q = new CGI();\r\n\r\n    my $min = $q->param('min') or 0;\r\n    my $max = $q->param('max') or 1;\r\n    my $int = $q->param('int');\r\n\r\n    my $num = $min + ( rand() * ($max - $min));\r\n    if ($int) { $num = int($num); }\r\n\r\n    print $q->header('text/xml');\r\n    print \"<rand>$num</rand>\\n\";\r\n\r\nThis is a very simple CGI.  It accepts the parameters `max`, `min`, and `int`.  The values of these parameters determine the maximum and minimum value for the random number returned, and whether or not it should be an integer.  For example, the [following URL][rand_url] should return an integer between 10 and 20:\r\n\r\n    http://www.decafbad.com/2004/05/random-xml?\r\n    int=1&#38;min=10&#38;max=20\r\n\r\nUsing this as a web service in the stylesheet with the `document()` function, we can get a random number.  If you've got web space where you can host CGI scripts, I suggest you host a copy of this script yourself, since I can't guarantee how long mine will stick around.  But, for as long at works, feel free to use the service from my server.\r\n\r\nMoving along, let's add a new named template to the stylesheet, called `randomWishlistProduct`:\r\n\r\n      <xsl:template name=\"randomWishlistProduct\">\r\n    \r\n        <xsl:param name=\"wishlist\" /> <!-- Wishlist ID -->\r\n        \r\n        <!-- Gather all the products for the current wishlist -->\r\n        <xsl:variable name=\"products\">\r\n          <xsl:call-template name=\"processWishlist\">\r\n            <xsl:with-param name=\"wishlist\" select=\"$wishlist\" />\r\n          </xsl:call-template>\r\n        </xsl:variable>\r\n\r\nJust like the `processWishlist` template, we start by defining the parameter `wishlist` to accept a wishlist ID.  Using this ID, we call the `processWishlist` template itself and store the complete list of products queried from the wishlist into the variable `$products`.\r\n\r\n        <!-- Count the products in the wishlist -->\r\n        <xsl:variable name=\"max_products\"\r\n                      select=\"count(exsl:node-set($products)/Details)\" />\r\n\r\nThis next step counts the number of products found in the wishlist.  The one tricky bit here is the use of the EXSLT function [`exsl:node-set()`][exsl_node_set]: The `$products` variable contains what's called a [*result tree fragment*][xslt_result_tree_fragment], which is a kind of cross between XML data nodes and a plain old string.  This type of data does not normally allow the full set of XPath operators to be used on it, so first we need to use `exsl:node-set()` to turn it into a full-fledged node set.  Then we can look up the `Details` element nodes and count them.  \r\n\r\n        <!-- Conjure up a random index within the list of products -->\r\n        <xsl:variable name=\"rand_product_num\"\r\n                      select=\"document(concat(\r\n                      'http://www.decafbad.com/2004/05/random-xml?',\r\n                      'int=1&amp;',\r\n                      'min=1&amp;',\r\n                      'max=',$max_products))/rand\" />\r\n\r\nHere is where the random number service comes in handy.  The `concat()` function is used to build the URL to the service, with parameters specifying that the number should be an integer, and should fall between 1 and the number of products in the wishlist.  The `document()` function grabs the XML document from the service, and the value is extracted from the single element the document contains.\r\n\r\nThere is an alternative to this last bit, should you happen to have a properly working `math:random()` function in your XSLT processor:\r\n\r\n        <xsl:variable name=\"rand_product_num\" select=\"round( math:random() *\r\n                      $max_products ) + 1\" />\r\n\r\nIf you can use this instead, you'll have no need for the random number web service.  This version is obviously more concise, and doesn't require another trip out to a web service.  You might want to try it--but if you find that you keep getting the same wishlist items selected, then you've run into the problem I found with the random number generator.\r\n\r\nNow, let's wrap this template up by selecting an item:\r\n        \r\n        <!-- Copy the product as indexed by the random number -->\r\n        <xsl:copy-of select=\"exsl:node-set($products)/Details[\r\n                     position()=$rand_product_num]\" />\r\n           \r\n      </xsl:template>\r\n\r\nAgain, we need to use the `exsl:node-set()` function to turn the result tree fragment in the `$products` variable into a node set, from which we select and copy the `Details` element whose position in the data is indexed by the random number we just selected.  Just one last tweak needed to wrap up this iteration of our stylesheet.  We need to swap out the call to the `processWishlist` function at the end and replace it with a call to `randomWishlistProduct`:\r\n\r\n      <xsl:template match=\"/wishes:wishes\">\r\n\r\n        <xsl:for-each select=\"//wishes:wishlist\">\r\n          <wishes:wishitem>\r\n            <xsl:copy-of select=\".\" />\r\n            <xsl:call-template name=\"randomWishlistProduct\">\r\n              <xsl:with-param name=\"wishlist\" select=\".\" />\r\n            </xsl:call-template>\r\n          </wishes:wishitem>\r\n        </xsl:for-each>\r\n    \r\n      </xsl:template>\r\n\r\nAfter these changes, you should be able to run the stylesheet ([wishes-ex3.xsl][wishes_ex3]) and get something like the following:\r\n\r\n    <wishes:wishitem xmlns:wishes=\"http://www.decafbad.com/2004/05/wishes\">\r\n        <wishes:wishlist label=\"The Girl\">35OIOYWQ9XQAE</wishes:wishlist>\r\n        <Details ...>...</Details>\r\n    </wishes:wishitem>\r\n    <wishes:wishitem xmlns:wishes=\"http://www.decafbad.com/2004/05/wishes\">\r\n        <wishes:wishlist label=\"Me\">1QWYI6P2JF3Q5</wishes:wishlist>\r\n        <Details ...>...</Details>\r\n    </wishes:wishitem>\r\n\r\nThis is similar to the output of the previous iteration of the stylesheet, but this time there's only one product selected at random for each wishlist.  \r\n\r\n### Shopping Carts\r\n\r\nSome ready-made files are available for this section:\r\n* [`wishes-ex4.xsl`][wishes-ex4.xsl]: The fourth iteration of the stylesheet in development.\r\n\r\nBy this point, we've been able to query and filter products in Amazon wishlists, and we've selected an item at random from each wishlist we've queried.  Now, let's enable some purchases.\r\n\r\nThe AWS provides for Remote Shopping Cart functionality, whereby items can be added to an Amazon.com shopping cart programmatically.  This is about as close as we can get to automating the purchase of items selected from the wishlists--there is no API functionality for actually completing the ordering of items.  If you really think about it, this really is a good thing and *should* demand human intervention; we certainly wouldn't want this script going crazy and accidentally buying up everything on a wishlist.\r\n\r\nDocumentation for the AWS Remote Shopping Cart explains that a shopping cart can be created and items added with a URL like the following:\r\n\r\n    http://xml.amazon.com/onca/xml3?\r\n    ShoppingCart=add&#38;\r\n    f=xml&#38;\r\n    dev-t=[Developer Token goes here]&#38;\r\n    t=[Associates ID goes here]&#38;\r\n    Asin.[ASIN goes here]=[quantity goes here]&#38;\r\n    sims=true\r\n\r\nPart of this should look familiar, so we already know what to do with the developer token and the associates ID.  The last part, specifying product ASIN and quantity, can be filled out with information contained in the product records selected at random from the wishlists.  \r\n\r\nSo, let's start by revising the template at the end of the stylesheet:\r\n\r\n    <xsl:template match=\"/wishes:wishes\">\r\n\r\n        <xsl:variable name=\"random_products\">      \r\n          <xsl:for-each select=\"//wishes:wishlist\">\r\n            <wishes:wishitem>\r\n              <xsl:copy-of select=\".\" />\r\n              <xsl:call-template name=\"randomWishlistProduct\">\r\n                <xsl:with-param name=\"wishlist\" select=\".\" />\r\n              </xsl:call-template>\r\n            </wishes:wishitem>\r\n          </xsl:for-each>\r\n        </xsl:variable>\r\n\r\nHere, we've taken what was the output of the previous iteration of the stylesheet and stuffed it into the variable `$random_products`.  Next, let's fill in the blanks and build a Remote Shopping Cart URL:\r\n\r\n        <xsl:variable name=\"shopping_cart_create_url\">\r\n          <!-- Standard AWS URL -->\r\n          <xsl:text>http://xml.amazon.com/onca/xml3?</xsl:text>\r\n\r\n          <!-- Add in the selected items -->\r\n          <xsl:for-each select=\"exsl:node-set($random_products)\r\n                                /wishes:wishitem/Details\">\r\n            <xsl:text>Asin.</xsl:text><xsl:value-of select=\"Asin\" />\r\n            <xsl:text>=1&amp;</xsl:text>\r\n          </xsl:for-each>\r\n\r\n          <!-- Wrap up with the shopping cart function and required tokens -->\r\n          <xsl:text>ShoppingCart=add&amp;</xsl:text>\r\n          <xsl:text>f=xml&amp;</xsl:text>\r\n          <xsl:text>dev-t=</xsl:text><xsl:value-of select=\"$devtoken\" />\r\n          <xsl:text>&amp;</xsl:text>\r\n          <xsl:text>t=</xsl:text><xsl:value-of select=\"$associate\" />\r\n        </xsl:variable>\r\n    \r\nSince simple XPath doesn't allow for the looping needed for multiple items, we can't just concatenate this URL together in a `select` expression like we did with the wishlist item query.  So, we use `xslt:foreach` to build this with blocks of text using the `xsl:text` element.  We iterate though the random products chosen from wishlists and add an ASIN for each to the URL with a quantity of 1. Then, we use the `$devtoken` and `$associate` variables to fill in their respective spots.\r\n\r\nNote that this could have been written without using the `xsl:text` elements like so:\r\n\r\n        <xsl:variable name=\"shopping_cart_create_url\">http://xml.amazon.\r\n        com/onca/xml3?ShoppingCart=add&amp;f=xml&amp;dev-t=<xsl:value-of \r\n        select=\"$devtoken\" />&amp;t=<xsl:value-of select=\"$associate\" />\r\n        &amp;<xsl:for-each select=\"exsl:node-set($random_products)/\r\n        wishes:wishitem/Details\">Asin.<xsl:value-of select=\"Asin\" />=1\r\n        &amp;</xsl:for-each></xsl:variable>\r\n\r\nThis removes the clutter of all the `xsl:text` elements, but it would need to be piled all on one line in order to keep undesired whitespace from getting into the URL.  I made a small attempt at wrapping this line here, but line breaks and spaces would leave us with a non-functioning shopping cart URL.  It's up to you to decide which to use--personally, I prefer the `xsl:text` clutter for the ability to add in comments and clarify things a bit.\r\n\r\nFinally, having built the shopping cart URL, let's use it to get a shopping cart and wrap things up:\r\n\r\n        <xsl:variable name=\"shopping_cart\"\r\n                      select=\"document($shopping_cart_create_url)\" />\r\n    \r\n        <xsl:copy-of select=\"$shopping_cart\" />\r\n    \r\n    </xsl:template>  \r\n\r\nAs an aside, this part is pushing the concept of a REST web service a bit: In the REST philosophy, requests using the GET method (which is what `document()` uses) should only return existing resources and not create new resources or cause modifications to happen.  Instead, these sorts of actions should use a POST request.  But, since we've already accepted a few rough edges and workarounds in this project so far, we won't let a point of esoterica like that stop us.  (That and, well, this is the way Amazon designed their web service, so we'll take what we can get.)\r\n\r\nOnce you run this iteration of the stylesheet ([wishes-ex4.xsl][wishes_ex4]), you should get something like this XML as output:\r\n\r\n    <ShoppingCartResponse ...>\r\n      ...\r\n      <ShoppingCart>\r\n       <CartId>...</CartId>\r\n       <HMAC>...</HMAC>\r\n       <PurchaseUrl>...</PurchaseUrl>\r\n       <Items>\r\n        <Item>...</item>\r\n        <Item>...</item>\r\n       </Items>\r\n      </ShoppingCart>\r\n      ...\r\n    </ShoppingCartResponse>\r\n\r\nThe AWS documentation describes the vital elements here like so:\r\n\r\n* `CartId` - The Cart ID is the unique identifier for a given shopping cart.\r\n* `HMAC` - The HMAC is a security token that must be passed back to Amazon Web Services for using an existing cart.\r\n* `PurchaseUrl` - Use the purchase URL to transfer the remote shopping cart from your application to Amazon so that your application's users may complete their purchases.&#160; The purchase URL merges the remote shopping cart with the Amazon.com shopping cart. \r\n\r\nSo, in short, whenever we want to do any sort of manipulation on this Remote Shopping Cart via AWS, we'll need to remember and later supply both the `CartId` and `HMAC` found in the XML returned at its creation.  And, once we're all ready to check out, the `PurchaseUrl` points to where we'll need to browse in person.\r\n\r\n### Stay Tuned!\r\n\r\nThis concludes Part 2 of the Wish-of-the-Month Club.  Following this will be the final part, where we tie everything together and start firing off monthly emails!\r\n\r\n<!-- links -->\r\n\r\n[missadroit]: http://missadroit.livejournal.com \"Miss Adroit, my favorite girl in the world\"\r\n[mywishlist]: http://www.amazon.com/exec/obidos/registry/1QWYI6P2JF3Q5 \"Buy me something, will ya?\"\r\n[herwishlist]: http://www.amazon.com/exec/obidos/registry/35OIOYWQ9XQAE \"Buy her something, will ya?\"\r\n[amazonapi]: http://www.amazon.com/gp/aws/landing.html \"Amazon Web Services\"\r\n[libxml]: http://www.xmlsoft.org/\r\n[xalan]: http://xml.apache.org/xalan-j/\r\n[sablotron]: http://www.gingerall.com/charlie/ga/xml/p_sab.xml\r\n[saxon]: http://saxon.sourceforge.net/\r\n[exslt]: http://www.exslt.org/\r\n[libxslt]: http://www.xmlsoft.org/XSLT.html\r\n[spideringhacks]: http://www.amazon.com/exec/obidos/ASIN/0596005776/0xdecafbad-20 \"O'Reilly's Spidering Hacks\"\r\n[xslscraper]: http://www.decafbad.com/twiki/bin/view/Main/XslScraper \"Scrape RSS and Atom from HTML using Tidy and XSLT\"\r\n[awsdownload]: http://www.amazon.com/gp/browse.html/ref=sc_fe_c_2/002-7899886-3676027?%5Fencoding=UTF8&#38;node=3434641&#38;no=3435361&#38;me=A36L942TSJ2AJA\r\n[awstoken]: https://associates.amazon.com/exec/panama/associates/join/developer/application.html\r\n[amazonassociate]: http://associates.amazon.com\r\n[wlsearch]: http://www.amazon.com/gp/registry/search.html/002-7899886-3676027?%5Fencoding=UTF8&#38;type=wishlist\r\n[wlurl]: http://xml.amazon.com/onca/xml3?t=0xdecafbad-20&#38;dev-t=D8HVH869XA0NP&#38;type=lite&#38;WishlistSearch=35OIOYWQ9XQAE&#38;f=xml\r\n[detailsurl]: http://www.amazon.com/exec/obidos/ASIN/0262133601/0xdecafbad-20?dev-t=D8HVH869XA0NP%26camp=2025%26link_code=xm2\r\n[awslite]: http://xml.amazon.com/schemas3/dev-lite.xsd\r\n[fink]: http://fink.sourceforge.net\r\n[testxslt]: http://www.entropy.ch:16080/software/macosx/#testxslt\r\n[darwinports]: http://darwinports.opendarwin.org/\r\n[curl]: http://www.decafbad.com/#TODO\r\n[wget]: http://www.decafbad.com/#TODO\r\n[xpconcat]: http://www.w3.org/TR/2002/WD-xquery-operators-20020816/#func-concat\r\n[xpdocument]: http://www.w3.org/TR/2002/WD-xquery-operators-20020816/#func-document\r\n[wishescvs]: http://www.decafbad.com/cvs/hacks/wishes/\r\n[wishes.tar.gz]: http://www.decafbad.com/cvs/hacks/wishes/wishes.tar.gz?tarball=1 \"All Wish-of-the-Month Club files wrapped up in a tarball\"\r\n[wishes.xml]: http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes.xml\r\n[wishes-ex1.xsl]: http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes-ex1.xsl\r\n[wishes-ex2.xsl]: http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes-ex2.xsl\r\n[wishes-ex3.xsl]: http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes-ex3.xsl\r\n[wishes-ex4.xsl]: http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes-ex4.xsl\r\n[wishes-ex5.xsl]: http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes-ex5.xsl\r\n[wishes-ex6.xsl]: http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes-ex6.xsl\r\n[random-xml]: http://www.decafbad.com/cvs/*checkout*/hacks/wishes/random-xml\r\n[wishes_html_screenshot]: http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes.jpg\r\n[xslt_iteration]: http://www.dpawson.co.uk/xsl/sect2/N4806.html \"Iteration in XSLT\"\r\n[xslt_recursion]: http://www-106.ibm.com/developerworks/xml/library/x-xslrecur/ \"Use recursion effectively in XSL\"\r\n[exsl_random]: http://www.exslt.org/math/functions/random/index.html\r\n[exsl_node_set]: http://www.exslt.org/exsl/functions/node-set/index.html\r\n[rand_url]: http://www.decafbad.com/2004/05/random-xml?int=1&#38;min=10&#38;max=20 \"A random integer between 10 and 20, in XML\"\r\n[xslt_result_tree_fragment]: http://www.w3.org/TR/xslt#section-Result-Tree-Fragments\r\n\r\n[email_attach_anatomy]: http://www.dpo.uab.edu/Email/attach.html \"Anatomy of an Email Attachment\"\r\n[email_mime_and_html]: http://www.abiglime.com/webmaster/articles/cgi/010698.htm \"How to encapsulate HTML in an email message\"\r\n\r\n[email_html_and_text]: http://www.wilsonweb.com/wmt5/html-email-multi.htm \"Sending HTML and Plain Text E-Mail Simultaneously\"\r\n[man_sendmail]: http://www.hmug.org/man/8/sendmail.html \"man: sendmail\"\r\n[rfc1521]: http://www.faqs.org/rfcs/rfc1521.html \"RFC 1521\"\r\n[cron1]: http://www.lysator.liu.se/~forsberg/linux/cron.html \"Doing things periodically - Using CRON\"\r\n[cron2]: http://www.itworld.com/Comp/2378/swol-0825-unix101/ \"Using cron basics\"\r\n[python_libxml]: http://xmlsoft.org/python.html \r\n[part1]: http://www.decafbad.com/blog/2004/06/16/wishofthemonthclub1\r\n<!--more-->\r\nshortname=wishofthemonthclub2\r\n",
    "parentPath": "../blog.lmorchard.com/posts/archives/2004",
    "path": "2004/06/28/wishofthemonthclub2",
    "summary": "<p><i>Here&apos;s the next installment of the Wish-of-the-Month Club.  You can <a href=\"http://www.decafbad.com/blog/2004/06/16/wishofthemonthclub1\">revisit the first part</a>, too, if you&apos;ve missed it.  I&apos;d meant to post it within a week of the first part, so apologies all around to anyone who has been tapping a foot waiting for it.  Enjoy!</i></p>\n<h3 id=\"paging-through-wishes\">Paging Through Wishes</h3>\n<p>Some ready-made files are available for this section:</p>\n<ul>\n<li><a href=\"http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes-ex2.xsl\"><code>wishes-ex2.xsl</code></a>: The second iteration of the stylesheet in development.</li>\n</ul>\n<p>Now we&apos;ve got a way to make queries against Amazon Web Services, not entirely unlike what you might be used to if you tinker with MySQL databases on a regular basis.  At this point, though, we still have a bit of refining to make to this query.  If you take a look at the data produced by the query in its current state, and compare that to what you see on wishlists in your browser, you should notice some things missing.</p>\n<p>If you look at <a href=\"http://www.amazon.com/exec/obidos/registry/1QWYI6P2JF3Q5\" title=\"Buy me something, will ya?\">my wishlist</a>, you&apos;ll notice that items span several pages when visited by browser.  As it turns out, AWS queries work in a similar fashion--each query returns only a limited number of items (about 10), and an additional parameter supplied to further queries is required to step through further pages of results.  So, using what we&apos;ve built so far will only get us to the first page of wishlist items; to get all of the items, we&apos;ll need a way to step through all of the pages.</p>\n<p>In playing with this, I experienced a bit of hairpulling frustration:  The AWS documentation, under &quot;Generating Additional Product Results&quot;, claims that XML returned by the service will supply a count of the total pages available for a given query.  And although I see this element present in other types of searches, the <code>TotalPages</code> element is absent when querying on wishlists.  This may be a bug, or it may be an undocumented change in the service--either way, it was a surprise and leaves me with no official way to know how many pages I need to ask for in order to have a complete set of data.  </p>\n<p>With some further tinkering, though, I figured out a workaround: If a query is made for a page number beyond the final page, the XML returned will be a duplicate of the final page.  Once I see a duplicate item appear, I know it&apos;s time to stop paging through results.  This is completely undocumented behavior, and could break at any time (ie. if Amazon decided to start issuing an error for a page index out of bounds), but it&apos;ll work for now.</p>\n<p>This calls for reworking the <code>processWishlist</code> template.  For a given wishlist, it will need to iterate through a sequence of page numbers, requesting XML from AWS for each, stopping when the first duplicate page is found.  Since XSLT is heavily steeped in functional programming concepts, this sort of <a href=\"http://www.dpawson.co.uk/xsl/sect2/N4806.html\" title=\"Iteration in XSLT\">iteration in XSLT</a> is best done <a href=\"http://www-106.ibm.com/developerworks/xml/library/x-xslrecur/\" title=\"Use recursion effectively in XSL\">with recursion</a>:</p>\n<pre><code>  &lt;xsl:template name=&quot;processWishlist&quot;&gt;\n\n    &lt;xsl:param name=&quot;wishlist&quot; /&gt;              &lt;!-- Wishlist ID --&gt;\n    &lt;xsl:param name=&quot;max&quot;   select=&quot;50&quot; /&gt;     &lt;!-- Arbitrary upper loop limit --&gt;\n    &lt;xsl:param name=&quot;curr_page&quot; select=&quot;1&quot; /&gt;  &lt;!-- Curr page # --&gt;\n    &lt;xsl:param name=&quot;prev_first_asin&quot; /&gt;       &lt;!-- Keeping track of repeats --&gt;</code></pre>\n<p>The first modification to this template is the addition of three parameters:</p>\n<ul>\n<li><code>max</code> provides an arbitrary upper limit to the number of pages through which this template will iterate.</li>\n<li><code>curr_page</code> contains the number of the page to be requested in this iteration.</li>\n<li><code>prev_first_asin</code> will contain the ASIN number of the first item from the previous iteration&apos;s page of results.</li>\n</ul>\n<p>Next, we modify the URL used to query for wishlist data:</p>\n<pre><code>    &lt;!-- Fetch the wishlist products --&gt;\n    &lt;xsl:variable name=&quot;details&quot; select=&quot;document(concat(\n                  &apos;http://xml.amazon.com/onca/xml3?&apos;,\n                  &apos;t=&apos;,$associate,&apos;&amp;amp;&apos;,\n                  &apos;dev-t=&apos;,$devtoken,&apos;&amp;amp;&apos;,\n                  &apos;WishlistSearch=&apos;,$wishlist,&apos;&amp;amp;&apos;,\n                  &apos;type=lite&amp;amp;f=xml&amp;amp;&apos;,\n                  &apos;page=&apos;,$curr_page))//Details&quot; /&gt;</code></pre>\n<p>The only addition here beyond the previous version is the <code>page</code> parameter in the URL.  Not much mystery here--this parameter specifies which page of results we want.  Now, let&apos;s build the loop:</p>\n<pre><code>    &lt;!-- Snag the first item Asin --&gt;\n    &lt;xsl:variable name=&quot;curr_first_asin&quot; select=&quot;$details/Asin/text()&quot; /&gt;\n\n    &lt;!-- If we haven&apos;t exceeded the loop limit, and this first Asin isn&apos;t --&gt;\n    &lt;!-- a repeat of the previous loop (indicating we&apos;ve run out of new   --&gt;\n    &lt;!-- pages), then go ahead...                                         --&gt;\n    &lt;xsl:if test=&quot;(($curr_page+1) &amp;lt; $max) and\n                  (string-length($curr_first_asin) &amp;gt; 0) and\n                  not($curr_first_asin = $prev_first_asin)&quot;&gt;</code></pre>\n<p>We capture the ASIN of the first item in this page of results and check to see if we should continue.  This <code>if</code> conditional first ensures that we&apos;re not past the sanity guard for loop iterations, makes sure that we actually got a non-empty current first ASIN, then checks our current first product&apos;s ASIN against what was passed in as the previous iteration&apos;s first product&apos;s ASIN.  If this was the first time through the loop, this value should be empty and therefore wouldn&apos;t match the current ASIN.  But, if we&apos;ve gone past the end of results, the previous and current ASIN values should match, and the conditional will fail.</p>\n<p>Moving along into the body of the conditional, we copy in wishlist products filtered on a price maximum, just as before:</p>\n<pre><code>      &lt;!-- Copy products, filtering on a maximum price --&gt;\n      &lt;xsl:copy-of select=&quot;$details/OurPrice[number(substring(\n                   text(),2)) &amp;lt; $maxprice]/..&quot; /&gt;</code></pre>\n<p>Having done that, we move onto the recursive end of this template:</p>\n<pre><code>      &lt;!-- Loop by recursion to get the next page --&gt;\n      &lt;xsl:call-template name=&quot;processWishlist&quot;&gt;\n        &lt;xsl:with-param name=&quot;wishlist&quot;        select=&quot;$wishlist&quot; /&gt;\n        &lt;xsl:with-param name=&quot;max&quot;             select=&quot;$max&quot; /&gt;\n        &lt;xsl:with-param name=&quot;curr_page&quot;       select=&quot;$curr_page + 1&quot; /&gt;\n        &lt;xsl:with-param name=&quot;prev_first_asin&quot; select=&quot;$curr_first_asin&quot; /&gt;\n      &lt;/xsl:call-template&gt;\n\n    &lt;/xsl:if&gt;    \n  &lt;/xsl:template&gt;</code></pre>\n<p>Here, the template makes a recursive call back to itself, passing through the wishlist ID and the maximum iteration count.  Since variables in XSLT are immutable, meaning that their values can&apos;t be changed once they&apos;ve been set, we can&apos;t increment <code>$curr_page</code> in-place like a loop counter in other languages--so, the current page count <em>value</em> is incremented and passed to the recursive call as a parameter.  Finally, the current first item&apos;s ASIN is passed along, to become the previous ASIN for the next iteration.</p>\n<p>Note that when the conditional fails--that is, if the loop limit is passed or a duplicate page is detected--the loop ends.  In other words, nothing further happens and execution pops back up out of all the levels of recursion and the top-level template ends.  </p>\n<p>I wrote &quot;<em>when</em> the conditional fails&quot;.  This is a key point: for the loop to eventually end, this conditional <em>must</em> fail (or be made to fail) at some point, else this loop will happily progress through page requests forever.  This is the reason for the <code>$max</code> parameter limiting the number of iterations, in case something goes haywire--like, oh say, a failure of our duplicate-page detection hack as a loop ending condition.  A useful exercise for the reader might be to add some additional diagnostic code to report that the limit was hit versus a natural end to results.</p>\n<h3 id=\"random-numbers\">Random Numbers</h3>\n<p>Some ready-made files are available for this section:</p>\n<ul>\n<li><a href=\"http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes-ex3.xsl\"><code>wishes-ex3.xsl</code></a>: The third iteration of the stylesheet in development.</li>\n<li><a href=\"http://www.decafbad.com/cvs/*checkout*/hacks/wishes/random-xml\"><code>random-xml</code></a>: A Perl CGI script used as a web service to generate random numbers.</li>\n</ul>\n<p>Armed with a template that will query against the full set of items in a wishlist, we&apos;re ready to look into making a random selection from a list of products.  </p>\n<p>But first, we need to pick a random number.  Unfortunately, there doesn&apos;t appear to be any <code>random()</code> function in the XPath or XSLT standards.  There <em>is</em> a <a href=\"http://www.exslt.org/math/functions/random/index.html\"><code>math:random()</code></a> from EXSLT implemented in <code>libxslt</code>, but I seem to be having a bit of a problem getting it to produce anything other than the same sequence of numbers.  I suspect there&apos;s a problem in seeding the random number generator, but I&apos;ve yet to work out how to fix it.  (Suggestions welcome.)</p>\n<p>So, I cheated and made another workaround with a CGI script on my web server that generates random numbers in a simple XML document.  Currently, it&apos;s hosted here:</p>\n<pre><code>http://www.decafbad.com/2004/05/random-xml</code></pre>\n<p>And this is what the script looks like:</p>\n<pre><code>#!/usr/bin/perl\n\nuse strict;\nuse CGI;\n\nmy $q = new CGI();\n\nmy $min = $q-&gt;param(&apos;min&apos;) or 0;\nmy $max = $q-&gt;param(&apos;max&apos;) or 1;\nmy $int = $q-&gt;param(&apos;int&apos;);\n\nmy $num = $min + ( rand() * ($max - $min));\nif ($int) { $num = int($num); }\n\nprint $q-&gt;header(&apos;text/xml&apos;);\nprint &quot;&lt;rand&gt;$num&lt;/rand&gt;\\n&quot;;</code></pre>\n<p>This is a very simple CGI.  It accepts the parameters <code>max</code>, <code>min</code>, and <code>int</code>.  The values of these parameters determine the maximum and minimum value for the random number returned, and whether or not it should be an integer.  For example, the <a href=\"http://www.decafbad.com/2004/05/random-xml?int=1&amp;min=10&amp;max=20\" title=\"A random integer between 10 and 20, in XML\">following URL</a> should return an integer between 10 and 20:</p>\n<pre><code>http://www.decafbad.com/2004/05/random-xml?\nint=1&amp;#38;min=10&amp;#38;max=20</code></pre>\n<p>Using this as a web service in the stylesheet with the <code>document()</code> function, we can get a random number.  If you&apos;ve got web space where you can host CGI scripts, I suggest you host a copy of this script yourself, since I can&apos;t guarantee how long mine will stick around.  But, for as long at works, feel free to use the service from my server.</p>\n<p>Moving along, let&apos;s add a new named template to the stylesheet, called <code>randomWishlistProduct</code>:</p>\n<pre><code>  &lt;xsl:template name=&quot;randomWishlistProduct&quot;&gt;\n\n    &lt;xsl:param name=&quot;wishlist&quot; /&gt; &lt;!-- Wishlist ID --&gt;\n\n    &lt;!-- Gather all the products for the current wishlist --&gt;\n    &lt;xsl:variable name=&quot;products&quot;&gt;\n      &lt;xsl:call-template name=&quot;processWishlist&quot;&gt;\n        &lt;xsl:with-param name=&quot;wishlist&quot; select=&quot;$wishlist&quot; /&gt;\n      &lt;/xsl:call-template&gt;\n    &lt;/xsl:variable&gt;</code></pre>\n<p>Just like the <code>processWishlist</code> template, we start by defining the parameter <code>wishlist</code> to accept a wishlist ID.  Using this ID, we call the <code>processWishlist</code> template itself and store the complete list of products queried from the wishlist into the variable <code>$products</code>.</p>\n<pre><code>    &lt;!-- Count the products in the wishlist --&gt;\n    &lt;xsl:variable name=&quot;max_products&quot;\n                  select=&quot;count(exsl:node-set($products)/Details)&quot; /&gt;</code></pre>\n<p>This next step counts the number of products found in the wishlist.  The one tricky bit here is the use of the EXSLT function <a href=\"http://www.exslt.org/exsl/functions/node-set/index.html\"><code>exsl:node-set()</code></a>: The <code>$products</code> variable contains what&apos;s called a <a href=\"http://www.w3.org/TR/xslt#section-Result-Tree-Fragments\"><em>result tree fragment</em></a>, which is a kind of cross between XML data nodes and a plain old string.  This type of data does not normally allow the full set of XPath operators to be used on it, so first we need to use <code>exsl:node-set()</code> to turn it into a full-fledged node set.  Then we can look up the <code>Details</code> element nodes and count them.  </p>\n<pre><code>    &lt;!-- Conjure up a random index within the list of products --&gt;\n    &lt;xsl:variable name=&quot;rand_product_num&quot;\n                  select=&quot;document(concat(\n                  &apos;http://www.decafbad.com/2004/05/random-xml?&apos;,\n                  &apos;int=1&amp;amp;&apos;,\n                  &apos;min=1&amp;amp;&apos;,\n                  &apos;max=&apos;,$max_products))/rand&quot; /&gt;</code></pre>\n<p>Here is where the random number service comes in handy.  The <code>concat()</code> function is used to build the URL to the service, with parameters specifying that the number should be an integer, and should fall between 1 and the number of products in the wishlist.  The <code>document()</code> function grabs the XML document from the service, and the value is extracted from the single element the document contains.</p>\n<p>There is an alternative to this last bit, should you happen to have a properly working <code>math:random()</code> function in your XSLT processor:</p>\n<pre><code>    &lt;xsl:variable name=&quot;rand_product_num&quot; select=&quot;round( math:random() *\n                  $max_products ) + 1&quot; /&gt;</code></pre>\n<p>If you can use this instead, you&apos;ll have no need for the random number web service.  This version is obviously more concise, and doesn&apos;t require another trip out to a web service.  You might want to try it--but if you find that you keep getting the same wishlist items selected, then you&apos;ve run into the problem I found with the random number generator.</p>\n<p>Now, let&apos;s wrap this template up by selecting an item:</p>\n<pre><code>    &lt;!-- Copy the product as indexed by the random number --&gt;\n    &lt;xsl:copy-of select=&quot;exsl:node-set($products)/Details[\n                 position()=$rand_product_num]&quot; /&gt;\n\n  &lt;/xsl:template&gt;</code></pre>\n<p>Again, we need to use the <code>exsl:node-set()</code> function to turn the result tree fragment in the <code>$products</code> variable into a node set, from which we select and copy the <code>Details</code> element whose position in the data is indexed by the random number we just selected.  Just one last tweak needed to wrap up this iteration of our stylesheet.  We need to swap out the call to the <code>processWishlist</code> function at the end and replace it with a call to <code>randomWishlistProduct</code>:</p>\n<pre><code>  &lt;xsl:template match=&quot;/wishes:wishes&quot;&gt;\n\n    &lt;xsl:for-each select=&quot;//wishes:wishlist&quot;&gt;\n      &lt;wishes:wishitem&gt;\n        &lt;xsl:copy-of select=&quot;.&quot; /&gt;\n        &lt;xsl:call-template name=&quot;randomWishlistProduct&quot;&gt;\n          &lt;xsl:with-param name=&quot;wishlist&quot; select=&quot;.&quot; /&gt;\n        &lt;/xsl:call-template&gt;\n      &lt;/wishes:wishitem&gt;\n    &lt;/xsl:for-each&gt;\n\n  &lt;/xsl:template&gt;</code></pre>\n<p>After these changes, you should be able to run the stylesheet ([wishes-ex3.xsl][wishes_ex3]) and get something like the following:</p>\n<pre><code>&lt;wishes:wishitem xmlns:wishes=&quot;http://www.decafbad.com/2004/05/wishes&quot;&gt;\n    &lt;wishes:wishlist label=&quot;The Girl&quot;&gt;35OIOYWQ9XQAE&lt;/wishes:wishlist&gt;\n    &lt;Details ...&gt;...&lt;/Details&gt;\n&lt;/wishes:wishitem&gt;\n&lt;wishes:wishitem xmlns:wishes=&quot;http://www.decafbad.com/2004/05/wishes&quot;&gt;\n    &lt;wishes:wishlist label=&quot;Me&quot;&gt;1QWYI6P2JF3Q5&lt;/wishes:wishlist&gt;\n    &lt;Details ...&gt;...&lt;/Details&gt;\n&lt;/wishes:wishitem&gt;</code></pre>\n<p>This is similar to the output of the previous iteration of the stylesheet, but this time there&apos;s only one product selected at random for each wishlist.  </p>\n<h3 id=\"shopping-carts\">Shopping Carts</h3>\n<p>Some ready-made files are available for this section:</p>\n<ul>\n<li><a href=\"http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes-ex4.xsl\"><code>wishes-ex4.xsl</code></a>: The fourth iteration of the stylesheet in development.</li>\n</ul>\n<p>By this point, we&apos;ve been able to query and filter products in Amazon wishlists, and we&apos;ve selected an item at random from each wishlist we&apos;ve queried.  Now, let&apos;s enable some purchases.</p>\n<p>The AWS provides for Remote Shopping Cart functionality, whereby items can be added to an Amazon.com shopping cart programmatically.  This is about as close as we can get to automating the purchase of items selected from the wishlists--there is no API functionality for actually completing the ordering of items.  If you really think about it, this really is a good thing and <em>should</em> demand human intervention; we certainly wouldn&apos;t want this script going crazy and accidentally buying up everything on a wishlist.</p>\n<p>Documentation for the AWS Remote Shopping Cart explains that a shopping cart can be created and items added with a URL like the following:</p>\n<pre><code>http://xml.amazon.com/onca/xml3?\nShoppingCart=add&amp;#38;\nf=xml&amp;#38;\ndev-t=[Developer Token goes here]&amp;#38;\nt=[Associates ID goes here]&amp;#38;\nAsin.[ASIN goes here]=[quantity goes here]&amp;#38;\nsims=true</code></pre>\n<p>Part of this should look familiar, so we already know what to do with the developer token and the associates ID.  The last part, specifying product ASIN and quantity, can be filled out with information contained in the product records selected at random from the wishlists.  </p>\n<p>So, let&apos;s start by revising the template at the end of the stylesheet:</p>\n<pre><code>&lt;xsl:template match=&quot;/wishes:wishes&quot;&gt;\n\n    &lt;xsl:variable name=&quot;random_products&quot;&gt;      \n      &lt;xsl:for-each select=&quot;//wishes:wishlist&quot;&gt;\n        &lt;wishes:wishitem&gt;\n          &lt;xsl:copy-of select=&quot;.&quot; /&gt;\n          &lt;xsl:call-template name=&quot;randomWishlistProduct&quot;&gt;\n            &lt;xsl:with-param name=&quot;wishlist&quot; select=&quot;.&quot; /&gt;\n          &lt;/xsl:call-template&gt;\n        &lt;/wishes:wishitem&gt;\n      &lt;/xsl:for-each&gt;\n    &lt;/xsl:variable&gt;</code></pre>\n<p>Here, we&apos;ve taken what was the output of the previous iteration of the stylesheet and stuffed it into the variable <code>$random_products</code>.  Next, let&apos;s fill in the blanks and build a Remote Shopping Cart URL:</p>\n<pre><code>    &lt;xsl:variable name=&quot;shopping_cart_create_url&quot;&gt;\n      &lt;!-- Standard AWS URL --&gt;\n      &lt;xsl:text&gt;http://xml.amazon.com/onca/xml3?&lt;/xsl:text&gt;\n\n      &lt;!-- Add in the selected items --&gt;\n      &lt;xsl:for-each select=&quot;exsl:node-set($random_products)\n                            /wishes:wishitem/Details&quot;&gt;\n        &lt;xsl:text&gt;Asin.&lt;/xsl:text&gt;&lt;xsl:value-of select=&quot;Asin&quot; /&gt;\n        &lt;xsl:text&gt;=1&amp;amp;&lt;/xsl:text&gt;\n      &lt;/xsl:for-each&gt;\n\n      &lt;!-- Wrap up with the shopping cart function and required tokens --&gt;\n      &lt;xsl:text&gt;ShoppingCart=add&amp;amp;&lt;/xsl:text&gt;\n      &lt;xsl:text&gt;f=xml&amp;amp;&lt;/xsl:text&gt;\n      &lt;xsl:text&gt;dev-t=&lt;/xsl:text&gt;&lt;xsl:value-of select=&quot;$devtoken&quot; /&gt;\n      &lt;xsl:text&gt;&amp;amp;&lt;/xsl:text&gt;\n      &lt;xsl:text&gt;t=&lt;/xsl:text&gt;&lt;xsl:value-of select=&quot;$associate&quot; /&gt;\n    &lt;/xsl:variable&gt;</code></pre>\n<p>Since simple XPath doesn&apos;t allow for the looping needed for multiple items, we can&apos;t just concatenate this URL together in a <code>select</code> expression like we did with the wishlist item query.  So, we use <code>xslt:foreach</code> to build this with blocks of text using the <code>xsl:text</code> element.  We iterate though the random products chosen from wishlists and add an ASIN for each to the URL with a quantity of 1. Then, we use the <code>$devtoken</code> and <code>$associate</code> variables to fill in their respective spots.</p>\n<p>Note that this could have been written without using the <code>xsl:text</code> elements like so:</p>\n<pre><code>    &lt;xsl:variable name=&quot;shopping_cart_create_url&quot;&gt;http://xml.amazon.\n    com/onca/xml3?ShoppingCart=add&amp;amp;f=xml&amp;amp;dev-t=&lt;xsl:value-of \n    select=&quot;$devtoken&quot; /&gt;&amp;amp;t=&lt;xsl:value-of select=&quot;$associate&quot; /&gt;\n    &amp;amp;&lt;xsl:for-each select=&quot;exsl:node-set($random_products)/\n    wishes:wishitem/Details&quot;&gt;Asin.&lt;xsl:value-of select=&quot;Asin&quot; /&gt;=1\n    &amp;amp;&lt;/xsl:for-each&gt;&lt;/xsl:variable&gt;</code></pre>\n<p>This removes the clutter of all the <code>xsl:text</code> elements, but it would need to be piled all on one line in order to keep undesired whitespace from getting into the URL.  I made a small attempt at wrapping this line here, but line breaks and spaces would leave us with a non-functioning shopping cart URL.  It&apos;s up to you to decide which to use--personally, I prefer the <code>xsl:text</code> clutter for the ability to add in comments and clarify things a bit.</p>\n<p>Finally, having built the shopping cart URL, let&apos;s use it to get a shopping cart and wrap things up:</p>\n<pre><code>    &lt;xsl:variable name=&quot;shopping_cart&quot;\n                  select=&quot;document($shopping_cart_create_url)&quot; /&gt;\n\n    &lt;xsl:copy-of select=&quot;$shopping_cart&quot; /&gt;\n\n&lt;/xsl:template&gt;  </code></pre>\n<p>As an aside, this part is pushing the concept of a REST web service a bit: In the REST philosophy, requests using the GET method (which is what <code>document()</code> uses) should only return existing resources and not create new resources or cause modifications to happen.  Instead, these sorts of actions should use a POST request.  But, since we&apos;ve already accepted a few rough edges and workarounds in this project so far, we won&apos;t let a point of esoterica like that stop us.  (That and, well, this is the way Amazon designed their web service, so we&apos;ll take what we can get.)</p>\n<p>Once you run this iteration of the stylesheet ([wishes-ex4.xsl][wishes_ex4]), you should get something like this XML as output:</p>\n<pre><code>&lt;ShoppingCartResponse ...&gt;\n  ...\n  &lt;ShoppingCart&gt;\n   &lt;CartId&gt;...&lt;/CartId&gt;\n   &lt;HMAC&gt;...&lt;/HMAC&gt;\n   &lt;PurchaseUrl&gt;...&lt;/PurchaseUrl&gt;\n   &lt;Items&gt;\n    &lt;Item&gt;...&lt;/item&gt;\n    &lt;Item&gt;...&lt;/item&gt;\n   &lt;/Items&gt;\n  &lt;/ShoppingCart&gt;\n  ...\n&lt;/ShoppingCartResponse&gt;</code></pre>\n<p>The AWS documentation describes the vital elements here like so:</p>\n<ul>\n<li><code>CartId</code> - The Cart ID is the unique identifier for a given shopping cart.</li>\n<li><code>HMAC</code> - The HMAC is a security token that must be passed back to Amazon Web Services for using an existing cart.</li>\n<li><code>PurchaseUrl</code> - Use the purchase URL to transfer the remote shopping cart from your application to Amazon so that your application&apos;s users may complete their purchases.&#xA0; The purchase URL merges the remote shopping cart with the Amazon.com shopping cart. </li>\n</ul>\n<p>So, in short, whenever we want to do any sort of manipulation on this Remote Shopping Cart via AWS, we&apos;ll need to remember and later supply both the <code>CartId</code> and <code>HMAC</code> found in the XML returned at its creation.  And, once we&apos;re all ready to check out, the <code>PurchaseUrl</code> points to where we&apos;ll need to browse in person.</p>\n<h3 id=\"stay-tuned\">Stay Tuned!</h3>\n<p>This concludes Part 2 of the Wish-of-the-Month Club.  Following this will be the final part, where we tie everything together and start firing off monthly emails!</p>\n<!-- links -->\n\n",
    "prevPostPath": "2004/06/28/radioiorock-scraper",
    "nextPostPath": "2004/06/16/wishofthemonthclub1"
  },
  {
    "comments_archived": true,
    "date": "2004-06-16T11:42:48.000Z",
    "excerpt": "For some time now, my girlfriend and I have been accumulating things we want in wishlists on Amazon.com.  Though they have come in handy with relatives at Christmas and on birthdays, neither of us really expects to see a regular flow of gifts from them.  For the most part, they've just become holding tanks for things we intend to buy for each other or ourselves.  On one particular visit, though, the notion of a Wish-of-the-Month club popped into my head.",
    "layout": "post",
    "tags": [
      "hacks",
      "xml"
    ],
    "title": "Wish-of-the-Month Club, Part 1 of 3",
    "wordpress_id": 529,
    "wordpress_slug": "wishofthemonthclub1",
    "wordpress_url": "http://www.decafbad.com/blog/?p=529",
    "year": "2004",
    "month": "06",
    "day": "16",
    "isDir": false,
    "slug": "wishofthemonthclub1",
    "postName": "2004-06-16-wishofthemonthclub1",
    "html": "<p><i>Remember that <a href=\"http://www.decafbad.com/blog/2004/05/25/i_was_a_preteen_transactor_author_wannabe_and_still_am\">I wrote a little while ago</a> about wanting to publish some articles here that I&#39;d want to read?  Well, I&#39;ve been hard at work since then to turn out the first set and I think I&#39;ve finally got something for you.  I <a href=\"http://www.decafbad.com/blog/2004/06/13/i_will_do_the_fandango\">mentioned</a> earlier this week that I was taking this seriously, so I hope it shows.  So, with many thanks to <a href=\"http://missadroit.livejournal.com\" title=\"Miss Adroit, my favorite girl in the world\">my girlfriend&#39;s</a> kind editorial help, and with some measure of anxiety, here goes...</i></p>\n<h3 id=\"introduction\">Introduction</h3>\n<p>For some time now, my girlfriend and I have been accumulating things we want in wishlists on Amazon.com.  <a href=\"http://www.amazon.com/exec/obidos/registry/1QWYI6P2JF3Q5\">Here&#39;s mine</a> and <a href=\"http://www.amazon.com/exec/obidos/registry/35OIOYWQ9XQAE\">here&#39;s hers</a> - if you visit them, you can see we&#39;ve both got quite a few things listed.  Though they have come in handy with relatives at Christmas and on birthdays, neither of us really expects to see a regular flow of gifts from them.  For the most part, they&#39;ve just become holding tanks for things we intend to buy for each other or ourselves.  </p>\n<p>However, I tend to forget we have these lists except for occasional visit to Amazon when I think, &quot;Oh yeah, wishlists.  I should pick up a thing or two, there&#39;s some good stuff piled up in them.&quot;  On one particular visit, though, the notion of a Wish-of-the-Month club popped into my head: We could afford to grab at least one item for each of us from our wishlists on a monthly basis, provided that we remembered to place an order.  It&#39;d be better than signing up for a book or music club, driven by someone else&#39;s idea of what we wanted.  Unfortunately, there&#39;s that problem for busy, absentminded, and people like us: remembering to place an order.</p>\n<p>But wait, isn&#39;t this the sort of thing computers are for?  I should be able to cobble something together that would peruse our wishlists and--given some criteria like a price maximum--select an item at random for each of us and send them on their way.  With this, I could schedule a monthly run and start whittling down those lists.</p>\n<h3 id=\"gathering-tools\">Gathering Tools</h3>\n<p>Before I start working through the project itself, let&#39;s establish some assumptions and then gather some tools and materials:</p>\n<p>I&#39;m going to assume that you&#39;re using a UN*X operating system (ie. Linux, Mac OS X, etc.) and that you&#39;re reasonably familiar with getting around in a shell and editing files.  Things presented here could be adapted for Windows fairly easily, but I&#39;ll leave that as an exercise to the reader.  Also, you may need to build and install a package or two, so know-how in that regard will serve as well.  And finally: some familiarity with XML and XSLT would be useful, but you won&#39;t need to be a guru with either.</p>\n<p>Oh, and all the files I&#39;ll be introducing in this project can be downloaded from my website as a tarball:  <a href=\"http://www.decafbad.com/cvs/hacks/wishes/wishes.tar.gz?tarball=1\" title=\"All Wish-of-the-Month Club files wrapped up in a tarball\"><code>wishes.tar.gz</code></a>.  If you feel like browsing, you can see these files in my <a href=\"http://www.decafbad.com/cvs/hacks/wishes/\">CVS repository</a>.  And if you feel like checking out a copy via anonymous CVS, the username is <code>anoncvs</code> and the password is blank--email me for help, if you need it.</p>\n<p>So, how do we get a look at these wishlists?  Lately, I&#39;ve been tinkering a bit with <a href=\"http://www.decafbad.com/twiki/bin/view/Main/XslScraper\" title=\"Scrape RSS and Atom from HTML using Tidy and XSLT\">scraping information from</a> and <a href=\"http://www.amazon.com/exec/obidos/ASIN/0596005776/0xdecafbad-20\" title=\"O&#39;Reilly&#39;s Spidering Hacks\">automating access to</a> websites.  It&#39;s a bit like a puzzle game, with all the accompanying frustrations and happy breakthroughs.  However, where most puzzle games are designed with a solution in mind, this game isn&#39;t even necessarily meant to be played depending on the intentions of website owners.</p>\n<p>Fortunately, the folks at Amazon.com have made things very friendly to tinkerers by providing an API, called <a href=\"http://www.amazon.com/gp/aws/landing.html\" title=\"Amazon Web Services\">Amazon Web Services</a> (or AWS).  You&#39;ll want to <a href=\"http://www.amazon.com/gp/browse.html/ref=sc_fe_c_2/002-7899886-3676027?%5Fencoding=UTF8&#38;node=3434641&#38;no=3435361&#38;me=A36L942TSJ2AJA\">download</a> the AWS developer&#39;s kit, which contains a wealth of documentation and examples.  After downloading these materials, you should <a href=\"https://associates.amazon.com/exec/panama/associates/join/developer/application.html\">apply for a developer&#39;s token</a> for use with the service.  AWS provides both SOAP and REST interfaces to functionality and data at their site; personally, I prefer the HTTP-and-XML approach taken by the REST interface, so that&#39;s what we&#39;ll be using here. </p>\n<p>To handle the XML produced by AWS, we&#39;ll be using the <code>xsltproc</code> command from <a href=\"http://www.xmlsoft.org/XSLT.html\">the XML C parser and toolkit of Gnome</a>.  There are other XSLT processors--such as <a href=\"http://xml.apache.org/xalan-j/\">Xalan</a>, <a href=\"http://www.gingerall.com/charlie/ga/xml/p_sab.xml\">Sablotron</a>, and <a href=\"http://saxon.sourceforge.net/\">Saxon</a>--but I&#39;ve found <a href=\"http://www.xmlsoft.org/XSLT.html\">libxslt</a> easiest to feed and care for on the various platforms with which I tinker.  It also seems to support a very large swath of <a href=\"http://www.exslt.org/\">EXSLT extensions</a>, all of which come in very handy, yet seem to receive uneven support in other XSLT processors.  We&#39;ll be pulling a trick or two out of that bag, so its support is key.</p>\n<p>You may or may not already have <a href=\"http://www.xmlsoft.org/XSLT.html\">libsxlt</a> installed.  Depending on your variant of Linux, it might be as simple as a single package-management command or it might be a bit more complex if you need to compile from source.  For Mac OS X, I recommend using <a href=\"http://fink.sourceforge.net\">Fink</a> for your packaging needs.  Although, <a href=\"http://darwinports.opendarwin.org/\">DarwinPorts</a> is nice as well, if you&#39;re used to The BSD Way.</p>\n<p>A bonus for OS X users: Marc Liyanage has provided a great Open Source tool named <a href=\"http://www.entropy.ch:16080/software/macosx/#testxslt\">TestXSLT</a> that embeds <a href=\"http://www.xmlsoft.org/XSLT.html\">libxslt</a>, among other XSLT processors, in a slick GUI for easier use.  This might come in handy for you as things develop.</p>\n<h3 id=\"wishlists-in-xml\">Wishlists in XML</h3>\n<p>Okay, we&#39;ve got a working environment, a head start on accessing Amazon wishlists as XML, and a way to manipulate that XML using <code>xsltproc</code>.  Let&#39;s start playing.  First things first, we need to gain access to Amazon wishlists in XML form.  Reading through the <a href=\"http://www.amazon.com/gp/browse.html/ref=sc_fe_c_2/002-7899886-3676027?%5Fencoding=UTF8&#38;node=3434641&#38;no=3435361&#38;me=A36L942TSJ2AJA\">AWS documentation</a> reveals that wish list searches are available via a URL constructed like so:</p>\n<pre><code>http://xml.amazon.com/onca/xml3?\nt=[Associates ID goes here]&amp;#38;\ndev-t=[Developer Token goes here]&amp;#38;\nWishlistSearch=[wishlist ID goes here]&amp;#38;\ntype=[lite or heavy]&amp;#38;\nf=xml</code></pre>\n<p>I received an ID of <code>0xdecafbad-20</code> when I <a href=\"http://associates.amazon.com\">signed up to be an associate</a> a few years ago.  This will ensure that I get credited for sales made via the API--which isn&#39;t as important for the present project, since I&#39;ll be buying items myself, but it&#39;ll come in handy in later projects.  Also, when I <a href=\"https://associates.amazon.com/exec/panama/associates/join/developer/application.html\">signed up for a developer&#39;s token</a>, this is what I was given: <code>D8HVH869XA0NP</code>  I&#39;m disclosing my own here for the sake of example, but you should <a href=\"https://associates.amazon.com/exec/panama/associates/join/developer/application.html\">sign up</a> and get your own.</p>\n<p>So, that fills in the first two parts of the URL.  For the purposes of this project, let&#39;s just go with the <code>lite</code> option for type.  As for the wishlist ID, let&#39;s take a look the wishlist URLs to which I linked earlier:</p>\n<pre><code>http://www.amazon.com/exec/obidos/registry/35OIOYWQ9XQAE\nhttp://www.amazon.com/exec/obidos/registry/1QWYI6P2JF3Q5</code></pre>\n<p>You can discover these wishlist URLs using <a href=\"http://www.amazon.com/gp/registry/search.html/002-7899886-3676027?%5Fencoding=UTF8&#38;type=wishlist\">Amazon&#39;s Wish List Search</a> feature, in which case a wishlist URL might appear like so:</p>\n<pre><code>http://www.amazon.com/gp/registry/registry.html/\n002-7899886-3676027?%5Fencoding=UTF8&amp;#38;\nid=35OIOYWQ9XQAE</code></pre>\n<p>In either case, there is a 13-character ID in each variety of wish list URL: this string is the wish list ID.  So, the ID for my girlfriend&#39;s wishlist is <code> 35OIOYWQ9XQAE</code> and mine is <code>1QWYI6P2JF3Q5</code>.  Given this piece of the puzzle, we can fill in the blanks to come up with the following URL for my girlfriend&#39;s wish list:</p>\n<pre><code>http://xml.amazon.com/onca/xml3?\nt=0xdecafbad-20&amp;#38;\ndev-t=D8HVH869XA0NP&amp;#38;\ntype=lite&amp;#38;\nWishlistSearch=35OIOYWQ9XQAE&amp;#38;\nf=xml</code></pre>\n<p><a href=\"http://xml.amazon.com/onca/xml3?t=0xdecafbad-20&#38;dev-t=D8HVH869XA0NP&#38;type=lite&#38;WishlistSearch=35OIOYWQ9XQAE&#38;f=xml\">Check out the XML resulting from this URL</a>--you may want to use a tool such as <code>curl</code> or <code>wget</code> instead of viewing this directly in your browser.  You&#39;ll see some XML that looks something like this:</p>\n<pre><code>&lt;ProductInfo&gt;\n...\n&lt;Details url=&quot;(some long URL)&quot;&gt;\n  &lt;Asin&gt;0262133601&lt;/Asin&gt;\n  &lt;ProductName&gt;Foundations of Statistical Natural Language Processing&lt;/ProductName&gt;\n  &lt;Catalog&gt;Book&lt;/Catalog&gt;\n  &lt;Authors&gt;\n     &lt;Author&gt;Christopher D. Manning&lt;/Author&gt;\n     &lt;Author&gt;Hinrich Sch&amp;#252;tze&lt;/Author&gt;\n  &lt;/Authors&gt;\n  &lt;ReleaseDate&gt;18 June, 1999&lt;/ReleaseDate&gt;\n  &lt;Manufacturer&gt;MIT Press&lt;/Manufacturer&gt;\n  &lt;ImageUrlSmall&gt;(another long url)&lt;/ImageUrlSmall&gt;\n  &lt;ImageUrlMedium&gt;(yet another long url)&lt;/ImageUrlMedium&gt;\n  &lt;ImageUrlLarge&gt;(one last long url)&lt;/ImageUrlLarge&gt;\n  &lt;Availability&gt;Usually ships within 24 hours&lt;/Availability&gt;\n  &lt;ListPrice&gt;$75.00&lt;/ListPrice&gt;\n  &lt;OurPrice&gt;$63.75&lt;/OurPrice&gt;\n  &lt;UsedPrice&gt;$49.99&lt;/UsedPrice&gt;\n&lt;/Details&gt;\n...\n&lt;/ProductInfo&gt;</code></pre>\n<p>Note that the <a href=\"http://www.amazon.com/exec/obidos/ASIN/0262133601/0xdecafbad-20?dev-t=D8HVH869XA0NP%26camp=2025%26link_code=xm2\">long URL</a> in the <code>Detail</code> element&#39;s <code>url</code> attribute links to the human-viewable product detail page at Amazon.  I&#39;ve also left a few other things out, such as the URLs to product images; I just thought I&#39;d edit it a bit to be friendlier to your browser at home.  There&#39;s a <a href=\"http://xml.amazon.com/schemas3/dev-lite.xsd\">schema</a> for this XML data, and the ins-and-outs are explained in the AWS documentation under &quot;Amazon Web Services Data Model&quot;.</p>\n<h3 id=\"querying-the-wishes\">Querying The Wishes</h3>\n<p>Some ready-made files are available for this section:</p>\n<ul>\n<li><a href=\"http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes-ex1.xsl\"><code>wishes-ex1.xsl</code></a>: The first iteration of the stylesheet in development.</li>\n<li><a href=\"http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes.xml\"><code>wishes.xml</code></a>: An XML document used as input with the stylesheet.</li>\n</ul>\n<p>Now that we&#39;ve got some XML from Amazon to play with, let&#39;s start tinkering with an XSLT stylesheet to process it.  In the interests of flexibility and reusability, we can parameterize a few things in XML before starting in on the stylesheet:</p>\n<pre><code>&lt;wishes xmlns=&quot;http://www.decafbad.com/2004/05/wishes&quot;&gt;\n  &lt;maxprice&gt;15.00&lt;/maxprice&gt;\n  &lt;associate&gt;0xdecafbad-20&lt;/associate&gt;\n  &lt;devtoken&gt;D8HVH869XA0NP&lt;/devtoken&gt;\n  &lt;email&gt;deus_x@pobox.com&lt;/email&gt;\n  &lt;wishlists&gt;\n    &lt;wishlist label=&quot;The Girl&quot;&gt;35OIOYWQ9XQAE&lt;/wishlist&gt;\n    &lt;wishlist label=&quot;Me&quot;&gt;1QWYI6P2JF3Q5&lt;/wishlist&gt;\n  &lt;/wishlists&gt;\n&lt;/wishes&gt;</code></pre>\n<p>Hopefully, the data here is fairly self-explanatory:  I&#39;ve established a maximum price for item selection; provided my associate ID and developer token; there&#39;s an email address to which I eventually want to send the results of all this work; and I&#39;ve made a list of wishlist IDs, each with a readable label. Given this, let&#39;s start out simple and  use this to get some data from Amazon:</p>\n<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;\n&lt;xsl:stylesheet version=&quot;1.0&quot;\n            xmlns:wishes=&quot;http://www.decafbad.com/2004/05/wishes&quot;\n            xmlns:xsl=&quot;http://www.w3.org/1999/XSL/Transform&quot;&gt;\n  &lt;xsl:output indent=&quot;yes&quot; /&gt;\n\n  &lt;!-- Grab our global settings --&gt;\n  &lt;xsl:variable name=&quot;maxprice&quot;  select=&quot;/wishes:wishes/wishes:maxprice&quot; /&gt;  \n  &lt;xsl:variable name=&quot;associate&quot; select=&quot;/wishes:wishes/wishes:associate&quot; /&gt;\n  &lt;xsl:variable name=&quot;devtoken&quot;  select=&quot;/wishes:wishes/wishes:devtoken&quot; /&gt;</code></pre>\n<p>So far so good--things start off by pulling in some of the parameters into variables.  Next, let&#39;s dig into actually querying wishlist data with a reusable template:</p>\n<pre><code>  &lt;xsl:template name=&quot;processWishlist&quot;&gt;\n    &lt;xsl:param name=&quot;wishlist&quot; /&gt;\n\n    &lt;xsl:variable name=&quot;details&quot; select=&quot;document(concat(\n        &#39;http://xml.amazon.com/onca/xml3?&#39;,\n        &#39;t=&#39;,$associate,&#39;&amp;amp;&#39;,\n        &#39;dev-t=&#39;,$devtoken,&#39;&amp;amp;&#39;,\n        &#39;WishlistSearch=&#39;,$wishlist,&#39;&amp;amp;&#39;,\n        &#39;type=lite&amp;amp;f=xml&#39;))//Details&quot; /&gt;</code></pre>\n<p>First thing into this template, we accept a parameter named <code>wishlist</code> which is expected to contain a wishlist ID string.  Next, we build an AWS URL by concatenating together the pieces we have in variables (associate ID, developer&#39;s token, and wishlist ID) using the XPath function <a href=\"http://www.w3.org/TR/2002/WD-xquery-operators-20020816/#func-concat\"><code>concat()</code></a>.  Once we have this URL, we use the function <a href=\"http://www.w3.org/TR/2002/WD-xquery-operators-20020816/#func-document\"><code>document()</code></a> to make a request and fetch the XML data for that URL.  From this, we select all the <code>Details</code> elements.  </p>\n<p>Then with that data, we can do some filtering on the price and availability.  We want to make sure that not only will we select items that are within our budget, but that they are available to buy in the first place:</p>\n<pre><code>    &lt;xsl:copy-of select=&quot;$details[\n      number(substring(OurPrice/text(),2)) &amp;lt; $maxprice\n      and\n      contains(Availability, &#39;Usually ships within&#39;)\n      ]&quot; /&gt;\n\n  &lt;/xsl:template&gt;</code></pre>\n<p>This code is just a little bit funky, since the price data given by Amazon contains a dollar sign, and we want to make a numerical comparison.  So, we chop the dollar sign off and convert to a number before making the comparison.  Also, there&#39;s an assumption here about what will show up in the <code>Availability</code> element: &quot;Usually ships within&quot;  Other things that might show up will declare that the item is out of stock, discontinued, or otherwise not shipping.  This might need some tweaking someday, but it seems to work for now.</p>\n<p>Taken all together, this template has the effect of a SQL SELECT statement somewhat like this:</p>\n<pre><code>SELECT * \nFROM Amazon.WishlistItems \nWHERE WishlistID = $wishlist AND \n      OurPrice &lt; $maxprice AND\n      Availability like &#39;%Usually ships within%&#39;;</code></pre>\n<p><code>document()</code> is a very useful XPath function.  It allows us to pull in XML from external files and, in our case, from external URLs via HTTP requests.  This gives us the ability to make queries against REST web services like AWS--which, among many other reasons, is why I prefer REST web services over SOAP.  (I don&#39;t even want to think about trying to access a SOAP service from XSLT.)</p>\n<p>Now, let&#39;s wrap up this first iteration of the stylesheet by trying out the query template on each of the wishlist IDs:</p>\n<pre><code>  &lt;xsl:template match=&quot;/wishes:wishes&quot;&gt;\n    &lt;xsl:for-each select=&quot;//wishes:wishlist&quot;&gt;\n      &lt;wishes:wishitem&gt;\n        &lt;xsl:copy-of select=&quot;.&quot; /&gt;\n        &lt;xsl:call-template name=&quot;processWishlist&quot;&gt;\n              &lt;xsl:with-param name=&quot;wishlist&quot; \n                              select=&quot;.&quot; /&gt;\n        &lt;/xsl:call-template&gt;\n      &lt;/wishes:wishitem&gt;\n    &lt;/xsl:for-each&gt;\n  &lt;/xsl:template&gt;\n\n&lt;/xsl:stylesheet&gt;</code></pre>\n<p>You can get a <a href=\"http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes-ex1.xsl\">completed version of this stylesheet</a>, along with <a href=\"http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes.xml\">the input XML</a>, in case you haven&#39;t been cutting and pasting together a copy of your own along the way.  Try it out in a shell with:</p>\n<pre><code>$ xsltproc wishes_ex1.xsl wishes.xml</code></pre>\n<p>Alternately, you could check it out using <a href=\"http://www.entropy.ch:16080/software/macosx/#testxslt\">TestXSLT</a> under OS X.  You should get something like the following:</p>\n<pre><code>&lt;wishes:wishitem xmlns:wishes=&quot;http://www.decafbad.com/2004/05/wishes&quot;&gt;\n    &lt;wishes:wishlist label=&quot;The Girl&quot;&gt;35OIOYWQ9XQAE&lt;/wishes:wishlist&gt;\n    &lt;Details ...&gt;...&lt;/Details&gt;\n    &lt;Details ...&gt;...&lt;/Details&gt;\n    ...\n&lt;/wishes:wishitem&gt;\n&lt;wishes:wishitem xmlns:wishes=&quot;http://www.decafbad.com/2004/05/wishes&quot;&gt;\n    &lt;wishes:wishlist label=&quot;Me&quot;&gt;1QWYI6P2JF3Q5&lt;/wishes:wishlist&gt;\n    &lt;Details ...&gt;...&lt;/Details&gt;\n    &lt;Details ...&gt;...&lt;/Details&gt;\n    ...\n&lt;/wishes:wishitem&gt;</code></pre>\n<p>Obviously, this example XML is much abridged, but hopefully you can get the gist:  For each wishlist ID, there is a containing <code>wishitem</code> element.  It contains a copy of the <code>wishlist</code> element from the input XML, followed by all the <code>Details</code> elements filtered and copied from the Amazon XML with the help of the <code>processWishlist</code> template.</p>\n<h3 id=\"thats-all-for-now\">That&#39;s All for Now!</h3>\n<p>And that&#39;s the end of Part 1.  Next up, we&#39;ll be delving into a few more wrinkles in the wishlist querying process, selecting random items in XSLT, and the Remote Shopping Cart interface in Amazon Web Services.  Stay tuned!</p>\n<!-- links -->\n\n<!--more-->\n<p>shortname=wishofthemonthclub1</p>\n<div id=\"comments\" class=\"comments archived-comments\"><h3>Archived Comments</h3>\n<ul class=\"comments\">\n<li class=\"comment\" id=\"comment-221082740\">\n<div class=\"meta\">\n<div class=\"author\">\n<a class=\"avatar image\" rel=\"nofollow\" \nhref=\"http://inflatus.net\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=2c0ee9a9038c85c0510a7a5fd3f030ab&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\n<a class=\"avatar name\" rel=\"nofollow\" \nhref=\"http://inflatus.net\">poepping</a>\n</div>\n<a href=\"#comment-221082740\" class=\"permalink\"><time datetime=\"2004-06-16T17:24:01\">2004-06-16T17:24:01</time></a>\n</div>\n<div class=\"content\">I was using the amazon wish list api a few months ago, and back then the wish list was out of date, and was missing the newest week or two of stuff.  You might want to check this if this is important to you.\nIn the future, you could add the feature of ordering the stuff that has a higher priority on it. :)\nCool project though. I think i'm going to go back through my old code and try it again to see if they fixed the delay.\nbtw, I miss your links of the day.</div>\n</li>\n<li class=\"comment\" id=\"comment-221082741\">\n<div class=\"meta\">\n<div class=\"author\">\n<a class=\"avatar image\" rel=\"nofollow\" \nhref=\"http://www.mpwilson.com/uccu/\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=99b77c34a0e26fd04a058f8c2dbab290&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\n<a class=\"avatar name\" rel=\"nofollow\" \nhref=\"http://www.mpwilson.com/uccu/\">Mad William Flint</a>\n</div>\n<a href=\"#comment-221082741\" class=\"permalink\"><time datetime=\"2004-06-16T19:40:55\">2004-06-16T19:40:55</time></a>\n</div>\n<div class=\"content\">very nice.  whipping out my emacs now...</div>\n</li>\n</ul>\n</div>\n",
    "body": "<i>Remember that [I wrote a little while ago][lasttime] about wanting to publish some articles here that I'd want to read?  Well, I've been hard at work since then to turn out the first set and I think I've finally got something for you.  I [mentioned][lasttime2] earlier this week that I was taking this seriously, so I hope it shows.  So, with many thanks to [my girlfriend's][missadroit] kind editorial help, and with some measure of anxiety, here goes...</i>\r\n\r\n### Introduction\r\n\r\nFor some time now, my girlfriend and I have been accumulating things we want in wishlists on Amazon.com.  [Here's mine][mywishlist] and [here's hers][herwishlist] - if you visit them, you can see we've both got quite a few things listed.  Though they have come in handy with relatives at Christmas and on birthdays, neither of us really expects to see a regular flow of gifts from them.  For the most part, they've just become holding tanks for things we intend to buy for each other or ourselves.  \r\n\r\nHowever, I tend to forget we have these lists except for occasional visit to Amazon when I think, \"Oh yeah, wishlists.  I should pick up a thing or two, there's some good stuff piled up in them.\"  On one particular visit, though, the notion of a Wish-of-the-Month club popped into my head: We could afford to grab at least one item for each of us from our wishlists on a monthly basis, provided that we remembered to place an order.  It'd be better than signing up for a book or music club, driven by someone else's idea of what we wanted.  Unfortunately, there's that problem for busy, absentminded, and people like us: remembering to place an order.\r\n\r\nBut wait, isn't this the sort of thing computers are for?  I should be able to cobble something together that would peruse our wishlists and--given some criteria like a price maximum--select an item at random for each of us and send them on their way.  With this, I could schedule a monthly run and start whittling down those lists.\r\n\r\n### Gathering Tools\r\n\r\nBefore I start working through the project itself, let's establish some assumptions and then gather some tools and materials:\r\n\r\nI'm going to assume that you're using a UN*X operating system (ie. Linux, Mac OS X, etc.) and that you're reasonably familiar with getting around in a shell and editing files.  Things presented here could be adapted for Windows fairly easily, but I'll leave that as an exercise to the reader.  Also, you may need to build and install a package or two, so know-how in that regard will serve as well.  And finally: some familiarity with XML and XSLT would be useful, but you won't need to be a guru with either.\r\n\r\nOh, and all the files I'll be introducing in this project can be downloaded from my website as a tarball:  [`wishes.tar.gz`][wishes.tar.gz].  If you feel like browsing, you can see these files in my [CVS repository][wishescvs].  And if you feel like checking out a copy via anonymous CVS, the username is `anoncvs` and the password is blank--email me for help, if you need it.\r\n\r\nSo, how do we get a look at these wishlists?  Lately, I've been tinkering a bit with [scraping information from][xslscraper] and [automating access to][spideringhacks] websites.  It's a bit like a puzzle game, with all the accompanying frustrations and happy breakthroughs.  However, where most puzzle games are designed with a solution in mind, this game isn't even necessarily meant to be played depending on the intentions of website owners.\r\n\r\nFortunately, the folks at Amazon.com have made things very friendly to tinkerers by providing an API, called [Amazon Web Services][amazonapi] (or AWS).  You'll want to [download][awsdownload] the AWS developer's kit, which contains a wealth of documentation and examples.  After downloading these materials, you should [apply for a developer's token][awstoken] for use with the service.  AWS provides both SOAP and REST interfaces to functionality and data at their site; personally, I prefer the HTTP-and-XML approach taken by the REST interface, so that's what we'll be using here. \r\n\r\nTo handle the XML produced by AWS, we'll be using the `xsltproc` command from [the XML C parser and toolkit of Gnome][libxslt].  There are other XSLT processors--such as [Xalan][xalan], [Sablotron][sablotron], and [Saxon][saxon]--but I've found [libxslt][libxslt] easiest to feed and care for on the various platforms with which I tinker.  It also seems to support a very large swath of [EXSLT extensions][exslt], all of which come in very handy, yet seem to receive uneven support in other XSLT processors.  We'll be pulling a trick or two out of that bag, so its support is key.\r\n\r\nYou may or may not already have [libsxlt][libxslt] installed.  Depending on your variant of Linux, it might be as simple as a single package-management command or it might be a bit more complex if you need to compile from source.  For Mac OS X, I recommend using [Fink][fink] for your packaging needs.  Although, [DarwinPorts][darwinports] is nice as well, if you're used to The BSD Way.\r\n\r\nA bonus for OS X users: Marc Liyanage has provided a great Open Source tool named [TestXSLT][testxslt] that embeds [libxslt][libxslt], among other XSLT processors, in a slick GUI for easier use.  This might come in handy for you as things develop.\r\n\r\n### Wishlists in XML\r\n\r\nOkay, we've got a working environment, a head start on accessing Amazon wishlists as XML, and a way to manipulate that XML using `xsltproc`.  Let's start playing.  First things first, we need to gain access to Amazon wishlists in XML form.  Reading through the [AWS documentation][awsdownload] reveals that wish list searches are available via a URL constructed like so:\r\n\r\n    http://xml.amazon.com/onca/xml3?\r\n    t=[Associates ID goes here]&#38;\r\n    dev-t=[Developer Token goes here]&#38;\r\n    WishlistSearch=[wishlist ID goes here]&#38;\r\n    type=[lite or heavy]&#38;\r\n    f=xml\r\n\r\nI received an ID of `0xdecafbad-20` when I [signed up to be an associate][amazonassociate] a few years ago.  This will ensure that I get credited for sales made via the API--which isn't as important for the present project, since I'll be buying items myself, but it'll come in handy in later projects.  Also, when I [signed up for a developer's token][awstoken], this is what I was given: `D8HVH869XA0NP`  I'm disclosing my own here for the sake of example, but you should [sign up][awstoken] and get your own.\r\n\r\nSo, that fills in the first two parts of the URL.  For the purposes of this project, let's just go with the `lite` option for type.  As for the wishlist ID, let's take a look the wishlist URLs to which I linked earlier:\r\n\r\n    http://www.amazon.com/exec/obidos/registry/35OIOYWQ9XQAE\r\n    http://www.amazon.com/exec/obidos/registry/1QWYI6P2JF3Q5\r\n\r\nYou can discover these wishlist URLs using [Amazon's Wish List Search][wlsearch] feature, in which case a wishlist URL might appear like so:\r\n\r\n    http://www.amazon.com/gp/registry/registry.html/\r\n    002-7899886-3676027?%5Fencoding=UTF8&#38;\r\n    id=35OIOYWQ9XQAE\r\n\r\nIn either case, there is a 13-character ID in each variety of wish list URL: this string is the wish list ID.  So, the ID for my girlfriend's wishlist is ` 35OIOYWQ9XQAE` and mine is `1QWYI6P2JF3Q5`.  Given this piece of the puzzle, we can fill in the blanks to come up with the following URL for my girlfriend's wish list:\r\n\r\n    http://xml.amazon.com/onca/xml3?\r\n    t=0xdecafbad-20&#38;\r\n    dev-t=D8HVH869XA0NP&#38;\r\n    type=lite&#38;\r\n    WishlistSearch=35OIOYWQ9XQAE&#38;\r\n    f=xml\r\n\r\n[Check out the XML resulting from this URL][wlurl]--you may want to use a tool such as `curl` or `wget` instead of viewing this directly in your browser.  You'll see some XML that looks something like this:\r\n\r\n    <ProductInfo>\r\n    ...\r\n    <Details url=\"(some long URL)\">\r\n      <Asin>0262133601</Asin>\r\n      <ProductName>Foundations of Statistical Natural Language Processing</ProductName>\r\n      <Catalog>Book</Catalog>\r\n      <Authors>\r\n         <Author>Christopher D. Manning</Author>\r\n         <Author>Hinrich Sch&#252;tze</Author>\r\n      </Authors>\r\n      <ReleaseDate>18 June, 1999</ReleaseDate>\r\n      <Manufacturer>MIT Press</Manufacturer>\r\n      <ImageUrlSmall>(another long url)</ImageUrlSmall>\r\n      <ImageUrlMedium>(yet another long url)</ImageUrlMedium>\r\n      <ImageUrlLarge>(one last long url)</ImageUrlLarge>\r\n      <Availability>Usually ships within 24 hours</Availability>\r\n      <ListPrice>$75.00</ListPrice>\r\n      <OurPrice>$63.75</OurPrice>\r\n      <UsedPrice>$49.99</UsedPrice>\r\n    </Details>\r\n    ...\r\n    </ProductInfo>\r\n\r\nNote that the [long URL][detailsurl] in the `Detail` element's `url` attribute links to the human-viewable product detail page at Amazon.  I've also left a few other things out, such as the URLs to product images; I just thought I'd edit it a bit to be friendlier to your browser at home.  There's a [schema][awslite] for this XML data, and the ins-and-outs are explained in the AWS documentation under \"Amazon Web Services Data Model\".\r\n\r\n### Querying The Wishes\r\n\r\nSome ready-made files are available for this section:\r\n* [`wishes-ex1.xsl`][wishes-ex1.xsl]: The first iteration of the stylesheet in development.\r\n* [`wishes.xml`][wishes.xml]: An XML document used as input with the stylesheet.\r\n\r\nNow that we've got some XML from Amazon to play with, let's start tinkering with an XSLT stylesheet to process it.  In the interests of flexibility and reusability, we can parameterize a few things in XML before starting in on the stylesheet:\r\n\r\n    <wishes xmlns=\"http://www.decafbad.com/2004/05/wishes\">\r\n      <maxprice>15.00</maxprice>\r\n      <associate>0xdecafbad-20</associate>\r\n      <devtoken>D8HVH869XA0NP</devtoken>\r\n      <email>deus_x@pobox.com</email>\r\n      <wishlists>\r\n        <wishlist label=\"The Girl\">35OIOYWQ9XQAE</wishlist>\r\n        <wishlist label=\"Me\">1QWYI6P2JF3Q5</wishlist>\r\n      </wishlists>\r\n    </wishes>\r\n\r\nHopefully, the data here is fairly self-explanatory:  I've established a maximum price for item selection; provided my associate ID and developer token; there's an email address to which I eventually want to send the results of all this work; and I've made a list of wishlist IDs, each with a readable label. Given this, let's start out simple and  use this to get some data from Amazon:\r\n\r\n    <?xml version=\"1.0\"?>\r\n    <xsl:stylesheet version=\"1.0\"\r\n                xmlns:wishes=\"http://www.decafbad.com/2004/05/wishes\"\r\n                xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\">\r\n      <xsl:output indent=\"yes\" />\r\n\r\n      <!-- Grab our global settings -->\r\n      <xsl:variable name=\"maxprice\"  select=\"/wishes:wishes/wishes:maxprice\" />  \r\n      <xsl:variable name=\"associate\" select=\"/wishes:wishes/wishes:associate\" />\r\n      <xsl:variable name=\"devtoken\"  select=\"/wishes:wishes/wishes:devtoken\" />\r\n\r\nSo far so good--things start off by pulling in some of the parameters into variables.  Next, let's dig into actually querying wishlist data with a reusable template:\r\n\r\n      <xsl:template name=\"processWishlist\">\r\n        <xsl:param name=\"wishlist\" />\r\n\r\n        <xsl:variable name=\"details\" select=\"document(concat(\r\n            'http://xml.amazon.com/onca/xml3?',\r\n            't=',$associate,'&amp;',\r\n            'dev-t=',$devtoken,'&amp;',\r\n            'WishlistSearch=',$wishlist,'&amp;',\r\n            'type=lite&amp;f=xml'))//Details\" />\r\n\r\nFirst thing into this template, we accept a parameter named `wishlist` which is expected to contain a wishlist ID string.  Next, we build an AWS URL by concatenating together the pieces we have in variables (associate ID, developer's token, and wishlist ID) using the XPath function [`concat()`][xpconcat].  Once we have this URL, we use the function [`document()`][xpdocument] to make a request and fetch the XML data for that URL.  From this, we select all the `Details` elements.  \r\n\r\nThen with that data, we can do some filtering on the price and availability.  We want to make sure that not only will we select items that are within our budget, but that they are available to buy in the first place:\r\n\r\n        <xsl:copy-of select=\"$details[\r\n          number(substring(OurPrice/text(),2)) &lt; $maxprice\r\n          and\r\n          contains(Availability, 'Usually ships within')\r\n          ]\" />\r\n\r\n      </xsl:template>\r\n\r\nThis code is just a little bit funky, since the price data given by Amazon contains a dollar sign, and we want to make a numerical comparison.  So, we chop the dollar sign off and convert to a number before making the comparison.  Also, there's an assumption here about what will show up in the `Availability` element: \"Usually ships within\"  Other things that might show up will declare that the item is out of stock, discontinued, or otherwise not shipping.  This might need some tweaking someday, but it seems to work for now.\r\n\r\nTaken all together, this template has the effect of a SQL SELECT statement somewhat like this:\r\n\r\n    SELECT * \r\n    FROM Amazon.WishlistItems \r\n    WHERE WishlistID = $wishlist AND \r\n          OurPrice < $maxprice AND\r\n          Availability like '%Usually ships within%';\r\n\r\n`document()` is a very useful XPath function.  It allows us to pull in XML from external files and, in our case, from external URLs via HTTP requests.  This gives us the ability to make queries against REST web services like AWS--which, among many other reasons, is why I prefer REST web services over SOAP.  (I don't even want to think about trying to access a SOAP service from XSLT.)\r\n\r\nNow, let's wrap up this first iteration of the stylesheet by trying out the query template on each of the wishlist IDs:\r\n\r\n      <xsl:template match=\"/wishes:wishes\">\r\n        <xsl:for-each select=\"//wishes:wishlist\">\r\n          <wishes:wishitem>\r\n            <xsl:copy-of select=\".\" />\r\n            <xsl:call-template name=\"processWishlist\">\r\n                  <xsl:with-param name=\"wishlist\" \r\n                                  select=\".\" />\r\n            </xsl:call-template>\r\n          </wishes:wishitem>\r\n        </xsl:for-each>\r\n      </xsl:template>\r\n  \r\n    </xsl:stylesheet>\r\n\r\nYou can get a [completed version of this stylesheet][wishes-ex1.xsl], along with [the input XML][wishes.xml], in case you haven't been cutting and pasting together a copy of your own along the way.  Try it out in a shell with:\r\n\r\n    $ xsltproc wishes_ex1.xsl wishes.xml\r\n\r\nAlternately, you could check it out using [TestXSLT][testxslt] under OS X.  You should get something like the following:\r\n\r\n    <wishes:wishitem xmlns:wishes=\"http://www.decafbad.com/2004/05/wishes\">\r\n        <wishes:wishlist label=\"The Girl\">35OIOYWQ9XQAE</wishes:wishlist>\r\n        <Details ...>...</Details>\r\n        <Details ...>...</Details>\r\n        ...\r\n    </wishes:wishitem>\r\n    <wishes:wishitem xmlns:wishes=\"http://www.decafbad.com/2004/05/wishes\">\r\n        <wishes:wishlist label=\"Me\">1QWYI6P2JF3Q5</wishes:wishlist>\r\n        <Details ...>...</Details>\r\n        <Details ...>...</Details>\r\n        ...\r\n    </wishes:wishitem>\r\n\r\nObviously, this example XML is much abridged, but hopefully you can get the gist:  For each wishlist ID, there is a containing `wishitem` element.  It contains a copy of the `wishlist` element from the input XML, followed by all the `Details` elements filtered and copied from the Amazon XML with the help of the `processWishlist` template.\r\n\r\n### That's All for Now!\r\n\r\nAnd that's the end of Part 1.  Next up, we'll be delving into a few more wrinkles in the wishlist querying process, selecting random items in XSLT, and the Remote Shopping Cart interface in Amazon Web Services.  Stay tuned!\r\n\r\n<!-- links -->\r\n\r\n[missadroit]: http://missadroit.livejournal.com \"Miss Adroit, my favorite girl in the world\"\r\n[mywishlist]: http://www.amazon.com/exec/obidos/registry/1QWYI6P2JF3Q5 \r\n[herwishlist]: http://www.amazon.com/exec/obidos/registry/35OIOYWQ9XQAE \r\n[amazonapi]: http://www.amazon.com/gp/aws/landing.html \"Amazon Web Services\"\r\n[libxml]: http://www.xmlsoft.org/\r\n[xalan]: http://xml.apache.org/xalan-j/\r\n[sablotron]: http://www.gingerall.com/charlie/ga/xml/p_sab.xml\r\n[saxon]: http://saxon.sourceforge.net/\r\n[exslt]: http://www.exslt.org/\r\n[libxslt]: http://www.xmlsoft.org/XSLT.html\r\n[spideringhacks]: http://www.amazon.com/exec/obidos/ASIN/0596005776/0xdecafbad-20 \"O'Reilly's Spidering Hacks\"\r\n[xslscraper]: http://www.decafbad.com/twiki/bin/view/Main/XslScraper \"Scrape RSS and Atom from HTML using Tidy and XSLT\"\r\n[awsdownload]: http://www.amazon.com/gp/browse.html/ref=sc_fe_c_2/002-7899886-3676027?%5Fencoding=UTF8&#38;node=3434641&#38;no=3435361&#38;me=A36L942TSJ2AJA\r\n[awstoken]: https://associates.amazon.com/exec/panama/associates/join/developer/application.html\r\n[amazonassociate]: http://associates.amazon.com\r\n[wlsearch]: http://www.amazon.com/gp/registry/search.html/002-7899886-3676027?%5Fencoding=UTF8&#38;type=wishlist\r\n[wlurl]: http://xml.amazon.com/onca/xml3?t=0xdecafbad-20&#38;dev-t=D8HVH869XA0NP&#38;type=lite&#38;WishlistSearch=35OIOYWQ9XQAE&#38;f=xml\r\n[detailsurl]: http://www.amazon.com/exec/obidos/ASIN/0262133601/0xdecafbad-20?dev-t=D8HVH869XA0NP%26camp=2025%26link_code=xm2\r\n[awslite]: http://xml.amazon.com/schemas3/dev-lite.xsd\r\n[fink]: http://fink.sourceforge.net\r\n[testxslt]: http://www.entropy.ch:16080/software/macosx/#testxslt\r\n[darwinports]: http://darwinports.opendarwin.org/\r\n[curl]: http://www.decafbad.com/#TODO\r\n[wget]: http://www.decafbad.com/#TODO\r\n[xpconcat]: http://www.w3.org/TR/2002/WD-xquery-operators-20020816/#func-concat\r\n[xpdocument]: http://www.w3.org/TR/2002/WD-xquery-operators-20020816/#func-document\r\n[wishescvs]: http://www.decafbad.com/cvs/hacks/wishes/\r\n[wishes.tar.gz]: http://www.decafbad.com/cvs/hacks/wishes/wishes.tar.gz?tarball=1 \"All Wish-of-the-Month Club files wrapped up in a tarball\"\r\n[wishes.xml]: http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes.xml\r\n[wishes-ex1.xsl]: http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes-ex1.xsl\r\n[wishes-ex2.xsl]: http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes-ex2.xsl\r\n[wishes-ex3.xsl]: http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes-ex3.xsl\r\n[wishes-ex4.xsl]: http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes-ex4.xsl\r\n[wishes-ex5.xsl]: http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes-ex5.xsl\r\n[wishes-ex6.xsl]: http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes-ex6.xsl\r\n[random-xml]: http://www.decafbad.com/cvs/*checkout*/hacks/wishes/random-xml\r\n[wishes_html_screenshot]: http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes.jpg\r\n[xslt_iteration]: http://www.dpawson.co.uk/xsl/sect2/N4806.html \"Iteration in XSLT\"\r\n[xslt_recursion]: http://www-106.ibm.com/developerworks/xml/library/x-xslrecur/ \"Use recursion effectively in XSL\"\r\n[exsl_random]: http://www.exslt.org/math/functions/random/index.html\r\n[exsl_node_set]: http://www.exslt.org/exsl/functions/node-set/index.html\r\n[rand_url]: http://www.decafbad.com/2004/05/random-xml?int=1&#38;min=10&#38;max=20 \"A random integer between 10 and 20, in XML\"\r\n[xslt_result_tree_fragment]: http://www.w3.org/TR/xslt#section-Result-Tree-Fragments\r\n\r\n[email_attach_anatomy]: http://www.dpo.uab.edu/Email/attach.html \"Anatomy of an Email Attachment\"\r\n[email_mime_and_html]: http://www.abiglime.com/webmaster/articles/cgi/010698.htm \"How to encapsulate HTML in an email message\"\r\n\r\n[email_html_and_text]: http://www.wilsonweb.com/wmt5/html-email-multi.htm \"Sending HTML and Plain Text E-Mail Simultaneously\"\r\n[man_sendmail]: http://www.hmug.org/man/8/sendmail.html \"man: sendmail\"\r\n[rfc1521]: http://www.faqs.org/rfcs/rfc1521.html \"RFC 1521\"\r\n[cron1]: http://www.lysator.liu.se/~forsberg/linux/cron.html \"Doing things periodically - Using CRON\"\r\n[cron2]: http://www.itworld.com/Comp/2378/swol-0825-unix101/ \"Using cron basics\"\r\n[python_libxml]: http://xmlsoft.org/python.html \r\n[lasttime]: http://www.decafbad.com/blog/2004/05/25/i_was_a_preteen_transactor_author_wannabe_and_still_am\r\n[lasttime2]: http://www.decafbad.com/blog/2004/06/13/i_will_do_the_fandango\r\n<!--more-->\r\nshortname=wishofthemonthclub1\r\n\r\n<div id=\"comments\" class=\"comments archived-comments\">\r\n            <h3>Archived Comments</h3>\r\n            \r\n        <ul class=\"comments\">\r\n            \r\n        <li class=\"comment\" id=\"comment-221082740\">\r\n            <div class=\"meta\">\r\n                <div class=\"author\">\r\n                    <a class=\"avatar image\" rel=\"nofollow\" \r\n                       href=\"http://inflatus.net\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=2c0ee9a9038c85c0510a7a5fd3f030ab&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\r\n                    <a class=\"avatar name\" rel=\"nofollow\" \r\n                       href=\"http://inflatus.net\">poepping</a>\r\n                </div>\r\n                <a href=\"#comment-221082740\" class=\"permalink\"><time datetime=\"2004-06-16T17:24:01\">2004-06-16T17:24:01</time></a>\r\n            </div>\r\n            <div class=\"content\">I was using the amazon wish list api a few months ago, and back then the wish list was out of date, and was missing the newest week or two of stuff.  You might want to check this if this is important to you.\r\n\r\nIn the future, you could add the feature of ordering the stuff that has a higher priority on it. :)\r\n\r\nCool project though. I think i'm going to go back through my old code and try it again to see if they fixed the delay.\r\n\r\nbtw, I miss your links of the day.</div>\r\n            \r\n        </li>\r\n    \r\n        <li class=\"comment\" id=\"comment-221082741\">\r\n            <div class=\"meta\">\r\n                <div class=\"author\">\r\n                    <a class=\"avatar image\" rel=\"nofollow\" \r\n                       href=\"http://www.mpwilson.com/uccu/\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=99b77c34a0e26fd04a058f8c2dbab290&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\r\n                    <a class=\"avatar name\" rel=\"nofollow\" \r\n                       href=\"http://www.mpwilson.com/uccu/\">Mad William Flint</a>\r\n                </div>\r\n                <a href=\"#comment-221082741\" class=\"permalink\"><time datetime=\"2004-06-16T19:40:55\">2004-06-16T19:40:55</time></a>\r\n            </div>\r\n            <div class=\"content\">very nice.  whipping out my emacs now...</div>\r\n            \r\n        </li>\r\n    \r\n        </ul>\r\n    \r\n        </div>\r\n    ",
    "parentPath": "../blog.lmorchard.com/posts/archives/2004",
    "path": "2004/06/16/wishofthemonthclub1",
    "summary": "<p><i>Remember that <a href=\"http://www.decafbad.com/blog/2004/05/25/i_was_a_preteen_transactor_author_wannabe_and_still_am\">I wrote a little while ago</a> about wanting to publish some articles here that I&apos;d want to read?  Well, I&apos;ve been hard at work since then to turn out the first set and I think I&apos;ve finally got something for you.  I <a href=\"http://www.decafbad.com/blog/2004/06/13/i_will_do_the_fandango\">mentioned</a> earlier this week that I was taking this seriously, so I hope it shows.  So, with many thanks to <a href=\"http://missadroit.livejournal.com\" title=\"Miss Adroit, my favorite girl in the world\">my girlfriend&apos;s</a> kind editorial help, and with some measure of anxiety, here goes...</i></p>\n<h3 id=\"introduction\">Introduction</h3>\n<p>For some time now, my girlfriend and I have been accumulating things we want in wishlists on Amazon.com.  <a href=\"http://www.amazon.com/exec/obidos/registry/1QWYI6P2JF3Q5\">Here&apos;s mine</a> and <a href=\"http://www.amazon.com/exec/obidos/registry/35OIOYWQ9XQAE\">here&apos;s hers</a> - if you visit them, you can see we&apos;ve both got quite a few things listed.  Though they have come in handy with relatives at Christmas and on birthdays, neither of us really expects to see a regular flow of gifts from them.  For the most part, they&apos;ve just become holding tanks for things we intend to buy for each other or ourselves.  </p>\n<p>However, I tend to forget we have these lists except for occasional visit to Amazon when I think, &quot;Oh yeah, wishlists.  I should pick up a thing or two, there&apos;s some good stuff piled up in them.&quot;  On one particular visit, though, the notion of a Wish-of-the-Month club popped into my head: We could afford to grab at least one item for each of us from our wishlists on a monthly basis, provided that we remembered to place an order.  It&apos;d be better than signing up for a book or music club, driven by someone else&apos;s idea of what we wanted.  Unfortunately, there&apos;s that problem for busy, absentminded, and people like us: remembering to place an order.</p>\n<p>But wait, isn&apos;t this the sort of thing computers are for?  I should be able to cobble something together that would peruse our wishlists and--given some criteria like a price maximum--select an item at random for each of us and send them on their way.  With this, I could schedule a monthly run and start whittling down those lists.</p>\n<h3 id=\"gathering-tools\">Gathering Tools</h3>\n<p>Before I start working through the project itself, let&apos;s establish some assumptions and then gather some tools and materials:</p>\n<p>I&apos;m going to assume that you&apos;re using a UN*X operating system (ie. Linux, Mac OS X, etc.) and that you&apos;re reasonably familiar with getting around in a shell and editing files.  Things presented here could be adapted for Windows fairly easily, but I&apos;ll leave that as an exercise to the reader.  Also, you may need to build and install a package or two, so know-how in that regard will serve as well.  And finally: some familiarity with XML and XSLT would be useful, but you won&apos;t need to be a guru with either.</p>\n<p>Oh, and all the files I&apos;ll be introducing in this project can be downloaded from my website as a tarball:  <a href=\"http://www.decafbad.com/cvs/hacks/wishes/wishes.tar.gz?tarball=1\" title=\"All Wish-of-the-Month Club files wrapped up in a tarball\"><code>wishes.tar.gz</code></a>.  If you feel like browsing, you can see these files in my <a href=\"http://www.decafbad.com/cvs/hacks/wishes/\">CVS repository</a>.  And if you feel like checking out a copy via anonymous CVS, the username is <code>anoncvs</code> and the password is blank--email me for help, if you need it.</p>\n<p>So, how do we get a look at these wishlists?  Lately, I&apos;ve been tinkering a bit with <a href=\"http://www.decafbad.com/twiki/bin/view/Main/XslScraper\" title=\"Scrape RSS and Atom from HTML using Tidy and XSLT\">scraping information from</a> and <a href=\"http://www.amazon.com/exec/obidos/ASIN/0596005776/0xdecafbad-20\" title=\"O&apos;Reilly&apos;s Spidering Hacks\">automating access to</a> websites.  It&apos;s a bit like a puzzle game, with all the accompanying frustrations and happy breakthroughs.  However, where most puzzle games are designed with a solution in mind, this game isn&apos;t even necessarily meant to be played depending on the intentions of website owners.</p>\n<p>Fortunately, the folks at Amazon.com have made things very friendly to tinkerers by providing an API, called <a href=\"http://www.amazon.com/gp/aws/landing.html\" title=\"Amazon Web Services\">Amazon Web Services</a> (or AWS).  You&apos;ll want to <a href=\"http://www.amazon.com/gp/browse.html/ref=sc_fe_c_2/002-7899886-3676027?%5Fencoding=UTF8&amp;node=3434641&amp;no=3435361&amp;me=A36L942TSJ2AJA\">download</a> the AWS developer&apos;s kit, which contains a wealth of documentation and examples.  After downloading these materials, you should <a href=\"https://associates.amazon.com/exec/panama/associates/join/developer/application.html\">apply for a developer&apos;s token</a> for use with the service.  AWS provides both SOAP and REST interfaces to functionality and data at their site; personally, I prefer the HTTP-and-XML approach taken by the REST interface, so that&apos;s what we&apos;ll be using here. </p>\n<p>To handle the XML produced by AWS, we&apos;ll be using the <code>xsltproc</code> command from <a href=\"http://www.xmlsoft.org/XSLT.html\">the XML C parser and toolkit of Gnome</a>.  There are other XSLT processors--such as <a href=\"http://xml.apache.org/xalan-j/\">Xalan</a>, <a href=\"http://www.gingerall.com/charlie/ga/xml/p_sab.xml\">Sablotron</a>, and <a href=\"http://saxon.sourceforge.net/\">Saxon</a>--but I&apos;ve found <a href=\"http://www.xmlsoft.org/XSLT.html\">libxslt</a> easiest to feed and care for on the various platforms with which I tinker.  It also seems to support a very large swath of <a href=\"http://www.exslt.org/\">EXSLT extensions</a>, all of which come in very handy, yet seem to receive uneven support in other XSLT processors.  We&apos;ll be pulling a trick or two out of that bag, so its support is key.</p>\n<p>You may or may not already have <a href=\"http://www.xmlsoft.org/XSLT.html\">libsxlt</a> installed.  Depending on your variant of Linux, it might be as simple as a single package-management command or it might be a bit more complex if you need to compile from source.  For Mac OS X, I recommend using <a href=\"http://fink.sourceforge.net\">Fink</a> for your packaging needs.  Although, <a href=\"http://darwinports.opendarwin.org/\">DarwinPorts</a> is nice as well, if you&apos;re used to The BSD Way.</p>\n<p>A bonus for OS X users: Marc Liyanage has provided a great Open Source tool named <a href=\"http://www.entropy.ch:16080/software/macosx/#testxslt\">TestXSLT</a> that embeds <a href=\"http://www.xmlsoft.org/XSLT.html\">libxslt</a>, among other XSLT processors, in a slick GUI for easier use.  This might come in handy for you as things develop.</p>\n<h3 id=\"wishlists-in-xml\">Wishlists in XML</h3>\n<p>Okay, we&apos;ve got a working environment, a head start on accessing Amazon wishlists as XML, and a way to manipulate that XML using <code>xsltproc</code>.  Let&apos;s start playing.  First things first, we need to gain access to Amazon wishlists in XML form.  Reading through the <a href=\"http://www.amazon.com/gp/browse.html/ref=sc_fe_c_2/002-7899886-3676027?%5Fencoding=UTF8&amp;node=3434641&amp;no=3435361&amp;me=A36L942TSJ2AJA\">AWS documentation</a> reveals that wish list searches are available via a URL constructed like so:</p>\n<pre><code>http://xml.amazon.com/onca/xml3?\nt=[Associates ID goes here]&amp;#38;\ndev-t=[Developer Token goes here]&amp;#38;\nWishlistSearch=[wishlist ID goes here]&amp;#38;\ntype=[lite or heavy]&amp;#38;\nf=xml</code></pre>\n<p>I received an ID of <code>0xdecafbad-20</code> when I <a href=\"http://associates.amazon.com\">signed up to be an associate</a> a few years ago.  This will ensure that I get credited for sales made via the API--which isn&apos;t as important for the present project, since I&apos;ll be buying items myself, but it&apos;ll come in handy in later projects.  Also, when I <a href=\"https://associates.amazon.com/exec/panama/associates/join/developer/application.html\">signed up for a developer&apos;s token</a>, this is what I was given: <code>D8HVH869XA0NP</code>  I&apos;m disclosing my own here for the sake of example, but you should <a href=\"https://associates.amazon.com/exec/panama/associates/join/developer/application.html\">sign up</a> and get your own.</p>\n<p>So, that fills in the first two parts of the URL.  For the purposes of this project, let&apos;s just go with the <code>lite</code> option for type.  As for the wishlist ID, let&apos;s take a look the wishlist URLs to which I linked earlier:</p>\n<pre><code>http://www.amazon.com/exec/obidos/registry/35OIOYWQ9XQAE\nhttp://www.amazon.com/exec/obidos/registry/1QWYI6P2JF3Q5</code></pre>\n<p>You can discover these wishlist URLs using <a href=\"http://www.amazon.com/gp/registry/search.html/002-7899886-3676027?%5Fencoding=UTF8&amp;type=wishlist\">Amazon&apos;s Wish List Search</a> feature, in which case a wishlist URL might appear like so:</p>\n<pre><code>http://www.amazon.com/gp/registry/registry.html/\n002-7899886-3676027?%5Fencoding=UTF8&amp;#38;\nid=35OIOYWQ9XQAE</code></pre>\n<p>In either case, there is a 13-character ID in each variety of wish list URL: this string is the wish list ID.  So, the ID for my girlfriend&apos;s wishlist is <code> 35OIOYWQ9XQAE</code> and mine is <code>1QWYI6P2JF3Q5</code>.  Given this piece of the puzzle, we can fill in the blanks to come up with the following URL for my girlfriend&apos;s wish list:</p>\n<pre><code>http://xml.amazon.com/onca/xml3?\nt=0xdecafbad-20&amp;#38;\ndev-t=D8HVH869XA0NP&amp;#38;\ntype=lite&amp;#38;\nWishlistSearch=35OIOYWQ9XQAE&amp;#38;\nf=xml</code></pre>\n<p><a href=\"http://xml.amazon.com/onca/xml3?t=0xdecafbad-20&amp;dev-t=D8HVH869XA0NP&amp;type=lite&amp;WishlistSearch=35OIOYWQ9XQAE&amp;f=xml\">Check out the XML resulting from this URL</a>--you may want to use a tool such as <code>curl</code> or <code>wget</code> instead of viewing this directly in your browser.  You&apos;ll see some XML that looks something like this:</p>\n<pre><code>&lt;ProductInfo&gt;\n...\n&lt;Details url=&quot;(some long URL)&quot;&gt;\n  &lt;Asin&gt;0262133601&lt;/Asin&gt;\n  &lt;ProductName&gt;Foundations of Statistical Natural Language Processing&lt;/ProductName&gt;\n  &lt;Catalog&gt;Book&lt;/Catalog&gt;\n  &lt;Authors&gt;\n     &lt;Author&gt;Christopher D. Manning&lt;/Author&gt;\n     &lt;Author&gt;Hinrich Sch&amp;#252;tze&lt;/Author&gt;\n  &lt;/Authors&gt;\n  &lt;ReleaseDate&gt;18 June, 1999&lt;/ReleaseDate&gt;\n  &lt;Manufacturer&gt;MIT Press&lt;/Manufacturer&gt;\n  &lt;ImageUrlSmall&gt;(another long url)&lt;/ImageUrlSmall&gt;\n  &lt;ImageUrlMedium&gt;(yet another long url)&lt;/ImageUrlMedium&gt;\n  &lt;ImageUrlLarge&gt;(one last long url)&lt;/ImageUrlLarge&gt;\n  &lt;Availability&gt;Usually ships within 24 hours&lt;/Availability&gt;\n  &lt;ListPrice&gt;$75.00&lt;/ListPrice&gt;\n  &lt;OurPrice&gt;$63.75&lt;/OurPrice&gt;\n  &lt;UsedPrice&gt;$49.99&lt;/UsedPrice&gt;\n&lt;/Details&gt;\n...\n&lt;/ProductInfo&gt;</code></pre>\n<p>Note that the <a href=\"http://www.amazon.com/exec/obidos/ASIN/0262133601/0xdecafbad-20?dev-t=D8HVH869XA0NP%26camp=2025%26link_code=xm2\">long URL</a> in the <code>Detail</code> element&apos;s <code>url</code> attribute links to the human-viewable product detail page at Amazon.  I&apos;ve also left a few other things out, such as the URLs to product images; I just thought I&apos;d edit it a bit to be friendlier to your browser at home.  There&apos;s a <a href=\"http://xml.amazon.com/schemas3/dev-lite.xsd\">schema</a> for this XML data, and the ins-and-outs are explained in the AWS documentation under &quot;Amazon Web Services Data Model&quot;.</p>\n<h3 id=\"querying-the-wishes\">Querying The Wishes</h3>\n<p>Some ready-made files are available for this section:</p>\n<ul>\n<li><a href=\"http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes-ex1.xsl\"><code>wishes-ex1.xsl</code></a>: The first iteration of the stylesheet in development.</li>\n<li><a href=\"http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes.xml\"><code>wishes.xml</code></a>: An XML document used as input with the stylesheet.</li>\n</ul>\n<p>Now that we&apos;ve got some XML from Amazon to play with, let&apos;s start tinkering with an XSLT stylesheet to process it.  In the interests of flexibility and reusability, we can parameterize a few things in XML before starting in on the stylesheet:</p>\n<pre><code>&lt;wishes xmlns=&quot;http://www.decafbad.com/2004/05/wishes&quot;&gt;\n  &lt;maxprice&gt;15.00&lt;/maxprice&gt;\n  &lt;associate&gt;0xdecafbad-20&lt;/associate&gt;\n  &lt;devtoken&gt;D8HVH869XA0NP&lt;/devtoken&gt;\n  &lt;email&gt;deus_x@pobox.com&lt;/email&gt;\n  &lt;wishlists&gt;\n    &lt;wishlist label=&quot;The Girl&quot;&gt;35OIOYWQ9XQAE&lt;/wishlist&gt;\n    &lt;wishlist label=&quot;Me&quot;&gt;1QWYI6P2JF3Q5&lt;/wishlist&gt;\n  &lt;/wishlists&gt;\n&lt;/wishes&gt;</code></pre>\n<p>Hopefully, the data here is fairly self-explanatory:  I&apos;ve established a maximum price for item selection; provided my associate ID and developer token; there&apos;s an email address to which I eventually want to send the results of all this work; and I&apos;ve made a list of wishlist IDs, each with a readable label. Given this, let&apos;s start out simple and  use this to get some data from Amazon:</p>\n<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;\n&lt;xsl:stylesheet version=&quot;1.0&quot;\n            xmlns:wishes=&quot;http://www.decafbad.com/2004/05/wishes&quot;\n            xmlns:xsl=&quot;http://www.w3.org/1999/XSL/Transform&quot;&gt;\n  &lt;xsl:output indent=&quot;yes&quot; /&gt;\n\n  &lt;!-- Grab our global settings --&gt;\n  &lt;xsl:variable name=&quot;maxprice&quot;  select=&quot;/wishes:wishes/wishes:maxprice&quot; /&gt;  \n  &lt;xsl:variable name=&quot;associate&quot; select=&quot;/wishes:wishes/wishes:associate&quot; /&gt;\n  &lt;xsl:variable name=&quot;devtoken&quot;  select=&quot;/wishes:wishes/wishes:devtoken&quot; /&gt;</code></pre>\n<p>So far so good--things start off by pulling in some of the parameters into variables.  Next, let&apos;s dig into actually querying wishlist data with a reusable template:</p>\n<pre><code>  &lt;xsl:template name=&quot;processWishlist&quot;&gt;\n    &lt;xsl:param name=&quot;wishlist&quot; /&gt;\n\n    &lt;xsl:variable name=&quot;details&quot; select=&quot;document(concat(\n        &apos;http://xml.amazon.com/onca/xml3?&apos;,\n        &apos;t=&apos;,$associate,&apos;&amp;amp;&apos;,\n        &apos;dev-t=&apos;,$devtoken,&apos;&amp;amp;&apos;,\n        &apos;WishlistSearch=&apos;,$wishlist,&apos;&amp;amp;&apos;,\n        &apos;type=lite&amp;amp;f=xml&apos;))//Details&quot; /&gt;</code></pre>\n<p>First thing into this template, we accept a parameter named <code>wishlist</code> which is expected to contain a wishlist ID string.  Next, we build an AWS URL by concatenating together the pieces we have in variables (associate ID, developer&apos;s token, and wishlist ID) using the XPath function <a href=\"http://www.w3.org/TR/2002/WD-xquery-operators-20020816/#func-concat\"><code>concat()</code></a>.  Once we have this URL, we use the function <a href=\"http://www.w3.org/TR/2002/WD-xquery-operators-20020816/#func-document\"><code>document()</code></a> to make a request and fetch the XML data for that URL.  From this, we select all the <code>Details</code> elements.  </p>\n<p>Then with that data, we can do some filtering on the price and availability.  We want to make sure that not only will we select items that are within our budget, but that they are available to buy in the first place:</p>\n<pre><code>    &lt;xsl:copy-of select=&quot;$details[\n      number(substring(OurPrice/text(),2)) &amp;lt; $maxprice\n      and\n      contains(Availability, &apos;Usually ships within&apos;)\n      ]&quot; /&gt;\n\n  &lt;/xsl:template&gt;</code></pre>\n<p>This code is just a little bit funky, since the price data given by Amazon contains a dollar sign, and we want to make a numerical comparison.  So, we chop the dollar sign off and convert to a number before making the comparison.  Also, there&apos;s an assumption here about what will show up in the <code>Availability</code> element: &quot;Usually ships within&quot;  Other things that might show up will declare that the item is out of stock, discontinued, or otherwise not shipping.  This might need some tweaking someday, but it seems to work for now.</p>\n<p>Taken all together, this template has the effect of a SQL SELECT statement somewhat like this:</p>\n<pre><code>SELECT * \nFROM Amazon.WishlistItems \nWHERE WishlistID = $wishlist AND \n      OurPrice &lt; $maxprice AND\n      Availability like &apos;%Usually ships within%&apos;;</code></pre>\n<p><code>document()</code> is a very useful XPath function.  It allows us to pull in XML from external files and, in our case, from external URLs via HTTP requests.  This gives us the ability to make queries against REST web services like AWS--which, among many other reasons, is why I prefer REST web services over SOAP.  (I don&apos;t even want to think about trying to access a SOAP service from XSLT.)</p>\n<p>Now, let&apos;s wrap up this first iteration of the stylesheet by trying out the query template on each of the wishlist IDs:</p>\n<pre><code>  &lt;xsl:template match=&quot;/wishes:wishes&quot;&gt;\n    &lt;xsl:for-each select=&quot;//wishes:wishlist&quot;&gt;\n      &lt;wishes:wishitem&gt;\n        &lt;xsl:copy-of select=&quot;.&quot; /&gt;\n        &lt;xsl:call-template name=&quot;processWishlist&quot;&gt;\n              &lt;xsl:with-param name=&quot;wishlist&quot; \n                              select=&quot;.&quot; /&gt;\n        &lt;/xsl:call-template&gt;\n      &lt;/wishes:wishitem&gt;\n    &lt;/xsl:for-each&gt;\n  &lt;/xsl:template&gt;\n\n&lt;/xsl:stylesheet&gt;</code></pre>\n<p>You can get a <a href=\"http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes-ex1.xsl\">completed version of this stylesheet</a>, along with <a href=\"http://www.decafbad.com/cvs/*checkout*/hacks/wishes/wishes.xml\">the input XML</a>, in case you haven&apos;t been cutting and pasting together a copy of your own along the way.  Try it out in a shell with:</p>\n<pre><code>$ xsltproc wishes_ex1.xsl wishes.xml</code></pre>\n<p>Alternately, you could check it out using <a href=\"http://www.entropy.ch:16080/software/macosx/#testxslt\">TestXSLT</a> under OS X.  You should get something like the following:</p>\n<pre><code>&lt;wishes:wishitem xmlns:wishes=&quot;http://www.decafbad.com/2004/05/wishes&quot;&gt;\n    &lt;wishes:wishlist label=&quot;The Girl&quot;&gt;35OIOYWQ9XQAE&lt;/wishes:wishlist&gt;\n    &lt;Details ...&gt;...&lt;/Details&gt;\n    &lt;Details ...&gt;...&lt;/Details&gt;\n    ...\n&lt;/wishes:wishitem&gt;\n&lt;wishes:wishitem xmlns:wishes=&quot;http://www.decafbad.com/2004/05/wishes&quot;&gt;\n    &lt;wishes:wishlist label=&quot;Me&quot;&gt;1QWYI6P2JF3Q5&lt;/wishes:wishlist&gt;\n    &lt;Details ...&gt;...&lt;/Details&gt;\n    &lt;Details ...&gt;...&lt;/Details&gt;\n    ...\n&lt;/wishes:wishitem&gt;</code></pre>\n<p>Obviously, this example XML is much abridged, but hopefully you can get the gist:  For each wishlist ID, there is a containing <code>wishitem</code> element.  It contains a copy of the <code>wishlist</code> element from the input XML, followed by all the <code>Details</code> elements filtered and copied from the Amazon XML with the help of the <code>processWishlist</code> template.</p>\n<h3 id=\"thats-all-for-now\">That&apos;s All for Now!</h3>\n<p>And that&apos;s the end of Part 1.  Next up, we&apos;ll be delving into a few more wrinkles in the wishlist querying process, selecting random items in XSLT, and the Remote Shopping Cart interface in Amazon Web Services.  Stay tuned!</p>\n<!-- links -->\n\n",
    "prevPostPath": "2004/06/28/wishofthemonthclub2",
    "nextPostPath": "2004/06/14/info-freako-or-whos-already-past-arguing-about-syndication-formats"
  },
  {
    "comments_archived": true,
    "date": "2004-06-14T22:54:55.000Z",
    "excerpt": "Where's the state-of-the-art for feed aggregators, and what's next?  I'm tired of reverse-chronological versus three-pane; I'm tired of copying Usenet and email.  What needs to happen next to expand our Info Freako capacity by an order of magnitude or two?  The invention of aggregators has opened the door to the first few orders, but I need more.",
    "layout": "post",
    "tags": [
      "syndication"
    ],
    "title": "Info Freako, or who's already past arguing about syndication formats?",
    "wordpress_id": 528,
    "wordpress_slug": "info-freako-or-whos-already-past-arguing-about-syndication-formats",
    "wordpress_url": "http://www.decafbad.com/blog/?p=528",
    "year": "2004",
    "month": "06",
    "day": "14",
    "isDir": false,
    "slug": "info-freako-or-whos-already-past-arguing-about-syndication-formats",
    "postName": "2004-06-14-info-freako-or-whos-already-past-arguing-about-syndication-formats",
    "html": "<blockquote>\n<p>You know that sleep is getting hard to get<br />\n&#39;Cause you never know what you&#39;ll forget<br />\nAnd I&#39;ve got to know of all the news<br />\n&#39;Cause one day there&#39;ll be news for me<br />\n<br />\nI never let a headline by<br />\n&#39;Cause every one will catch my eye<br />\nAnd though it&#39;s tough to keep alert<br />\nYou never know what could hurt me<br /></p>\n</blockquote>\n<div class=\"credit\" align=\"right\"><small>Source: <cite><a href=\n\"http://www.jesusjonesarchive.com/lyrics.htm#Info%20Freako\">Jesus Jones, \"Info Freako\"</a></cite></small></div>\n\n<p>The Syndications Wars are over--at least, as far as I&#39;m concerned.  </p>\n<p>It&#39;s hard to resist <a href=\"http://www.reactuate.com/index.php?itemid=682\">jumping in</a> where I think someone&#39;s got it wrong or when my fingers compel me to feed trolls, but resisting that impulse is what needs to be done.  At this point, all that seems to happen is that the same old threads get recycled.  No one&#39;s got anything new to bring, except maybe ad hominem grousing or possibly a Yo Mama joke (though I&#39;ve yet to see that particular innovation).  Anyone who cares can do a bit of Googling to catch up on the story.</p>\n<p>The fact is, I no longer care about RSS versus Atom versus $foo.  Mark Pilgrim enables me to do so with his <a href=\"http://diveintomark.org/projects/feed_parser/\">feed parser</a>, and most other aggregators I might care about have also implemented support for both Atom and RSS.  And if they don&#39;t, I can <a href=\"http://www.decafbad.com/blog/2004/05/03/put_on_your_rsscolored_glasses_and_forget_about_atom\">route around the damage</a> just like I do when I <a href=\"http://www.decafbad.com/twiki/bin/view/Main/XslScraper\">scrape sites</a> devoid of any feeds.</p>\n<p>While I do prefer Atom over RSS, almost a year later I still say <a href=\"http://www.decafbad.com/blog/2003/07/07/syndications_formats\">the magic is in syndication, not the format</a>.  I&#39;ll let the tag-level grumblers foam on without comment and just thank them for their work when a good new spec  bubbles up or when something fun and useful comes out.  I&#39;m circling that whole area of concern and sticking a post-it on it that reads: <em>RSS and Atom both useful, neither perfect, neither going away.</em></p>\n<p>Whew.  That&#39;s a weight off my brain.  Now what is there left to talk about?</p>\n<p>Well, how about let&#39;s talk some more about what to do with the items we get, once we <em>do</em> manage to parse a feed?  (Sheesh, you mean we&#39;re not already fighting over that topic?)  </p>\n<p><a href=\"http://www.cadenhead.org/workbench/2004/06/10.html#a1802\">Rogers Cadenhead wants gluttonous RSS feeders</a>:</p>\n<blockquote>\n<p>With thousands of information sources producing RSS and Atom feeds, we need people like Thauvin [whose linkblog is <a href=\"http://www.thauvin.net/linkblog/\">here</a>] who have integrated weblogging into their daily news-gathering routine. Weblog links are like ant trails -- a lot of people have to link to something good in order to get noticed.</p>\n</blockquote>\n<p>I self-identify as such, since my feed list has topped 550 in count.  But I&#39;m happily surprised to find that I&#39;m not even in the <a href=\"http://feeds.scripting.com/prolificSubscribers\">top 10 of prolific subscribers</a>--at least I&#39;m not the biggest Info Freako.  (Yet.)  </p>\n<p>I&#39;m adding between 2-3 feeds to my list daily, so I can see myself approaching 1000 eventually.  But I&#39;m starting to hesitate at adding one more feed now.  Even with my current streamlined multi-pass skimming process, I&#39;m starting to see diminishing returns.  I breeze past screen loads of chaff that I&#39;ll never view, but it still bogs me down.  I can only think that people with twice the subscriptions as I either have more free time, or have a better mousetrap.</p>\n<p>The usual response I get toward my subscriptions is, &quot;Why don&#39;t you cut that list down to about 100 essentials?&quot;  And even that&#39;s said with a smirk, usually by someone with under 50 subscriptions and usually by someone who&#39;s not as obsessive an <a href=\"http://www.jesusjonesarchive.com/lyrics.htm#Info%20Freako\" title=\"Jesus Jones lyrics\">Info Freako</a> as I am.  Thing is, though, good stuff has at one point or another shown up on each and every feed I monitor.  I want to figure out how to scale <em>up</em> from 1 to 10 to 100 to 1000 to 10000 sources and beyond.</p>\n<p>(Singing interlude: &quot;Info Freako / There is no end to what I want to know&quot;)</p>\n<p>Besides, this is an area where I can tinker with and learn about another area I&#39;m interested in: machine intelligence and intelligence amplification.  Rogers says, &quot;I want a Bayesian filter that can guess which new headlines I&#39;m most likely to read&quot;  Though someone else might apply Bayes in a way that works for them, I didn&#39;t find <a href=\"http://www.decafbad.com/blog/2003/08/16/bayes_agg_one\">my experiments with SpamBayes</a> very satisfying.  I suspect it has something to do with the fact that SpamBayes is geared toward sorting out a quasi-binary world of spam-versus-ham, while I&#39;m interested in a spectrum between must-read and shrugs.</p>\n<p>But, the idea of introducing another pass through items at the head of the process, this one partially or completely automated, has great appeal.  Done right, this could be the bit that adds an order of magnitude to my capacity to monitor feeds.  I need to investigate other machine learning approaches.</p>\n<p>The idea is that, while I freakishly want to catch as much info as possible, I can only handle so much in a day.  For certain, I can&#39;t handle everything that might be interesting to me, so I need some prioritization and some pre-filtering before my attention gets applied to the flow.</p>\n<p>The way I picture this is trying to apply a sort of <a href=\"http://mtsu32.mtsu.edu:11178/171/pyramid.htm\">inverted pyramid</a> approach to the incoming flow of items.</p>\n<p>I started with a few primitive tools in <a href=\"http://www.decafbad.com/blog/2002/08/05/ooobbf\">AmphetaOutlines&#39; adaptivity to reading patterns</a>, limited mostly to just sorting channels by a count of items read historically.  I also introduced some information hiding and exploration aspects:  I tried to hide or de-emphasize older items by use of font size and weight; and I put items into a JavaScript-driven outline where item descriptions and more ancient items could be hidden or revealed via disclosure triangle.</p>\n<p>In my <a href=\"http://www.decafbad.com/cvs/dbagg2\">latest attempt</a>, I&#39;ve not yet implemented any adaptive sorting, but I&#39;ve kept and improved the outline display (see: <a href=\"http://www.decafbad.com/2004/06/dbagg2a.jpg\">screenshot #1</a>, <a href=\"http://www.decafbad.com/2004/06/dbagg2b.jpg\">screenshot #2</a>).  Also, I can now mark items as seen and/or flag them to be viewed in a queue for later.  I&#39;ve got some lame SpamBayes integration in there, but I&#39;ve let it atrophy in daily use due to a complete lack of usefulness.</p>\n<p>I&#39;m starting to think about next steps now toward a more advanced aggregator.  I&#39;ve still got my <a href=\"http://www.decafbad.com/twiki/bin/view/Main/AmphetaOutlinesWishList\">wishlist</a> for AmphetaOutlines, and I&#39;ve actually covered quite a few of the items with this new aggregator.  But, I&#39;m thinking things like the following would be useful to pursue:</p>\n<ul>\n<li><p><strong>What do you think is more important?</strong>  Do you value one group of feeds over another?  Personally, I want to see every single web comic that appears in my queue, most items from <a href=\"http://www.engadget.com/\">Engadget</a> and <a href=\"http://boingboing.net/\">Boing Boing</a>, and maybe only a few from some of the firehoses I&#39;ve hooked myself up to.  Also, there are <a href=\"http://interconnected.org/home/\">some</a> <a href=\"http://www.ecyrd.com/ButtUgly/Wiki.jsp?page=Main_blogentry_130604_1\">bloggers</a> who post somewhat infrequently, but I don&#39;t want to miss a thing when they <em>do</em> post.  I need to be able to group and prioritize manually.</p>\n</li>\n<li><p><strong>What do you <em>demonstrate</em> as important?</strong>  Which feeds&#39; items receive more of your attention, and within those feeds, what topics and phrases appear most frequently?  The machine should be able to make some observations about your history of behavior and give some input into the organization of items presented.  Also, it should give me some way to give feedback to its recommendations with a simple and lazy thumbs up and thumbs down.</p>\n</li>\n<li><p><strong>Republishing of interesting items to a linkblog is a must.</strong>  On the flip-side, it would be nice to somehow pull in others&#39; linkblogs in a more meaningful way than simply watching their feeds.  I should be able to triangulate some things and get some recommendations based on mutual links predicting future interest in items.  We need to start chasing ant trails unconsciously and automatically.</p>\n</li>\n<li><p><strong>Time-limited subscriptions which expire after a set time, or request renewal from the user.</strong>  Use these to track comment threads which offer RSS feeds.  (Like <a href=\"http://www.pycs.net/system/comments.py?u=0000001&#38;p=1802&#38;format=rss\">this one</a>.)</p>\n</li>\n<li><p><strong>More statistics and health monitoring of subscriptions.</strong>  How active are your feeds?  Which are dead &#38; gone, or merely just in hiatus?  Have any moved?</p>\n</li>\n</ul>\n<p>Now, I haven&#39;t done any sort of comprehensive survey of the aggregator landscape in a long time, so I&#39;d be very intrigued if any existing software implements these sorts of things.  I&#39;ve seen some progress toward monitoring feed health, but I&#39;ve seen next to nothing toward automatic filtering of items and recommendations based on past behavior.  I have seen manually constructed filters, but I&#39;m too lazy to try to figure out how to tell the computer what I want.   I want the machine to ride shotgun, watch and learn.</p>\n<p><a href=\"http://www.johnny-five.com/\"><img src=\"http://www.johnny-five.com/simplenet/Shortcircuit/Pics/Pictures/Misc/jfive.gif\" align=\"right\" alt=\"Need more input!\" hspace=\"10\" /></a> Where&#39;s the state-of-the-art for feed aggregators, and what&#39;s next?  I&#39;m tired of reverse-chronological versus three-pane; I&#39;m tired of copying Usenet and email.  What needs to happen next to expand our Info Freako capacity by an order of magnitude or two?  The invention of aggregators has opened the door to the first few orders, but I need more.   </p>\n<p>Exit singing (while twitching for more info):</p>\n<blockquote>\n<p>Info Freako, <br />\nThere is no end to what I want to know <br />\n<br />\nBut it means I&#39;ll have the edge over you<br />\nAnd it means I&#39;ll always have the edge over you<br />\nAnd you know there&#39;s nothing that you can do<br /></p>\n</blockquote>\n<div id=\"comments\" class=\"comments archived-comments\"><h3>Archived Comments</h3>\n<ul class=\"comments\">\n<li class=\"comment\" id=\"comment-221086587\">\n<div class=\"meta\">\n<div class=\"author\">\n<a class=\"avatar image\" rel=\"nofollow\" \nhref=\"http://www.majid.info/\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=8c5548eb0b2b80924f237953392df5e7&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\n<a class=\"avatar name\" rel=\"nofollow\" \nhref=\"http://www.majid.info/\">Fazal Majid</a>\n</div>\n<a href=\"#comment-221086587\" class=\"permalink\"><time datetime=\"2004-06-14T21:52:14\">2004-06-14T21:52:14</time></a>\n</div>\n<div class=\"content\">I am trying to do this with my own home-made aggregator (www.temboz.com).\nI have serious doubts about software that could automatically determine whether an article is interesting or not, and would settle for a way to suppress duplicates as a meme propagates through the blogosphere, e.g. by finding posts that link to the same URLs.\nI did implement a simplistic kill-file style filtering system, that works reasonably well, no false positives and 204 out of 1701 items in the last 7 days caught.</div>\n</li>\n<li class=\"comment\" id=\"comment-221086588\">\n<div class=\"meta\">\n<div class=\"author\">\n<a class=\"avatar image\" rel=\"nofollow\" \nhref=\"http://wurldbook.blogspot.com\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=994697b1f98037fb294ca1679209aaa8&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\n<a class=\"avatar name\" rel=\"nofollow\" \nhref=\"http://wurldbook.blogspot.com\">Olav</a>\n</div>\n<a href=\"#comment-221086588\" class=\"permalink\"><time datetime=\"2004-06-14T22:25:55\">2004-06-14T22:25:55</time></a>\n</div>\n<div class=\"content\">Anyone who subscribes to so many feeds has a mental illness. Like some kind of obsessive compulsive disorder. I bet they record all the channels on T.V. in case they miss something. I bet they don't throw away any newspapers either. I subscribe to about 144 feeds but that is only because I am testing an app. Most of it is garbage.</div>\n</li>\n<li class=\"comment\" id=\"comment-221086590\">\n<div class=\"meta\">\n<div class=\"author\">\n<a class=\"avatar image\" rel=\"nofollow\" \nhref=\"http://blogory.com\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=8b702a775f458d5c2cf53f1021cb3ac6&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\n<a class=\"avatar name\" rel=\"nofollow\" \nhref=\"http://blogory.com\">Greg Linden</a>\n</div>\n<a href=\"#comment-221086590\" class=\"permalink\"><time datetime=\"2004-06-14T22:49:58\">2004-06-14T22:49:58</time></a>\n</div>\n<div class=\"content\">You might check out Findory Blogory (http://blogory.com).  It's a personalized weblog reader that learns from the weblog articles you read and recommends other articles that appear to match your interests.\nIt's not a Bayesean filter like SpamBayes, but it does \"guess which new headlines you're most likely to read.\"  It is \"automatic filtering of items and recommendations based on past behavior.\"  You don't have to manually construct filters or \"tell the computer what you want.\"  Findory Blogory will \"ride shotgun, watch and learn.\"\nTake a look.  I'd love to hear your thoughts.</div>\n</li>\n<li class=\"comment\" id=\"comment-221086592\">\n<div class=\"meta\">\n<div class=\"author\">\n<a class=\"avatar image\" rel=\"nofollow\" \nhref=\"http://www.decafbad.com/blog/\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=8a5273f79cfe7579ad46023f93377aa8&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\n<a class=\"avatar name\" rel=\"nofollow\" \nhref=\"http://www.decafbad.com/blog/\">l.m.orchard</a>\n</div>\n<a href=\"#comment-221086592\" class=\"permalink\"><time datetime=\"2004-06-14T23:03:38\">2004-06-14T23:03:38</time></a>\n</div>\n<div class=\"content\">Olav: See, for me, it isn't so much a compulsion as a challenge.  I don't read through my aggregator all day - I spend about an hour per day at lunch and maybe another at home skimming through items and maybe another hour or so reading longer stories.  \nThis isn't handwashing OCD behavior here - I don't read or even linger on more than a small fraction of what ends up on the screen.  It's a lot of quick glancing, page-down, and mass delete.  It's pretty much as much time as I used to spend just between a half-dozen sites like slashdot.org and cnet.com, only now my sources are so much more varied.\nThe challenge is that wanting to squeeze more coverage of more sources into my daily browsing has me wanting to learn more about certain technologies.  That, and just dealing with more volumes of data is kinda fun in a geeky sort of way.\nAnd, no, I don't record all the channels on the TV.  Besides watching 1 or 2 shows, and playing a few video games, this is what I do instead of sitting in front of the tube.</div>\n</li>\n<li class=\"comment\" id=\"comment-221086593\">\n<div class=\"meta\">\n<div class=\"author\">\n<a class=\"avatar image\" rel=\"nofollow\" \nhref=\"http://www.reactuate.com/\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=43e4a9fa1d0e5d52a6979ddad94bc483&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\n<a class=\"avatar name\" rel=\"nofollow\" \nhref=\"http://www.reactuate.com/\">Ron</a>\n</div>\n<a href=\"#comment-221086593\" class=\"permalink\"><time datetime=\"2004-06-16T19:52:44\">2004-06-16T19:52:44</time></a>\n</div>\n<div class=\"content\">When I wrote that story I wasn't expecting people to jump into a fight about RSS vs Atom. Should have known better. I just didn't like the original story and wanted to rant about it.\nOh well you learn something new everyday.</div>\n</li>\n<li class=\"comment\" id=\"comment-221086594\">\n<div class=\"meta\">\n<div class=\"author\">\n<a class=\"avatar image\" rel=\"nofollow\" \nhref=\"http://blog.ianbicking.org\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=cc8334869c9d2a9e603017f2da805eb3&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\n<a class=\"avatar name\" rel=\"nofollow\" \nhref=\"http://blog.ianbicking.org\">Ian Bicking</a>\n</div>\n<a href=\"#comment-221086594\" class=\"permalink\"><time datetime=\"2004-06-17T00:36:51\">2004-06-17T00:36:51</time></a>\n</div>\n<div class=\"content\">You might want to look at Reverend: http://www.divmod.org/Home/Projects/Reverend/\nIt's a general-purpose Bayesian classifier.</div>\n</li>\n<li class=\"comment\" id=\"comment-221086595\">\n<div class=\"meta\">\n<div class=\"author\">\n<a class=\"avatar image\" rel=\"nofollow\" \nhref=\"http://quillio.com/\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=b228325c24751d6a33a893fb8116d32f&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\n<a class=\"avatar name\" rel=\"nofollow\" \nhref=\"http://quillio.com/\">Lou Quillio</a>\n</div>\n<a href=\"#comment-221086595\" class=\"permalink\"><time datetime=\"2004-06-17T05:01:13\">2004-06-17T05:01:13</time></a>\n</div>\n<div class=\"content\">There&#8217;s what we&#8217;d like to scan and what we&#8217;d like to read, yes?  Moving content to the must-read front burner will always be deliberate, but improving the content we scan can probably be improved mechanically.  I doubt Brother Orchard thinks there&#8217;s an algorithm for the A-list.  We&#8217;re just talking about making more productive use of the proximate cloud.\nScenario #1:  You Google for topics of interest.  The hits returned can&#8217;t be reliably filtered by creation date, and they consider (in general) the entire indexed Web.  It&#8217;s a good thing, if scatter-shot.  Queries can only be refined so much.\nScenario #2:  You amass and manage a list of syndicated feeds in which you have a passing interest.  The feed content is indexed (Bloglines does this; Google does or will, too).  You can query this universe discretely, using Google-like devices and maybe a few new ones.\nWhat&#8217;s different about these two?\nIn #2, you selected (and casually manage) the universe.  Its chunks &#8212; though perhaps truncated or paraphrased by the author &#8212; are reliably date-stamped.  A free-text query of such data would be very different from an identical Google query.\nAdd a few more discriminators and it gets better.  Suppose you could discriminate by\n1. Paragraph length.  That might get you recent topical hits of terse or essay length, as you choose.\n2. Link-text density.  Filter in or out those link-collection posts.\n3. Query-term density or semantic weight.  Is the topic being addressed directly or simply mentioned?\n4. Item weight or image density.  Helps filter noisy or glitzy corporate feeds (think Ziff-Davis).\nThere are plenty more.\nThe main differences, though, are date discrimination and the pre-filter of a confined universe.  Me, I&#8217;m down to thirty feeds and ignore half of those.  Better tools might change that.\nLQ</div>\n</li>\n<li class=\"comment\" id=\"comment-221086596\">\n<div class=\"meta\">\n<div class=\"author\">\n<a class=\"avatar image\" rel=\"nofollow\" \nhref=\"http://advogato.org/person/mbrubeck/\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=85232f8499fd6ee91623408fc23835d1&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\n<a class=\"avatar name\" rel=\"nofollow\" \nhref=\"http://advogato.org/person/mbrubeck/\">Matt Brubeck</a>\n</div>\n<a href=\"#comment-221086596\" class=\"permalink\"><time datetime=\"2004-06-19T21:06:49\">2004-06-19T21:06:49</time></a>\n</div>\n<div class=\"content\">I love the picture.  Need input!</div>\n</li>\n</ul>\n</div>\n",
    "body": "> You know that sleep is getting hard to get<br />\r\n> 'Cause you never know what you'll forget<br />\r\n> And I've got to know of all the news<br />\r\n> 'Cause one day there'll be news for me<br />\r\n> <br />\r\n> I never let a headline by<br />\r\n> 'Cause every one will catch my eye<br />\r\n> And though it's tough to keep alert<br />\r\n> You never know what could hurt me<br />\r\n\r\n<div class=\"credit\" align=\"right\"><small>Source: <cite><a href=\r\n\"http://www.jesusjonesarchive.com/lyrics.htm#Info%20Freako\">Jesus Jones, \"Info Freako\"</a></cite></small></div>\r\n\r\nThe Syndications Wars are over--at least, as far as I'm concerned.  \r\n\r\nIt's hard to resist [jumping in][jumping in] where I think someone's got it wrong or when my fingers compel me to feed trolls, but resisting that impulse is what needs to be done.  At this point, all that seems to happen is that the same old threads get recycled.  No one's got anything new to bring, except maybe ad hominem grousing or possibly a Yo Mama joke (though I've yet to see that particular innovation).  Anyone who cares can do a bit of Googling to catch up on the story.\r\n\r\nThe fact is, I no longer care about RSS versus Atom versus $foo.  Mark Pilgrim enables me to do so with his [feed parser][feedparser], and most other aggregators I might care about have also implemented support for both Atom and RSS.  And if they don't, I can [route around the damage][rss colored glasses] just like I do when I [scrape sites][scrape sites] devoid of any feeds.\r\n\r\nWhile I do prefer Atom over RSS, almost a year later I still say [the magic is in syndication, not the format][syndication magic].  I'll let the tag-level grumblers foam on without comment and just thank them for their work when a good new spec  bubbles up or when something fun and useful comes out.  I'm circling that whole area of concern and sticking a post-it on it that reads: *RSS and Atom both useful, neither perfect, neither going away.*\r\n\r\nWhew.  That's a weight off my brain.  Now what is there left to talk about?\r\n\r\nWell, how about let's talk some more about what to do with the items we get, once we *do* manage to parse a feed?  (Sheesh, you mean we're not already fighting over that topic?)  \r\n\r\n[Rogers Cadenhead wants gluttonous RSS feeders][cadenhead]:\r\n\r\n> With thousands of information sources producing RSS and Atom feeds, we need people like Thauvin [whose linkblog is [here][thauvin]] who have integrated weblogging into their daily news-gathering routine. Weblog links are like ant trails -- a lot of people have to link to something good in order to get noticed.\r\n\r\nI self-identify as such, since my feed list has topped 550 in count.  But I'm happily surprised to find that I'm not even in the [top 10 of prolific subscribers][prolific subs]--at least I'm not the biggest Info Freako.  (Yet.)  \r\n\r\nI'm adding between 2-3 feeds to my list daily, so I can see myself approaching 1000 eventually.  But I'm starting to hesitate at adding one more feed now.  Even with my current streamlined multi-pass skimming process, I'm starting to see diminishing returns.  I breeze past screen loads of chaff that I'll never view, but it still bogs me down.  I can only think that people with twice the subscriptions as I either have more free time, or have a better mousetrap.\r\n\r\nThe usual response I get toward my subscriptions is, \"Why don't you cut that list down to about 100 essentials?\"  And even that's said with a smirk, usually by someone with under 50 subscriptions and usually by someone who's not as obsessive an [Info Freako][info freako] as I am.  Thing is, though, good stuff has at one point or another shown up on each and every feed I monitor.  I want to figure out how to scale *up* from 1 to 10 to 100 to 1000 to 10000 sources and beyond.\r\n\r\n(Singing interlude: \"Info Freako / There is no end to what I want to know\")\r\n\r\nBesides, this is an area where I can tinker with and learn about another area I'm interested in: machine intelligence and intelligence amplification.  Rogers says, \"I want a Bayesian filter that can guess which new headlines I'm most likely to read\"  Though someone else might apply Bayes in a way that works for them, I didn't find [my experiments with SpamBayes][bayes] very satisfying.  I suspect it has something to do with the fact that SpamBayes is geared toward sorting out a quasi-binary world of spam-versus-ham, while I'm interested in a spectrum between must-read and shrugs.\r\n\r\nBut, the idea of introducing another pass through items at the head of the process, this one partially or completely automated, has great appeal.  Done right, this could be the bit that adds an order of magnitude to my capacity to monitor feeds.  I need to investigate other machine learning approaches.\r\n\r\nThe idea is that, while I freakishly want to catch as much info as possible, I can only handle so much in a day.  For certain, I can't handle everything that might be interesting to me, so I need some prioritization and some pre-filtering before my attention gets applied to the flow.\r\n\r\nThe way I picture this is trying to apply a sort of [inverted pyramid][inverted pyramid] approach to the incoming flow of items.\r\n\r\nI started with a few primitive tools in [AmphetaOutlines' adaptivity to reading patterns][amphetaoutlines], limited mostly to just sorting channels by a count of items read historically.  I also introduced some information hiding and exploration aspects:  I tried to hide or de-emphasize older items by use of font size and weight; and I put items into a JavaScript-driven outline where item descriptions and more ancient items could be hidden or revealed via disclosure triangle.\r\n\r\nIn my [latest attempt][dbagg2], I've not yet implemented any adaptive sorting, but I've kept and improved the outline display (see: [screenshot #1][dbagg2 screen1], [screenshot #2][dbagg2 screen2]).  Also, I can now mark items as seen and/or flag them to be viewed in a queue for later.  I've got some lame SpamBayes integration in there, but I've let it atrophy in daily use due to a complete lack of usefulness.\r\n\r\nI'm starting to think about next steps now toward a more advanced aggregator.  I've still got my [wishlist][wishlist] for AmphetaOutlines, and I've actually covered quite a few of the items with this new aggregator.  But, I'm thinking things like the following would be useful to pursue:\r\n\r\n* **What do you think is more important?**  Do you value one group of feeds over another?  Personally, I want to see every single web comic that appears in my queue, most items from [Engadget][engadget] and [Boing Boing][boing boing], and maybe only a few from some of the firehoses I've hooked myself up to.  Also, there are [some][blogger1] [bloggers][blogger2] who post somewhat infrequently, but I don't want to miss a thing when they *do* post.  I need to be able to group and prioritize manually.\r\n\r\n* **What do you *demonstrate* as important?**  Which feeds' items receive more of your attention, and within those feeds, what topics and phrases appear most frequently?  The machine should be able to make some observations about your history of behavior and give some input into the organization of items presented.  Also, it should give me some way to give feedback to its recommendations with a simple and lazy thumbs up and thumbs down.\r\n\r\n* **Republishing of interesting items to a linkblog is a must.**  On the flip-side, it would be nice to somehow pull in others' linkblogs in a more meaningful way than simply watching their feeds.  I should be able to triangulate some things and get some recommendations based on mutual links predicting future interest in items.  We need to start chasing ant trails unconsciously and automatically.\r\n\r\n* **Time-limited subscriptions which expire after a set time, or request renewal from the user.**  Use these to track comment threads which offer RSS feeds.  (Like [this one][comment feed].)\r\n\r\n* **More statistics and health monitoring of subscriptions.**  How active are your feeds?  Which are dead &#38; gone, or merely just in hiatus?  Have any moved?\r\n\r\nNow, I haven't done any sort of comprehensive survey of the aggregator landscape in a long time, so I'd be very intrigued if any existing software implements these sorts of things.  I've seen some progress toward monitoring feed health, but I've seen next to nothing toward automatic filtering of items and recommendations based on past behavior.  I have seen manually constructed filters, but I'm too lazy to try to figure out how to tell the computer what I want.   I want the machine to ride shotgun, watch and learn.\r\n\r\n<a href=\"http://www.johnny-five.com/\"><img src=\"http://www.johnny-five.com/simplenet/Shortcircuit/Pics/Pictures/Misc/jfive.gif\" align=\"right\" alt=\"Need more input!\" hspace=\"10\" /></a> Where's the state-of-the-art for feed aggregators, and what's next?  I'm tired of reverse-chronological versus three-pane; I'm tired of copying Usenet and email.  What needs to happen next to expand our Info Freako capacity by an order of magnitude or two?  The invention of aggregators has opened the door to the first few orders, but I need more.   \r\n\r\nExit singing (while twitching for more info):\r\n\r\n> Info Freako, <br />\r\n> There is no end to what I want to know <br />\r\n> <br />\r\n> But it means I'll have the edge over you<br />\r\n> And it means I'll always have the edge over you<br />\r\n> And you know there's nothing that you can do<br />\r\n\r\n[jumping in]: http://www.reactuate.com/index.php?itemid=682\r\n[blogger1]: http://interconnected.org/home/\r\n[blogger2]: http://www.ecyrd.com/ButtUgly/Wiki.jsp?page=Main_blogentry_130604_1\r\n[engadget]: http://www.engadget.com/\r\n[boing boing]: http://boingboing.net/\r\n[inverted pyramid]: http://mtsu32.mtsu.edu:11178/171/pyramid.htm\r\n[comment feed]: http://www.pycs.net/system/comments.py?u=0000001&#38;p=1802&#38;format=rss\r\n[wishlist]: http://www.decafbad.com/twiki/bin/view/Main/AmphetaOutlinesWishList\r\n[dbagg2 screen1]: http://www.decafbad.com/2004/06/dbagg2a.jpg\r\n[dbagg2 screen2]: http://www.decafbad.com/2004/06/dbagg2b.jpg\r\n[dbagg2]: http://www.decafbad.com/cvs/dbagg2\r\n[amphetaoutlines]: http://www.decafbad.com/blog/2002/08/05/ooobbf\r\n[bayes]: http://www.decafbad.com/blog/2003/08/16/bayes_agg_one\r\n[info freako]: http://www.jesusjonesarchive.com/lyrics.htm#Info%20Freako \"Jesus Jones lyrics\"\r\n[thauvin]: http://www.thauvin.net/linkblog/\r\n[prolific subs]: http://feeds.scripting.com/prolificSubscribers\r\n[cadenhead]: http://www.cadenhead.org/workbench/2004/06/10.html#a1802\r\n[syndication magic]: http://www.decafbad.com/blog/2003/07/07/syndications_formats\r\n[scrape sites]: http://www.decafbad.com/twiki/bin/view/Main/XslScraper\r\n[rss colored glasses]: http://www.decafbad.com/blog/2004/05/03/put_on_your_rsscolored_glasses_and_forget_about_atom\r\n[feedparser]: http://diveintomark.org/projects/feed_parser/\r\n\r\n<div id=\"comments\" class=\"comments archived-comments\">\r\n            <h3>Archived Comments</h3>\r\n            \r\n        <ul class=\"comments\">\r\n            \r\n        <li class=\"comment\" id=\"comment-221086587\">\r\n            <div class=\"meta\">\r\n                <div class=\"author\">\r\n                    <a class=\"avatar image\" rel=\"nofollow\" \r\n                       href=\"http://www.majid.info/\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=8c5548eb0b2b80924f237953392df5e7&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\r\n                    <a class=\"avatar name\" rel=\"nofollow\" \r\n                       href=\"http://www.majid.info/\">Fazal Majid</a>\r\n                </div>\r\n                <a href=\"#comment-221086587\" class=\"permalink\"><time datetime=\"2004-06-14T21:52:14\">2004-06-14T21:52:14</time></a>\r\n            </div>\r\n            <div class=\"content\">I am trying to do this with my own home-made aggregator (www.temboz.com).\r\n\r\nI have serious doubts about software that could automatically determine whether an article is interesting or not, and would settle for a way to suppress duplicates as a meme propagates through the blogosphere, e.g. by finding posts that link to the same URLs.\r\n\r\nI did implement a simplistic kill-file style filtering system, that works reasonably well, no false positives and 204 out of 1701 items in the last 7 days caught.</div>\r\n            \r\n        </li>\r\n    \r\n        <li class=\"comment\" id=\"comment-221086588\">\r\n            <div class=\"meta\">\r\n                <div class=\"author\">\r\n                    <a class=\"avatar image\" rel=\"nofollow\" \r\n                       href=\"http://wurldbook.blogspot.com\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=994697b1f98037fb294ca1679209aaa8&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\r\n                    <a class=\"avatar name\" rel=\"nofollow\" \r\n                       href=\"http://wurldbook.blogspot.com\">Olav</a>\r\n                </div>\r\n                <a href=\"#comment-221086588\" class=\"permalink\"><time datetime=\"2004-06-14T22:25:55\">2004-06-14T22:25:55</time></a>\r\n            </div>\r\n            <div class=\"content\">Anyone who subscribes to so many feeds has a mental illness. Like some kind of obsessive compulsive disorder. I bet they record all the channels on T.V. in case they miss something. I bet they don't throw away any newspapers either. I subscribe to about 144 feeds but that is only because I am testing an app. Most of it is garbage.</div>\r\n            \r\n        </li>\r\n    \r\n        <li class=\"comment\" id=\"comment-221086590\">\r\n            <div class=\"meta\">\r\n                <div class=\"author\">\r\n                    <a class=\"avatar image\" rel=\"nofollow\" \r\n                       href=\"http://blogory.com\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=8b702a775f458d5c2cf53f1021cb3ac6&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\r\n                    <a class=\"avatar name\" rel=\"nofollow\" \r\n                       href=\"http://blogory.com\">Greg Linden</a>\r\n                </div>\r\n                <a href=\"#comment-221086590\" class=\"permalink\"><time datetime=\"2004-06-14T22:49:58\">2004-06-14T22:49:58</time></a>\r\n            </div>\r\n            <div class=\"content\">You might check out Findory Blogory (http://blogory.com).  It's a personalized weblog reader that learns from the weblog articles you read and recommends other articles that appear to match your interests.\r\n\r\nIt's not a Bayesean filter like SpamBayes, but it does \"guess which new headlines you're most likely to read.\"  It is \"automatic filtering of items and recommendations based on past behavior.\"  You don't have to manually construct filters or \"tell the computer what you want.\"  Findory Blogory will \"ride shotgun, watch and learn.\"\r\n\r\nTake a look.  I'd love to hear your thoughts.</div>\r\n            \r\n        </li>\r\n    \r\n        <li class=\"comment\" id=\"comment-221086592\">\r\n            <div class=\"meta\">\r\n                <div class=\"author\">\r\n                    <a class=\"avatar image\" rel=\"nofollow\" \r\n                       href=\"http://www.decafbad.com/blog/\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=8a5273f79cfe7579ad46023f93377aa8&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\r\n                    <a class=\"avatar name\" rel=\"nofollow\" \r\n                       href=\"http://www.decafbad.com/blog/\">l.m.orchard</a>\r\n                </div>\r\n                <a href=\"#comment-221086592\" class=\"permalink\"><time datetime=\"2004-06-14T23:03:38\">2004-06-14T23:03:38</time></a>\r\n            </div>\r\n            <div class=\"content\">Olav: See, for me, it isn't so much a compulsion as a challenge.  I don't read through my aggregator all day - I spend about an hour per day at lunch and maybe another at home skimming through items and maybe another hour or so reading longer stories.  \r\n\r\nThis isn't handwashing OCD behavior here - I don't read or even linger on more than a small fraction of what ends up on the screen.  It's a lot of quick glancing, page-down, and mass delete.  It's pretty much as much time as I used to spend just between a half-dozen sites like slashdot.org and cnet.com, only now my sources are so much more varied.\r\n\r\nThe challenge is that wanting to squeeze more coverage of more sources into my daily browsing has me wanting to learn more about certain technologies.  That, and just dealing with more volumes of data is kinda fun in a geeky sort of way.\r\n\r\nAnd, no, I don't record all the channels on the TV.  Besides watching 1 or 2 shows, and playing a few video games, this is what I do instead of sitting in front of the tube.</div>\r\n            \r\n        </li>\r\n    \r\n        <li class=\"comment\" id=\"comment-221086593\">\r\n            <div class=\"meta\">\r\n                <div class=\"author\">\r\n                    <a class=\"avatar image\" rel=\"nofollow\" \r\n                       href=\"http://www.reactuate.com/\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=43e4a9fa1d0e5d52a6979ddad94bc483&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\r\n                    <a class=\"avatar name\" rel=\"nofollow\" \r\n                       href=\"http://www.reactuate.com/\">Ron</a>\r\n                </div>\r\n                <a href=\"#comment-221086593\" class=\"permalink\"><time datetime=\"2004-06-16T19:52:44\">2004-06-16T19:52:44</time></a>\r\n            </div>\r\n            <div class=\"content\">When I wrote that story I wasn't expecting people to jump into a fight about RSS vs Atom. Should have known better. I just didn't like the original story and wanted to rant about it.\r\n\r\nOh well you learn something new everyday.</div>\r\n            \r\n        </li>\r\n    \r\n        <li class=\"comment\" id=\"comment-221086594\">\r\n            <div class=\"meta\">\r\n                <div class=\"author\">\r\n                    <a class=\"avatar image\" rel=\"nofollow\" \r\n                       href=\"http://blog.ianbicking.org\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=cc8334869c9d2a9e603017f2da805eb3&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\r\n                    <a class=\"avatar name\" rel=\"nofollow\" \r\n                       href=\"http://blog.ianbicking.org\">Ian Bicking</a>\r\n                </div>\r\n                <a href=\"#comment-221086594\" class=\"permalink\"><time datetime=\"2004-06-17T00:36:51\">2004-06-17T00:36:51</time></a>\r\n            </div>\r\n            <div class=\"content\">You might want to look at Reverend: http://www.divmod.org/Home/Projects/Reverend/\r\n\r\nIt's a general-purpose Bayesian classifier.</div>\r\n            \r\n        </li>\r\n    \r\n        <li class=\"comment\" id=\"comment-221086595\">\r\n            <div class=\"meta\">\r\n                <div class=\"author\">\r\n                    <a class=\"avatar image\" rel=\"nofollow\" \r\n                       href=\"http://quillio.com/\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=b228325c24751d6a33a893fb8116d32f&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\r\n                    <a class=\"avatar name\" rel=\"nofollow\" \r\n                       href=\"http://quillio.com/\">Lou Quillio</a>\r\n                </div>\r\n                <a href=\"#comment-221086595\" class=\"permalink\"><time datetime=\"2004-06-17T05:01:13\">2004-06-17T05:01:13</time></a>\r\n            </div>\r\n            <div class=\"content\">There&#8217;s what we&#8217;d like to scan and what we&#8217;d like to read, yes?  Moving content to the must-read front burner will always be deliberate, but improving the content we scan can probably be improved mechanically.  I doubt Brother Orchard thinks there&#8217;s an algorithm for the A-list.  We&#8217;re just talking about making more productive use of the proximate cloud.\r\n\r\nScenario #1:  You Google for topics of interest.  The hits returned can&#8217;t be reliably filtered by creation date, and they consider (in general) the entire indexed Web.  It&#8217;s a good thing, if scatter-shot.  Queries can only be refined so much.\r\n\r\nScenario #2:  You amass and manage a list of syndicated feeds in which you have a passing interest.  The feed content is indexed (Bloglines does this; Google does or will, too).  You can query this universe discretely, using Google-like devices and maybe a few new ones.\r\n\r\nWhat&#8217;s different about these two?\r\n\r\nIn #2, you selected (and casually manage) the universe.  Its chunks &#8212; though perhaps truncated or paraphrased by the author &#8212; are reliably date-stamped.  A free-text query of such data would be very different from an identical Google query.\r\n\r\nAdd a few more discriminators and it gets better.  Suppose you could discriminate by\r\n\r\n\r\n1. Paragraph length.  That might get you recent topical hits of terse or essay length, as you choose.\r\n\r\n2. Link-text density.  Filter in or out those link-collection posts.\r\n\r\n3. Query-term density or semantic weight.  Is the topic being addressed directly or simply mentioned?\r\n\r\n4. Item weight or image density.  Helps filter noisy or glitzy corporate feeds (think Ziff-Davis).\r\n\r\nThere are plenty more.\r\n\r\nThe main differences, though, are date discrimination and the pre-filter of a confined universe.  Me, I&#8217;m down to thirty feeds and ignore half of those.  Better tools might change that.\r\n\r\nLQ</div>\r\n            \r\n        </li>\r\n    \r\n        <li class=\"comment\" id=\"comment-221086596\">\r\n            <div class=\"meta\">\r\n                <div class=\"author\">\r\n                    <a class=\"avatar image\" rel=\"nofollow\" \r\n                       href=\"http://advogato.org/person/mbrubeck/\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=85232f8499fd6ee91623408fc23835d1&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\r\n                    <a class=\"avatar name\" rel=\"nofollow\" \r\n                       href=\"http://advogato.org/person/mbrubeck/\">Matt Brubeck</a>\r\n                </div>\r\n                <a href=\"#comment-221086596\" class=\"permalink\"><time datetime=\"2004-06-19T21:06:49\">2004-06-19T21:06:49</time></a>\r\n            </div>\r\n            <div class=\"content\">I love the picture.  Need input!</div>\r\n            \r\n        </li>\r\n    \r\n        </ul>\r\n    \r\n        </div>\r\n    ",
    "parentPath": "../blog.lmorchard.com/posts/archives/2004",
    "path": "2004/06/14/info-freako-or-whos-already-past-arguing-about-syndication-formats",
    "thumbnail": "http://www.johnny-five.com/simplenet/Shortcircuit/Pics/Pictures/Misc/jfive.gif",
    "prevPostPath": "2004/06/16/wishofthemonthclub1",
    "nextPostPath": "2004/06/13/i-will-do-the-fandango"
  },
  {
    "comments_archived": true,
    "date": "2004-06-13T11:07:46.000Z",
    "layout": "post",
    "title": "I will do the Fandango.",
    "wordpress_id": 527,
    "wordpress_slug": "i-will-do-the-fandango",
    "wordpress_url": "http://www.decafbad.com/blog/?p=527",
    "year": "2004",
    "month": "06",
    "day": "13",
    "isDir": false,
    "slug": "i-will-do-the-fandango",
    "postName": "2004-06-13-i-will-do-the-fandango",
    "html": "<p>So, it&#39;s been almost a month since my last post here.  I didn&#39;t mean to go directly into radio silence, but I&#39;m almost ready to deliver on that first article <a href=\"http://www.decafbad.com/blog/2004/05/25/i_was_a_preteen_transactor_author_wannabe_and_still_am\">I&#39;d promised</a>.  I think I may have overdone it a bit, since I was shooting for 2500 words and have landed somewhere around 7000 words.  For the first time ever, I&#39;ve been going over a first draft hardcopy of something destined for my blog with a pen, and am asking <a href=\"http://missadroit.livejournal.com/\">my girlfriend</a> to help me out with some proofreading and editing.  Once we&#39;ve gotten this thing in some semblance of presentability, I&#39;ll let you all decide whether its worth the verbosity.</p>\n<p>Until then, I leave you with this bit of 0xDECAFBAD zen courtesy of <a href=\"http://stingthebee.nu/site/index.php\">one of my old pals</a>:</p>\n<p><img src=\"http://www.decafbad.com/2004/06/les.jpg\" alt=\"I will do the Fandango\"></p>\n<p>(Which reminds me, I think I need to get to rotating those images over there soon.)</p>\n<div id=\"comments\" class=\"comments archived-comments\"><h3>Archived Comments</h3>\n<ul class=\"comments\">\n<li class=\"comment\" id=\"comment-221088688\">\n<div class=\"meta\">\n<div class=\"author\">\n<a class=\"avatar image\" rel=\"nofollow\" \nhref=\"http://eric.perceive.net\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=535a458afd969143ed08a916a805333f&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\n<a class=\"avatar name\" rel=\"nofollow\" \nhref=\"http://eric.perceive.net\">Eric Vitiello</a>\n</div>\n<a href=\"#comment-221088688\" class=\"permalink\"><time datetime=\"2004-06-13T17:17:02\">2004-06-13T17:17:02</time></a>\n</div>\n<div class=\"content\">Send a bolt of lightning, very very frightning, please!</div>\n</li>\n<li class=\"comment\" id=\"comment-221088691\">\n<div class=\"meta\">\n<div class=\"author\">\n<a class=\"avatar image\" rel=\"nofollow\" \nhref=\"http://www.docsblog.com\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=9f93b95eeea27cb3c3d67c068c20c28c&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\n<a class=\"avatar name\" rel=\"nofollow\" \nhref=\"http://www.docsblog.com\">Doccus</a>\n</div>\n<a href=\"#comment-221088691\" class=\"permalink\"><time datetime=\"2004-06-14T12:11:30\">2004-06-14T12:11:30</time></a>\n</div>\n<div class=\"content\">Is that Baalzebub at the top?  I hear he has a devil put aside for me.  For me.  For meeeeeee!\nThanks for the 80s flashback :)\nD</div>\n</li>\n</ul>\n</div>\n",
    "body": "So, it's been almost a month since my last post here.  I didn't mean to go directly into radio silence, but I'm almost ready to deliver on that first article [I'd promised][lastpost].  I think I may have overdone it a bit, since I was shooting for 2500 words and have landed somewhere around 7000 words.  For the first time ever, I've been going over a first draft hardcopy of something destined for my blog with a pen, and am asking [my girlfriend][missadroit] to help me out with some proofreading and editing.  Once we've gotten this thing in some semblance of presentability, I'll let you all decide whether its worth the verbosity.\r\n\r\nUntil then, I leave you with this bit of 0xDECAFBAD zen courtesy of [one of my old pals][stingthebee]:\r\n\r\n![I will do the Fandango](http://www.decafbad.com/2004/06/les.jpg)\r\n\r\n(Which reminds me, I think I need to get to rotating those images over there soon.)\r\n\r\n[missadroit]: http://missadroit.livejournal.com/\r\n[stingthebee]: http://stingthebee.nu/site/index.php\r\n[lastpost]: http://www.decafbad.com/blog/2004/05/25/i_was_a_preteen_transactor_author_wannabe_and_still_am\r\n\r\n<div id=\"comments\" class=\"comments archived-comments\">\r\n            <h3>Archived Comments</h3>\r\n            \r\n        <ul class=\"comments\">\r\n            \r\n        <li class=\"comment\" id=\"comment-221088688\">\r\n            <div class=\"meta\">\r\n                <div class=\"author\">\r\n                    <a class=\"avatar image\" rel=\"nofollow\" \r\n                       href=\"http://eric.perceive.net\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=535a458afd969143ed08a916a805333f&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\r\n                    <a class=\"avatar name\" rel=\"nofollow\" \r\n                       href=\"http://eric.perceive.net\">Eric Vitiello</a>\r\n                </div>\r\n                <a href=\"#comment-221088688\" class=\"permalink\"><time datetime=\"2004-06-13T17:17:02\">2004-06-13T17:17:02</time></a>\r\n            </div>\r\n            <div class=\"content\">Send a bolt of lightning, very very frightning, please!</div>\r\n            \r\n        </li>\r\n    \r\n        <li class=\"comment\" id=\"comment-221088691\">\r\n            <div class=\"meta\">\r\n                <div class=\"author\">\r\n                    <a class=\"avatar image\" rel=\"nofollow\" \r\n                       href=\"http://www.docsblog.com\"><img src=\"http://www.gravatar.com/avatar.php?gravatar_id=9f93b95eeea27cb3c3d67c068c20c28c&amp;size=32&amp;default=http://mediacdn.disqus.com/1320279820/images/noavatar32.png\"/></a>\r\n                    <a class=\"avatar name\" rel=\"nofollow\" \r\n                       href=\"http://www.docsblog.com\">Doccus</a>\r\n                </div>\r\n                <a href=\"#comment-221088691\" class=\"permalink\"><time datetime=\"2004-06-14T12:11:30\">2004-06-14T12:11:30</time></a>\r\n            </div>\r\n            <div class=\"content\">Is that Baalzebub at the top?  I hear he has a devil put aside for me.  For me.  For meeeeeee!\r\n\r\nThanks for the 80s flashback :)\r\n\r\nD</div>\r\n            \r\n        </li>\r\n    \r\n        </ul>\r\n    \r\n        </div>\r\n    ",
    "parentPath": "../blog.lmorchard.com/posts/archives/2004",
    "path": "2004/06/13/i-will-do-the-fandango",
    "thumbnail": "http://www.decafbad.com/2004/06/les.jpg",
    "prevPostPath": "2004/06/14/info-freako-or-whos-already-past-arguing-about-syndication-formats",
    "nextPostPath": "2004/05/25/i-was-a-pre-teen-transactor-author-wannabe-and-still-am"
  }
]